{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YinYangFit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skosch/YinYangFit/blob/master/YinYangFit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XZBAz3JvUbx",
        "colab_type": "text"
      },
      "source": [
        "## Set up TF2 and import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHFF5hat2n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name != '/device:GPU:0':\n",
        "      raise SystemError('GPU device not found')\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    \n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5H5-Eq0vAve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "pi = np.pi\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.ndimage.interpolation import affine_transform\n",
        "import tensorflow as tf\n",
        "import random; random.seed()\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm as tqdm\n",
        "import sys\n",
        "from functools import reduce\n",
        "import random\n",
        "from itertools import cycle, islice, product\n",
        "import operator\n",
        "\n",
        "!pip install --quiet --upgrade git+git://github.com/simoncozens/tensorfont.git\n",
        "!pip install --quiet fonttools\n",
        "!pip install --upgrade git+git://github.com/simoncozens/fontParts.git@d444bde6e2a0adbcd9a16593a615a99823089c70\n",
        "!pip install booleanOperations\n",
        "!pip install --quiet --upgrade ufo-extractor\n",
        "!pip install --quiet --upgrade defcon\n",
        "!pip install --quiet --upgrade ufo2ft\n",
        "import fontParts\n",
        "import extractor\n",
        "import defcon\n",
        "from ufo2ft import compileOTF\n",
        "\n",
        "from tensorfont import Font\n",
        "\n",
        "print(\"✓ Dependencies imported.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAPx3wjHvzBt",
        "colab_type": "text"
      },
      "source": [
        "### Download font files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1E26X5XvoAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -q -O OpenSans-Regular.ttf https://github.com/googlefonts/opensans/blob/master/ttfs/OpenSans-Regular.ttf?raw=true\n",
        "#!wget -q -O Roboto.ttf https://github.com/google/fonts/blob/master/apache/roboto/Roboto-Regular.ttf?raw=true\n",
        "#!wget -q -O Roboto.otf https://github.com/AllThingsSmitty/fonts/blob/master/Roboto/Roboto-Regular/Roboto-Regular.otf?raw=true\n",
        "#!wget -q -O DroidSerif.ttf https://github.com/datactivist/sudweb/blob/master/fonts/droid-serif-v6-latin-regular.ttf?raw=true\n",
        "!wget -q -O CrimsonItalic.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Italic.otf?raw=true\n",
        "#!wget -q -O CrimsonBold.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Bold.otf?raw=true \n",
        "#!wget -q -O CrimsonRoman.otf https://github.com/alif-type/amiri/blob/master/Amiri-Regular.ttf?raw=true\n",
        "\n",
        "!wget -q -O CrimsonRoman.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Roman.otf?raw=true\n",
        "print(\"✓ Font file(s) downloaded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3n6gilxXPl",
        "colab_type": "text"
      },
      "source": [
        "## Load font data and set up global parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lod0tNeLxaDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glyph_char_list = \"abcdeghijlmnopqrstuzywvxkf\"\n",
        "#glyph_char_list = \"abgjqrst\"\n",
        "#glyph_char_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "#glyph_char_list = \"OO\"\n",
        "\n",
        "# ==== Create Font ====\n",
        "factor = 1.14 #1.539  # This scales the size of everything\n",
        "filename = \"CrimsonRoman.otf\"\n",
        "f = Font(filename, 34 * factor) # Roboto.ttf CrimsonRoman.otf # 34 for lowercase\n",
        "box_height = f.full_height_px\n",
        "box_width = int(121 * factor)\n",
        "box_width += (box_width + 1) % 2\n",
        "print(\"Box size:\", box_height, \"×\", box_width)\n",
        "\n",
        "# 37067520 allocated, maxAllocSize: 873707520\n",
        "\n",
        "batch_size = 1  # must be divisible by 8 to work on TPU\n",
        "n_sample_distances = 3 # should be an odd number\n",
        "\n",
        "n_sizes = 18\n",
        "n_pseudo_orientations = 4\n",
        "n_orientations = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd6aKEDCv3g_",
        "colab_type": "text"
      },
      "source": [
        "## Create Gabor filter bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x7Irsj_wLJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sigmas(skip_scales=0):\n",
        "    sigmas = []\n",
        "    for s in range(n_sizes):\n",
        "        min_sigma = 0.6\n",
        "        max_sigma = box_width / 12\n",
        "        #sigmas.append((max_sigma - min_sigma) * (s + skip_scales)**2 / (n_sizes - 1)**2 + min_sigma)\n",
        "        sigmas.append((max_sigma - min_sigma) * s / n_sizes + min_sigma)\n",
        "    return np.array(sigmas)\n",
        "print(\"Sigmas are\", get_sigmas())\n",
        "\n",
        "def get_3n_filters(skip_scales, display_filters=False):\n",
        "    def rotated_mgrid(oi):\n",
        "        \"\"\"Generate a meshgrid and rotate it by RotRad radians.\"\"\"\n",
        "        rotation = np.array([[ np.cos(pi*oi/n_orientations), np.sin(pi*oi/n_orientations)],\n",
        "                             [-np.sin(pi*oi/n_orientations), np.cos(pi*oi/n_orientations)]])\n",
        "        hh = box_height # / 2\n",
        "        bw = box_width # / 2\n",
        "        y, x = np.mgrid[-hh:hh, -bw:bw].astype(np.float32)\n",
        "        y += 0.5 # 0 if box_height % 2 == 0 else 0.5\n",
        "        x += 0.5 # 0 if box_width % 2 == 0 else 0.5\n",
        "        return np.einsum('ji, mni -> jmn', rotation, np.dstack([x, y]))\n",
        "\n",
        "    def get_filter(sigma, theta):\n",
        "        x, y = rotated_mgrid(theta)\n",
        "\n",
        "        # To minimize ringing etc., we create the filter as is, then run it through the DFT.\n",
        "        a1 = 0.25 # See Georgeson et al. 2007\n",
        "        s1 = sigma #a1 * sigma\n",
        "        d1_space = -np.exp(-(x**2+y**2)/(2*s1**2))*x/(2*pi*s1**4)\n",
        "        d1 = np.fft.fft2(d1_space + 1j * np.zeros_like(d1_space)) #, [box_height, box_width])\n",
        "\n",
        "        # Second derivative:\n",
        "        s2 = sigma #np.sqrt(1. - a1**2) * sigma # See Georgeson et al. (2007)\n",
        "        d2_space = np.exp(-(x**2+y**2)/(2*s2**2))/(2*pi*s2**4) - np.exp(-(x**2+y**2)/(2*s2**2))*x**2/(2*pi*s2**6)\n",
        "        d2 = sigma**1.5 * np.fft.fft2(d2_space + 1j * np.zeros_like(d2_space)) #, [box_height, box_width])\n",
        "\n",
        "        # For now: d1 is complex(-d2,d1)\n",
        "        d1c = 1j * (d2 + 1j*d1) #/ sigma #np.sqrt(sigma)\n",
        "        return (d1c, d2)\n",
        "\n",
        "    d1_bank = np.zeros((n_sizes, n_orientations, 2*box_height, 2*box_width)).astype(np.complex64)\n",
        "    d2_bank = np.zeros((n_sizes, n_orientations, 2*box_height, 2*box_width)).astype(np.complex64)\n",
        "\n",
        "    if display_filters:\n",
        "        sizediv = 60\n",
        "        fig, ax = plt.subplots(nrows=n_sizes*2, ncols=n_orientations, gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(box_width * n_orientations / sizediv, box_height * n_sizes * 2 / sizediv))\n",
        "\n",
        "    sigmas = get_sigmas()\n",
        "    for s in range(n_sizes):\n",
        "        sigma = sigmas[s]\n",
        "        for o in range(n_orientations):\n",
        "            (d1, d2) = get_filter(sigma, o)\n",
        "            if display_filters:\n",
        "                #ax[s*2, o].imshow(np.real(np.fft.ifft2(d1)), cmap=\"inferno\")\n",
        "                ax[s*2, o].imshow(np.abs(np.fft.fftshift(d1)), cmap=\"inferno\")\n",
        "                ax[s*2, o].set_aspect(\"auto\")\n",
        "                ax[s*2, o].set_yticklabels([])\n",
        "                ax[s*2+1, o].imshow(np.imag(np.fft.ifft2(d1)), cmap=\"inferno\")\n",
        "                ax[s*2+1, o].set_aspect(\"auto\")\n",
        "                ax[s*2+1, o].set_yticklabels([])\n",
        "            d1_bank[s, o, :, :] = d1\n",
        "            d2_bank[s, o, :, :] = d2\n",
        "\n",
        "    if display_filters:\n",
        "        plt.show()\n",
        "\n",
        "    return (d1_bank.astype(np.complex64), d2_bank)\n",
        "\n",
        "d1_filter_bank, d2_filter_bank = get_3n_filters(0, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojRjQUML16hz",
        "colab_type": "text"
      },
      "source": [
        "## Rasterize the glyphs into numpy arrays, and extract their ink widths in pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crNhHm8d2RQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_filter_bank(input_image, filter_bank):\n",
        "    \"\"\"\n",
        "    Input image should have dimensions <h, w> or <s, o, h, w> or <b, s, o, h, w, d>.\n",
        "    Filter bank should have dimensions <s, o, h, w>\n",
        "    \"\"\"\n",
        "    if len(input_image.shape) == 2:\n",
        "        bdsohw_input_image = input_image[None, None, None, None, :, :]\n",
        "    elif len(input_image.shape) == 4:\n",
        "        bdsohw_input_image = input_image[None, None, :, :, :, :]\n",
        "    elif len(input_image.shape) == 6:\n",
        "        bdsohw_input_image = tf.einsum(\"bsohwd->bdsohw\", input_image)\n",
        "\n",
        "    # pad image to filter size, which is 2*box_height, 2*box_width (to prevent too much wrapping)\n",
        "    padded_input = tf.pad(bdsohw_input_image, [[0, 0], [0, 0], [0, 0], [0, 0],\n",
        "                            [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "                            [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT')\n",
        "\n",
        "    input_in_freqdomain = tf.signal.fft2d(tf.dtypes.cast(tf.complex(padded_input, tf.zeros_like(padded_input)), tf.complex64))\n",
        "\n",
        "    padded_result = (tf.signal.ifft2d(input_in_freqdomain * filter_bank[None, None, :, :, :, :]))\n",
        "\n",
        "    if len(input_image.shape) == 2:\n",
        "        presult = tf.signal.fftshift(padded_result[0, 0, :, :, :, :], axes=[2, 3])\n",
        "        # Return <s, o, h, w>\n",
        "        return (presult[:, :, int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))],\n",
        "                input_in_freqdomain)\n",
        "    elif len(input_image.shape) == 4:\n",
        "        presult = tf.signal.fftshift(padded_result[0, 0, :, :, :, :], axes=[2, 3])\n",
        "        # Return <s, o, h, w>\n",
        "        return (presult[:, :, int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))],\n",
        "                input_in_freqdomain)\n",
        "    elif len(input_image.shape) == 6:\n",
        "        presult = tf.einsum(\"bdsohw->bsohwd\", tf.signal.fftshift(padded_result, axes=[2, 3]))\n",
        "        return (presult[:, :, :, :,\n",
        "                           int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))],\n",
        "                input_in_freqdomain)\n",
        "\n",
        "def get_glyph_data_with_filtered_as_dict(glyph_char):\n",
        "    \"\"\"\n",
        "    Returns a dict containing relevant glyph data, including filtered images.\n",
        "    @param glyph_char: string of length 1\n",
        "    \"\"\"\n",
        "    glyph_image = f.glyph(glyph_char).as_matrix(normalize=True).with_padding_to_constant_box_width(box_width).astype(np.float32)\n",
        "    filtered_image, fft_image = apply_filter_bank(glyph_image, d1_filter_bank)\n",
        "\n",
        "    return {\n",
        "        'glyph_char': glyph_char,\n",
        "        'glyph_image': glyph_image,\n",
        "        'glyph_ink_width': f.glyph(glyph_char).ink_width,\n",
        "        'glyph_d1_filtered_images': filtered_image,\n",
        "    }\n",
        "\n",
        "def get_sample_distances_and_translations(gd1, gd2, target_ink_distance, distances=None):\n",
        "    \"\"\"Returns a list of distances at which the box images have to be shifted left and right before they can be overlaid to sample their normalized interaction\"\"\"\n",
        "    total_width_at_minimum_ink_distance = gd1['glyph_ink_width'] + gd2['glyph_ink_width'] - f.minimum_ink_distance(gd1['glyph_char'], gd2['glyph_char'])\n",
        "    \n",
        "    if distances is None:\n",
        "        relative_sample_distances = [0]\n",
        "        simax = int((n_sample_distances - 1)/2)\n",
        "        \n",
        "        pos_ad = +2 #/ 2.\n",
        "        neg_ad = -2 #(target_ink_distance - 2)\n",
        "        next_pos = pos_ad\n",
        "        next_neg = neg_ad\n",
        "        for si in range(simax):\n",
        "            # always append a positive, and then ...\n",
        "            relative_sample_distances.append(next_pos)\n",
        "            next_pos += pos_ad\n",
        "            pos_ad *= 1.\n",
        "            # ... append a negative only if there is room\n",
        "            if target_ink_distance + next_neg >= -3 or True:\n",
        "                relative_sample_distances.append(next_neg)\n",
        "                next_neg += neg_ad\n",
        "            else:\n",
        "                relative_sample_distances.append(next_pos)\n",
        "                next_pos += pos_ad\n",
        "                pos_ad *= 1.\n",
        "        \n",
        "        relative_sample_distances.sort()\n",
        "        zero_index_val = relative_sample_distances.index(0)\n",
        "        sample_distances = (np.array(relative_sample_distances) + target_ink_distance)\n",
        "        \n",
        "    else:\n",
        "        relative_sample_distances = distances\n",
        "        zero_index_val = 0   # doesn't really have a meaning in this case\n",
        "        sample_distances = np.array(distances) #+ f.minimum_ink_distance(gd1['glyph_char'], gd2['glyph_char'])\n",
        "\n",
        "    sample_distances_left = np.ceil(sample_distances / 2)\n",
        "    sample_distances_right = np.floor(sample_distances / 2)\n",
        "    \n",
        "    total_ink_width = gd1['glyph_ink_width'] + gd2['glyph_ink_width']\n",
        "    ink_width_left = np.floor(total_ink_width / 4)\n",
        "    ink_width_right = np.ceil(total_ink_width / 4)\n",
        "    \n",
        "    left_translations = (-(np.ceil(total_width_at_minimum_ink_distance/2) + sample_distances_left) - (-ink_width_left)).astype(np.int32)\n",
        "    right_translations = ((np.floor(total_width_at_minimum_ink_distance/2) + sample_distances_right) - ink_width_right).astype(np.int32)\n",
        "\n",
        "    return {\n",
        "        'sample_distances': sample_distances,\n",
        "        'relative_sample_distances': relative_sample_distances,\n",
        "        'left_translations': left_translations,\n",
        "        'right_translations': right_translations,\n",
        "        'zero_index': zero_index_val,\n",
        "    }\n",
        "\n",
        "def shift_sohw1_into_sohwd(input_images, translations):\n",
        "    \"\"\"Shifts images to left/right and back-fills with zeros.\n",
        "    @param images: <sizes, orientations, height, width, 1>\n",
        "    @param translations: <len(translations)>\n",
        "    @output        <sizes, orientations, height, width, len(translations)>\n",
        "    \"\"\"\n",
        "    images = tf.tile(input_images, [1, 1, 1, 1, translations.shape[0]]) # create len(shifts) channel copies\n",
        "    fill_constant = 0\n",
        "    left = tf.maximum(0, tf.reduce_max(translations)) # positive numbers are shifts to the right, for which we need to add zeros on the left\n",
        "    right = -tf.minimum(0, tf.reduce_min(translations)) # negative numbers are shifts to the left, for which we need to add zeros on the right\n",
        "    left_mask = tf.ones(shape=(tf.shape(images)[0], tf.shape(images)[1], tf.shape(images)[2], left, tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    right_mask = tf.ones(shape=(tf.shape(images)[0], tf.shape(images)[1], tf.shape(images)[2], right, tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    padded_images = tf.concat([left_mask, images, right_mask], axis=3) # pad on axis 3 (i.e. width-wise)\n",
        "\n",
        "    # Now that the images are all padded, we need to crop them to implement the shifts.\n",
        "    def crop_image_widthwise(image_and_shift):\n",
        "        image = image_and_shift[0] # sohw\n",
        "        shift = image_and_shift[1] # \n",
        "        return image[:, :, :, left-shift:left-shift+input_images.shape[3]] # positive shift: left-shift\n",
        "\n",
        "    return tf.einsum(\"dsohw->sohwd\", tf.map_fn(\n",
        "        crop_image_widthwise,\n",
        "        (tf.einsum(\"sohwd->dsohw\", padded_images), translations),\n",
        "        dtype=images.dtype))\n",
        "\n",
        "def shift_hwso1_into_hwsod(input_images, translations):\n",
        "    \"\"\"Shifts images to left/right and back-fills with zeros.\n",
        "    @param images: <height, width, sizes, orientations, 1>\n",
        "    @param translations: <len(translations)>\n",
        "    @output        <height, width, sizes, orientations, len(translations)>\n",
        "    \"\"\"\n",
        "    images = tf.tile(input_images, [1, 1, 1, 1, translations.shape[0]]) # create len(shifts) channel copies\n",
        "    fill_constant = 0\n",
        "    left = tf.maximum(0, tf.reduce_max(translations)) # positive numbers are shifts to the right, for which we need to add zeros on the left\n",
        "    right = -tf.minimum(0, tf.reduce_min(translations)) # negative numbers are shifts to the left, for which we need to add zeros on the right\n",
        "    left_mask = tf.ones(shape=(tf.shape(images)[0], left, tf.shape(images)[2], tf.shape(images)[3], tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    right_mask = tf.ones(shape=(tf.shape(images)[0], right, tf.shape(images)[2], tf.shape(images)[3], tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    padded_images = tf.concat([left_mask, images, right_mask], axis=1) # pad on axis 2 (i.e. width-wise)\n",
        "\n",
        "    # Now that the images are all padded, we need to crop them to implement the shifts.\n",
        "    def crop_image_widthwise(image_and_shift):\n",
        "        image = image_and_shift[0]\n",
        "        shift = image_and_shift[1]\n",
        "        return image[:, left-shift:left-shift+input_images.shape[1], :, :] # positive shift: left-shift\n",
        "\n",
        "    return tf.einsum(\"dhwso->hwsod\", tf.map_fn(\n",
        "        crop_image_widthwise,\n",
        "        (tf.einsum(\"hwsod->dhwso\", padded_images), translations),\n",
        "        dtype=images.dtype))\n",
        "\n",
        "def shift_fft_image(input_image, translations):\n",
        "    # Input is directly from apply_filter_bank, in format <bdsohw>, padded to 2*box_height, 2*box_width\n",
        "\n",
        "    xshift_array = np.mgrid[-box_width:box_width][None, None, None, None, :] * translations[:, None, None, None, None] / (2 * box_width)\n",
        "    tiled_centered_input_image = tf.tile(tf.signal.fftshift(input_image, [3, 4]), [translations.shape[0], 1, 1, 1, 1])\n",
        "    shifted_images = tf.signal.ifftshift(tiled_centered_input_image * np.exp(-2j * np.pi * xshift_array), [3, 4])\n",
        "\n",
        "    return tf.einsum(\"dsohw->sohwd\", shifted_images)\n",
        "\n",
        "def shift_and_overlay_pair_data(gd1, gd2, distances=None):\n",
        "    \"\"\"\n",
        "    Returns a 5D tensor <box_height, box_width, sizes, orientations, distances>.\n",
        "    \"\"\"\n",
        "\n",
        "    target_ink_distance = int(f.pair_distance(gd1['glyph_char'], gd2['glyph_char']) + f.minimum_ink_distance(gd1['glyph_char'], gd2['glyph_char']))\n",
        "    sdt = get_sample_distances_and_translations(gd1, gd2, target_ink_distance, distances)\n",
        "\n",
        "    shifted_gd1_d1_filtered_images = shift_sohw1_into_sohwd(gd1['glyph_d1_filtered_images'][..., None], sdt['left_translations'])\n",
        "    shifted_gd2_d1_filtered_images = shift_sohw1_into_sohwd(gd2['glyph_d1_filtered_images'][..., None], sdt['right_translations'])\n",
        "\n",
        "    # We want to shift both the original images (for display purposes only), as well as the filtered images.\n",
        "    pair_images = (shift_hwso1_into_hwsod(gd1['glyph_image'][..., None, None, None], sdt['left_translations']) + \n",
        "                   shift_hwso1_into_hwsod(gd2['glyph_image'][..., None, None, None], sdt['right_translations']))[:, :, 0, 0, :]\n",
        "    zero_index = sdt['zero_index']\n",
        "    sample_distances = sdt['sample_distances']\n",
        "\n",
        "    return  {\n",
        "        'shifted_gd1_d1_filtered_images': shifted_gd1_d1_filtered_images,\n",
        "        'shifted_gd2_d1_filtered_images': shifted_gd2_d1_filtered_images,\n",
        "        'ink_distance': target_ink_distance,\n",
        "        'pair_images': pair_images,\n",
        "        'zero_index': zero_index,\n",
        "        'sample_distances': sample_distances,\n",
        "    }\n",
        "\n",
        "class InputGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        print(\"Creating glyph images ...\", flush=True)\n",
        "        self.glyph_data = []\n",
        "        for glyph_char in tqdm(glyph_char_list):\n",
        "            self.glyph_data.append(get_glyph_data_with_filtered_as_dict(glyph_char))\n",
        "        self.n_pairs = len(glyph_char_list) ** 2\n",
        "        self.cached_pair_data = {}\n",
        "\n",
        "    def kill(self):\n",
        "        del self.glyph_data\n",
        "        del self.cached_pair_data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches in the dataset\"\"\"\n",
        "        return math.ceil(self.n_pairs / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return the content of batch idx.\n",
        "        Instead of always providing the same data for batch i,\n",
        "        we just pick batch_size random glyph pairs and return their glyph data.\n",
        "\n",
        "        This is run on the CPU, because otherwise the calculations sit in GPU memory,\n",
        "        are never released, and lead to annoying out-of-memory issues all the time.\n",
        "\n",
        "        Output: \n",
        "        ([shifted_gd1_filtered_images, shifted_gd2_filtered_images, sample_distances], [ink_distance, pair_images, zero_index])\n",
        "        \"\"\"\n",
        "        with tf.device('/CPU:0'):\n",
        "            g_shifted_gd1_d1_filtered_images = []\n",
        "            g_shifted_gd2_d1_filtered_images = []\n",
        "            g_sample_distances = []\n",
        "            g_ink_distance = []\n",
        "            g_pair_images = []\n",
        "            g_zero_index = []\n",
        "            for i in range(batch_size):\n",
        "                while True:\n",
        "                    g1 = random.choice(self.glyph_data)\n",
        "                    g2 = random.choice(self.glyph_data)\n",
        "                    if f.pair_distance(g1['glyph_char'], g2['glyph_char']) + f.minimum_ink_distance(g1['glyph_char'], g2['glyph_char']) > 2:\n",
        "                        break\n",
        "                #if (g1['glyph_char'] + g2['glyph_char']) not in self.cached_pair_data:\n",
        "                #    self.cached_pair_data[g1['glyph_char'] + g2['glyph_char']] = shift_and_overlay_pair_data(g1, g2)\n",
        "                #cpd = self.cached_pair_data[g1['glyph_char'] + g2['glyph_char']]\n",
        "\n",
        "                # Don't cache the data -- does that save RAM?\n",
        "                cpd = shift_and_overlay_pair_data(g1, g2)\n",
        "\n",
        "                g_shifted_gd1_d1_filtered_images.append(cpd['shifted_gd1_d1_filtered_images'])\n",
        "                g_shifted_gd2_d1_filtered_images.append(cpd['shifted_gd2_d1_filtered_images'])\n",
        "                g_sample_distances.append(cpd['sample_distances'])\n",
        "                g_ink_distance.append(cpd['ink_distance'] * 1.0)\n",
        "                g_pair_images.append(cpd['pair_images'])\n",
        "                g_zero_index.append(cpd['zero_index'])\n",
        "    \n",
        "            inputs = [\n",
        "                tf.stack(g_shifted_gd1_d1_filtered_images),\n",
        "                tf.stack(g_shifted_gd2_d1_filtered_images),\n",
        "                tf.stack(g_sample_distances),\n",
        "                tf.stack(g_pair_images),\n",
        "                tf.stack(g_zero_index),\n",
        "            ]   \n",
        "            outputs = tf.stack(g_ink_distance)\n",
        "    \n",
        "            return inputs, outputs \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AMRfFWJwNOj",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upbQl3OvHtz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "full_shape = ( n_sizes, n_orientations, box_height, box_width, n_sample_distances)\n",
        "\n",
        "eps = np.finfo(np.float32).tiny\n",
        "\n",
        "@tf.function\n",
        "def nd_softmax(target, axis, name=None):\n",
        "    max_axis = tf.reduce_max(target, axis, keepdims=True)\n",
        "    target_exp = tf.exp(target - max_axis)\n",
        "    normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n",
        "    softmax = target_exp / (normalize + eps)\n",
        "    return softmax\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def tilo(t):\n",
        "    return tf.concat([t[:, :, 0:1, :, :, :], t[:, :, 1:2, :, :, :], t[:, :, 2:3, :, :, :], t[:, :, 1:2, :, :, :]], axis=2)\n",
        "@tf.function\n",
        "def tilop(t):\n",
        "    return tf.nn.softplus(tilo(t))\n",
        "@tf.function\n",
        "def ptilo(t):\n",
        "    return tf.concat([t[:, :, 0:1, :, :, :, :], t[:, :, 1:2, :, :, :, :], t[:, :, 2:3, :, :, :, :], t[:, :, 1:2, :, :, :, :]], axis=2)\n",
        "@tf.function\n",
        "def ptilop(t):\n",
        "    return tf.nn.softplus(ptilo(t))\n",
        "@tf.function\n",
        "def tiloa(t):\n",
        "    return tf.concat([t[:, :, :, :, 0:1, :, :], t[:, :, :, :, 1:2, :, :], t[:, :, :, :, 2:3, :, :], t[:, :, :, :, 1:2, :, :]], axis=4)\n",
        "@tf.function\n",
        "def tiloap(t):\n",
        "    return tf.nn.softplus(tiloa(t))\n",
        "def invsp(t):\n",
        "    if t == 0:\n",
        "        return -1e10\n",
        "    else:\n",
        "        return np.log(np.exp(t) - 1).astype(np.float32)\n",
        "\n",
        "def rectify_phases(inputs):\n",
        "    # Inputs: <b, s, o, h, w, d>\n",
        "    # Output: <b, s, o, p, h, w, d> where p is [0, 1, 2, 3]\n",
        "    return tf.stack([tf.nn.relu(tf.math.real(inputs)),\n",
        "                     tf.nn.relu(tf.math.imag(inputs)),\n",
        "                     tf.nn.relu(-tf.math.real(inputs)),\n",
        "                     tf.nn.relu(-tf.math.imag(inputs))], axis=3)\n",
        "\n",
        "class BiasedAbs(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BiasedAbs, self).__init__(**kwargs)\n",
        "\n",
        "        self.bias_weights = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(0.),\n",
        "                                       name='bias_weights',\n",
        "                                       trainable=True)\n",
        "        self.exponent = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(2.2),\n",
        "                                       name='exponent',\n",
        "                                       trainable=True)\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(tf.nn.sigmoid(tilo(self.bias_weights))[0, :, :, 0, 0, 0])\n",
        "        print(\"Exponents:\")\n",
        "        plt.imshow(tilop(self.exponent)[0, :, :, 0, 0, 0])\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        squashed_bias_weights = tf.nn.sigmoid(tilo(self.bias_weights))\n",
        "        # 0.0 -> abs(real)\n",
        "        # 0.5 -> abs(inputs)\n",
        "        # 1.0 -> abs(imag)\n",
        "        fr = 1. - squashed_bias_weights\n",
        "        fi = squashed_bias_weights\n",
        "        biased_abs = (tf.sqrt(eps + fr * tf.math.real(inputs)**2 + fi * tf.math.imag(inputs)**2) * tf.sqrt(2.)) ** tilop(self.exponent)\n",
        "        #biased_abs = tf.sqrt(eps + fr * (inputs[:, :, :, 0, :, :, :] - inputs[:, :, :, 2, :, :, :])**2 + fi * (inputs[:, :, :, 1, :, :, :] - inputs[:, :, :, 3, :, :, :])**2) * tf.sqrt(2.)\n",
        "\n",
        "        return biased_abs\n",
        "\n",
        "class NormalizationPool(tf.keras.layers.Layer):\n",
        "    # This layer computes the unscaled normalization pool for each neuron.\n",
        "    # It effectively performs a blur over space, frequency, and orientation, using\n",
        "    # Gaussian blur via 3D FFT (instead of a convolutional layer) followed by\n",
        "    # a matrix multiplication over the orientation axis (arbitrary kernel).\n",
        "    #\n",
        "    # The input is a 7D tensor of reals <b, s, o, p, y, x, d> (scale_index, orientation_index, phase_index, vertical coordinate, horizontal coordinate)\n",
        "    # The output is a 7D tensor of reals <b, s, o, p, y, x, d> (scale_index, orientation_index, phase_index, vertical coordinate, horizontal coordinate)\n",
        "    #\n",
        "    # For more information, see Sawada & Petrov (2017)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NormalizationPool, self).__init__(**kwargs)\n",
        "\n",
        "        self.sigmas = get_sigmas()\n",
        "        self.wavelengths = self.get_wavelengths()\n",
        "        self.r2grid, self.sgrid = self.get_distgrids()\n",
        "\n",
        "        self.spatial_pool_size_factor = self.add_weight(shape=(),\n",
        "                                                        initializer=tf.keras.initializers.Constant(1.),\n",
        "                                                        name='spatial_pool_size_factor',\n",
        "                                                        trainable=True)\n",
        "        self.scale_pool_size_factor = self.add_weight(shape=(),\n",
        "                                                      initializer=tf.keras.initializers.Constant(4.),\n",
        "                                                      name='scale_pool_size_factor',\n",
        "                                                      trainable=True)\n",
        "\n",
        "        self.rescale_factor = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(-0.0),\n",
        "                                       name='npool_rescale_factor',\n",
        "                                       trainable=False)\n",
        "        #self.phase_congruency_coefficient = self.add_weight(shape=(),\n",
        "        #                                                      initializer=tf.keras.initializers.Constant(2.),\n",
        "        #                                                      name=\"phase_congruency_coefficient\",\n",
        "        #                                                      trainable=True)\n",
        "\n",
        "        # If this turns out to be symmetric, replace it with a von-Mises distribution\n",
        "        # factor = exp(1.22 * cos(angle-diff)**2)  -- 1.22 coefficient taken from Sawada\n",
        "        self.orientation_diff_matrix = np.array([[0, 45, 90, -45], [-45, 0, 45, 90], [90, -45, 0, 45], [45, 90, -45, 0]]).astype(np.float32) * np.pi / 180\n",
        "        self.orientation_inhibition_coefficient = self.add_weight(shape=(),\n",
        "                                                             initializer=tf.keras.initializers.Constant(1.22),\n",
        "                                                             name='orientation_inhibition_coefficient',\n",
        "                                                             trainable=True)\n",
        "\n",
        "        # factor = exp(1.22 * sin(0.5 angle-diff)**2)  -- 1.22 coefficient taken from Sawada\n",
        "        self.phase_diff_matrix = np.array([[0, 90, 180, -90], [-90, 0, 90, 180], [180, -90, 0, 90], [90, 180, -90, 0]]).astype(np.float32) * np.pi / 180\n",
        "        self.phase_inhibition_coefficient = self.add_weight(shape=(),\n",
        "                                                             initializer=tf.keras.initializers.Constant(1.0),\n",
        "                                                             name='phase_inhibition_coefficient',\n",
        "                                                             trainable=True)\n",
        "        \n",
        "        # Instead of an orientation inhibition matrix, we use a combined spatial/orientation filter.\n",
        "        # Inhibition should be highest for orientations either in the opposition direction, or in the same direction but\n",
        "        # aligned parallel.\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(\"Spatial pool size factor\", tf.nn.softplus(self.spatial_pool_size_factor.numpy()))\n",
        "        print(\"Scale pool size factor\", tf.nn.softplus(self.scale_pool_size_factor.numpy()))\n",
        "        print(\"Rescale factor\", self.rescale_factor)\n",
        "\n",
        "        orientation_inhibition_matrix = tf.exp(self.orientation_inhibition_coefficient * tf.cos(self.orientation_diff_matrix)**2)\n",
        "        phase_inhibition_matrix = tf.exp(self.phase_inhibition_coefficient * tf.sin(0.5 * self.phase_diff_matrix)**2)\n",
        "        print(\"Orientation inhibition matrix\")\n",
        "        plt.imshow(orientation_inhibition_matrix.numpy())\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        print(\"Phase inhibition matrix\")\n",
        "        plt.imshow(phase_inhibition_matrix.numpy())\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def get_wavelengths(self):\n",
        "        sigmas = self.sigmas\n",
        "        # We are padding to twice n_sizes\n",
        "        padded_sigmas = [sigmas[0]] * int(np.ceil(n_sizes / 2)) + list(sigmas) + [sigmas[-1]] * int(n_sizes / 2)\n",
        "        # Used for the distgrids, which are sorted <b, d, o, p, s, y, x> (because FFT works on innermost axes)\n",
        "        return np.array(padded_sigmas).astype(np.float32)[None, None, None, None, :, None, None]\n",
        "    \n",
        "    def get_distgrids(self):\n",
        "        # Computes the distance, spatially and in terms of log-wavelength, between two points\n",
        "        # in <b, d, o, p, s, y, x> space, on a grid that is spatially twice the size (will be zero-padded)\n",
        "        # in x and y, and twice the size in terms of scale as well (will be same-padded).\n",
        "        y, x = np.mgrid[-box_height:box_height,\n",
        "                        -box_width:box_width].astype(np.float32)\n",
        "        r2grid = (y**2 + x**2)[None, None, None, None, None, :, :]\n",
        "        sd = np.mgrid[-n_sizes:n_sizes].astype(np.float32)[None, None, None, None, :, None, None]\n",
        "        sgrid = sd**2 \n",
        "            # TODO: this may be incorrect; larger wavelengths may be less susceptible to\n",
        "            # the neighbouring octaves (the absolute difference may count more than the logarithmic)\n",
        "        return r2grid, sgrid\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Create the 4d Gaussian blur filter.\n",
        "        # The total distance is computed based on r2grid *and* sgrid.\n",
        "        # Sawada assumes that we can take the product of both filters.\n",
        "        psppsf = tf.nn.softplus(self.spatial_pool_size_factor)\n",
        "        s_spatial_filter = (tf.exp(-self.r2grid/(eps + psppsf*self.wavelengths**2)) / \n",
        "                            (tf.sqrt(2*np.pi)*(psppsf*self.wavelengths)**2 + eps))\n",
        "                            # SPATIAL FILTER: self.wavelengths need to be twice as big\n",
        "\n",
        "        pscpsf = tf.nn.softplus(self.scale_pool_size_factor)\n",
        "        s_scale_filter = (tf.exp(-self.sgrid/(eps + pscpsf**2)) / (tf.sqrt(2*np.pi)*pscpsf**2) + eps)\n",
        "                            \n",
        "        s_filters = s_spatial_filter * s_scale_filter\n",
        "\n",
        "        # The same filters, in frequency space\n",
        "        f_filters = tf.signal.fft3d(tf.signal.fftshift(tf.complex(s_filters, tf.zeros_like(s_filters))))\n",
        "\n",
        "        # Reshape inputs, so that <s, y, x> are the innermost dimensions, because fft3d works\n",
        "        # on the innermost dims\n",
        "\n",
        "        sigma_scale_factors = tf.exp(self.sigmas * self.rescale_factor)[None, :, None, None, None, None, None]\n",
        "        rescaled_inputs = inputs * sigma_scale_factors\n",
        "\n",
        "        r_s_inputs = tf.einsum(\"bsopyxd->bdopsyx\", rescaled_inputs)\n",
        "\n",
        "        # Pad the inputs (except for the phases, which we don't mind if the DFT wraps them)\n",
        "        #pr_s_inputs1 = tf.pad(r_s_inputs,\n",
        "        #                    [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n",
        "        #                    [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "        #                    [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT')\n",
        "        #pr_s_inputs = tf.pad(pr_s_inputs1,\n",
        "        #                    [[0, 0], [0, 0], [0, 0], [0, 0],\n",
        "        #                    [int(np.ceil(n_sizes / 2)), int(n_sizes / 2)],\n",
        "        #                    [0, 0], [0, 0]], mode='CONSTANT') # we would use edge, but tf only has symmetric\n",
        "\n",
        "        # Can use the above again as soon as TF2.1 comes out and supports padding above 6 dimensions ... jeez.\n",
        "        # For now, just use the first entry on dimension 0, relying on batch_size = 1\n",
        "\n",
        "        pr_s_inputs1 = tf.pad(r_s_inputs[0, :, :, :, :, :, :],\n",
        "                           [[0, 0], [0, 0], [0, 0], [0, 0], \n",
        "                            [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "                            [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT') #[None, :, :, :, :, :, :]\n",
        "        pr_s_inputs = tf.pad(pr_s_inputs1, #[0, :, :, :, :, :, :],\n",
        "                            [[0, 0], [0, 0], [0, 0],\n",
        "                            [int(np.ceil(n_sizes / 2)), int(n_sizes / 2)],\n",
        "                            [0, 0], [0, 0]], mode='CONSTANT')[None, :, :, :, :, :, :]\n",
        "\n",
        "        # Convert inputs to frequency domain\n",
        "        pr_f_inputs = tf.signal.fft3d(tf.complex(pr_s_inputs, tf.zeros_like(pr_s_inputs)))\n",
        "\n",
        "        # Perform the filtering and convert back to space domain\n",
        "        pr_s_filtered = tf.math.real(tf.signal.ifft3d(pr_f_inputs * f_filters))\n",
        "\n",
        "        # Crop away the padding\n",
        "        r_s_filtered = pr_s_filtered[:, :, :, :,\n",
        "                                     int(np.ceil(n_sizes / 2)):int(n_sizes + np.ceil(n_sizes / 2)),\n",
        "                                     int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                                     int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))]\n",
        "        \n",
        "        # Perform cross-orientation blurring\n",
        "\n",
        "        # factor = exp(1.22 * cos(angle-diff)**2)  -- 1.22 coefficient taken from Sawada\n",
        "        orientation_inhibition_matrix = tf.exp(self.orientation_inhibition_coefficient * tf.cos(self.orientation_diff_matrix)**2)\n",
        "        phase_inhibition_matrix = tf.exp(self.phase_inhibition_coefficient * tf.sin(0.5 * self.phase_diff_matrix)**2)\n",
        "\n",
        "        # Perform cross-orientation blurring\n",
        "        r_s_ob_filtered = tf.einsum(\"bdkpsyx,kq->bdqpsyx\", r_s_filtered,\n",
        "                                    (orientation_inhibition_matrix))\n",
        "        r_s_obpb_filtered = tf.einsum(\"bdoksyx,kq->bdoqsyx\", r_s_ob_filtered,\n",
        "                                    (phase_inhibition_matrix))\n",
        "\n",
        "        # Reorder dimensions\n",
        "        s_obpb_filtered = tf.einsum(\"bdopsyx->bsopyxd\", r_s_obpb_filtered) # r_s_obpb\n",
        "         \n",
        "        return s_obpb_filtered\n",
        "\n",
        "class ApplyCsf(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ApplyCsf, self).__init__(**kwargs)\n",
        "        self.active = False\n",
        "\n",
        "        self.a = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(0.4),\n",
        "                                       name='a',\n",
        "                                       trainable=self.active)\n",
        "        self.b = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(1.0),\n",
        "                                       name='b',\n",
        "                                       trainable=self.active)\n",
        "        self.c = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(2.0),\n",
        "                                       name='c',\n",
        "                                       trainable=self.active)\n",
        "        self.d = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(0.12), # ~0.1\n",
        "                                       name='d',\n",
        "                                       trainable=self.active)\n",
        "        self.e = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(0.0), # ~0.1\n",
        "                                       name='e',\n",
        "                                       trainable=False) #self.active)\n",
        "        self.sigmas = get_sigmas()[None, :, None, None, None, None]\n",
        "\n",
        "    def get_factors(self):\n",
        "        a, b, c, d, e, s = self.a, self.b, self.c, self.d, self.e, box_width/self.sigmas # tf.nn.softplus(self.a), tf.nn.softplus(self.b), tf.nn.softplus(self.c), tf.nn.softplus(self.d), self.e, box_width/self.sigmas\n",
        "\n",
        "        factors = b*(s*a)**c * tf.exp(-d*s) * (self.sigmas ** e)\n",
        "        # s here represents the frequency, whereas sigmas are more like wavelengths\n",
        "        #factors = tf.exp(-((s-a)/b + tf.exp(-(s-a)/b) ))\n",
        "        factors /= tf.reduce_max(factors) # Scale so that the max is at 1.0\n",
        "        return (factors, (a, b, c, d, e, s))\n",
        "\n",
        "    def print_weights(self):\n",
        "        factors, (a, b, c, d, e, s) = self.get_factors()\n",
        "        print(\"Factors are:\", factors[0, :, 0, 0, 0, 0])\n",
        "        print(\"sigmas are\", (box_width/self.sigmas)[0, :, 0, 0, 0, 0])\n",
        "        print(\"A, B, C, D, E:\", a, b, c, d, e)\n",
        "\n",
        "        plt.plot(self.sigmas[0, :, 0, 0, 0, 0], factors[0, :, 0, 0, 0, 0])\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        factors, (a, b, c, d, e, s) = self.get_factors()\n",
        "        return inputs * factors if self.active else inputs\n",
        "\n",
        "class Exponentiate(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Exponentiate, self).__init__(**kwargs)\n",
        "\n",
        "        #self.exponents = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1, 1),\n",
        "        self.exponents = self.add_weight(shape=(1, 1, n_orientations - 1, 1, 1, 1, 1), #(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(invsp(2.5)),\n",
        "                                       name='exponents',\n",
        "                                       trainable=True)\n",
        "\n",
        "    def print_weights(self):\n",
        "        #print(\"Exponents:\", ptilop(self.exponents))\n",
        "        plt.imshow(tilop(self.exponents)[0, :, :, 0, 0, 0])\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        rectified_inputs, absvals = inputs\n",
        "        pabsvals = absvals[:, :, :, None, :, :, :]\n",
        "        factor = (pabsvals + 1.e-6) ** tilop(self.exponents) / (1.e-6 + pabsvals)\n",
        "        return (rectified_inputs + 1.e-6) * factor\n",
        "        #return (inputs + eps) ** ptilop(self.exponents)\n",
        "\n",
        "class DivisiveNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        super(DivisiveNormalization, self).__init__(**kwargs)\n",
        "\n",
        "        self.factors = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(1.),\n",
        "                                       name='dn_factors',\n",
        "                                       trainable=False)\n",
        "        self.beta = self.add_weight(shape=(1, 1, n_orientations - 1, 1, 1, 1, 1), # n_sizes\n",
        "                                       initializer=tf.keras.initializers.Constant(2.46),\n",
        "                                       name='dn_beta',\n",
        "                                       trainable=True)\n",
        "\n",
        "    def print_weights(self):\n",
        "        #print(\"Factors:\")\n",
        "        #plt.imshow(tilop(self.factors)[0, :, :, 0, 0, 0])\n",
        "        #plt.colorbar()\n",
        "        #plt.show()\n",
        "\n",
        "        print(\"Beta:\", ptilop(self.beta)[0, 0, :, 0, 0, 0])\n",
        "        #plt.imshow(tilop(self.beta)[0, :, :, 0, 0, 0])\n",
        "        #plt.colorbar()\n",
        "        #plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        stimulus, normalization_pool = inputs\n",
        "        return ptilop(self.factors) * stimulus / (eps + ptilop(self.beta) + normalization_pool)\n",
        "\n",
        "class PenalizeZero(tf.keras.layers.Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        super(PenalizeZero, self).__init__(**kwargs)\n",
        "\n",
        "        self.gb = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(1.0)), \n",
        "                                  name='gb', trainable=True)\n",
        "        self.gc = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(2.0)),\n",
        "                                  name='gc', trainable=True)\n",
        "        self.gd = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(0.12)), \n",
        "                                  name='gd', trainable=True)\n",
        "        self.ge = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(1.0)),\n",
        "                                  name='ge', trainable=True)\n",
        "\n",
        "        self.gain_orientation_factors= self.add_weight(shape=(1, 1, n_orientations - 1, 1, 1, 1),\n",
        "                                                      initializer=tf.keras.initializers.Constant(-0.1),\n",
        "                                                      name='gap_orientation_factors', trainable=True)\n",
        "\n",
        "        self.lb = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(1.0)),\n",
        "                                  name='lb', trainable=True)\n",
        "        self.lc = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(2.0)),\n",
        "                                  name='lc', trainable=True)\n",
        "        self.ld = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(0.12)),\n",
        "                                  name='ld', trainable=True)\n",
        "        self.le = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(1.0)),\n",
        "                                  name='le', trainable=True)\n",
        "\n",
        "        self.elosf = self.add_weight(shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(invsp(1.0)),\n",
        "                                  name='elosf', trainable=True)\n",
        "\n",
        "        self.loss_orientation_factors= self.add_weight(shape=(1, 1, n_orientations - 1, 1, 1, 1),\n",
        "                                                      initializer=tf.keras.initializers.Constant(1.0),\n",
        "                                                      name='loss_orientation_factors', trainable=True)\n",
        "\n",
        "        self.sigmas = get_sigmas()[None, :, None, None, None, None]\n",
        "\n",
        "    def get_factors(self):\n",
        "        gb, gc, gd, ge, lb, lc, ld, le, s = (\n",
        "            tf.nn.softplus(self.gb), tf.nn.softplus(self.gc), tf.nn.softplus(self.gd), tilop(self.ge),\n",
        "            tf.nn.softplus(self.lb), tf.nn.softplus(self.lc), tf.nn.softplus(self.ld), tilop(self.le),\n",
        "            box_width / self.sigmas\n",
        "        )\n",
        "            \n",
        "        gfactors = gb*(.4*s)**gc * tf.exp(-gd*s)\n",
        "        lfactors = lb*(.4*s)**lc * tf.exp(-ld*s)\n",
        "\n",
        "        return (gfactors, lfactors)\n",
        "\n",
        "    def print_weights(self):\n",
        "        gfactors, lfactors = self.get_factors()\n",
        "        print(\"Gap gain curve, factor=\", tilop(self.ge)[0, :, 0, 0, 0, 0])\n",
        "        print(\"Gap vertical orientation factors:\", tilo(self.gain_orientation_factors)[0, 0, 1:, 0, 0, 0])\n",
        "        plt.plot(self.sigmas[0, :, 0, 0, 0, 0], gfactors[0, :, 0, 0, 0, 0])\n",
        "        plt.show()\n",
        "        print(\"Edge loss curve, factor=\", tilop(self.le)[0, :, 0, 0, 0, 0])\n",
        "        print(\"Loss vertical orientation factors:\", tilo(self.loss_orientation_factors)[0, 0, 1:, 0, 0, 0])\n",
        "        plt.plot(self.sigmas[0, :, 0, 0, 0, 0], lfactors[0, :, 0, 0, 0, 0])\n",
        "        plt.show()\n",
        "        print(\"ELOSF:\", self.elosf)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_sums, diffs = inputs\n",
        "        gfactors, lfactors = self.get_factors()\n",
        "        gap_gains = tf.nn.relu(diffs[:, :, 0:1, :, :, :])\n",
        "\n",
        "        edge_losses = tf.nn.relu(-diffs[:, :, 0:1, :, :, :])\n",
        "\n",
        "        horizontal_gap_gains = (tf.nn.relu(diffs[:, :, 0:1, :, :, :]) + eps) ** tilop(self.ge)[:, :, 0:1, :, :, :]\n",
        "        vertical_gap_gains = (tf.nn.relu(diffs[:, :, 1:, :, :, :]) + eps) ** tilop(self.ge)[:, :, 1:, :, :, :] * tilo(self.gain_orientation_factors)[:, :, 1:, :, :, :]\n",
        "        gap_gains = (eps + tf.concat([horizontal_gap_gains, vertical_gap_gains], axis=2)) * gfactors\n",
        "\n",
        "        horizontal_edge_losses = (tf.nn.relu(-diffs[:, :, 0:1, :, :, :]) + eps) ** tilop(self.le)[:, :, 0:1, :, :, :]\n",
        "        vertical_edge_losses = (tf.nn.relu(-diffs[:, :, 1:, :, :, :]) + eps) ** tilop(self.le)[:, :, 1:, :, :, :] * tilo(self.loss_orientation_factors)[:, :, 1:, :, :, :]\n",
        "        edge_losses = (eps + tf.concat([horizontal_edge_losses, vertical_edge_losses], axis=2)) * lfactors * ((original_sums + eps) ** self.elosf)\n",
        "\n",
        "        return (gap_gains - edge_losses) / (eps + tf.reduce_sum(tf.abs(gap_gains[:, :, :, :, :, 1:2]) + tf.abs(edge_losses[:, :, :, :, :, 1:2]), axis=[1,2,3,4], keepdims=True)) # Try to discourage CSF from just shrinking it\n",
        "\n",
        "\n",
        "class PenalizeZeroOld(tf.keras.layers.Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        super(PenalizeZeroOld, self).__init__(**kwargs)\n",
        "\n",
        "        self.scale_exponent = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                              initializer=tf.keras.initializers.Constant(0.55),\n",
        "                                              name='scale_exponent', trainable=True) \n",
        "        self.scale_beta = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                          initializer=tf.keras.initializers.Constant(0.55),\n",
        "                                          name='scale_beta', trainable=True) \n",
        "        self.edge_loss_relative_factor = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(0.62),\n",
        "                                          name='edge_loss_relative_factor', trainable=False) \n",
        "        self.scale_mean = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(6),\n",
        "                                          name='scale_mean', trainable=False) \n",
        "        self.scale_sigma = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(2),\n",
        "                                          name='scale_sigma', trainable=False) \n",
        "        self.orientation_gain = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                          initializer=tf.keras.initializers.Constant(-0.62),\n",
        "                                          name='orientation_gain', trainable=True) \n",
        "        self.orientation_loss = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                          initializer=tf.keras.initializers.Constant(-0.62),\n",
        "                                          name='orientation_loss', trainable=True)\n",
        "        \n",
        "        yfactor_default = np.zeros((1, 1, 1, box_height, 1, 1))\n",
        "        yfactor_default[0, 0, 0, :, 0, 0] = 1. #18:70\n",
        "        self.yfactor = self.add_weight(shape=(1, 1, 1, box_height, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(yfactor_default),\n",
        "                                       name='yfactor', trainable=False)\n",
        "\n",
        "        self.sigmas = get_sigmas().astype(np.float32)[None, :, None, None, None, None]\n",
        "\n",
        "    def print_weights(self):\n",
        "        se, sb, elrs, mu, sigma, og, ol, yfactor = self.getw()\n",
        "        print(\"Gain/loss factors:\")        \n",
        "        if True:\n",
        "            fig, ax = plt.subplots(1, 4)\n",
        "            for oi in range(n_orientations):\n",
        "                ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], og[0, :, oi, 0, 0, 0], color='b')\n",
        "                ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], ol[0, :, oi, 0, 0, 0], color='r')\n",
        "                ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], se[0, :, oi, 0, 0, 0], color='b', linestyle='dotted')\n",
        "                ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], sb[0, :, oi, 0, 0, 0], color='r', linestyle='dotted')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Orientation_gain:\", og[0, 0, 0, 0, 0, 0], -og[0, 0, 1:, 0, 0, 0])\n",
        "            print(\"Orientation_loss:\", -ol[0, 0, :, 0, 0, 0])\n",
        "            print(\"Factor\", se, sb)\n",
        "        #print(\"Yfactor:\")\n",
        "        #plt.plot(yfactor[0, 0, 0, :, 0, 0])\n",
        "        #plt.show()\n",
        "#\n",
        "        #print(\"edge loss relative factor:\", elrs.numpy())\n",
        "\n",
        "    def getw(self):\n",
        "        return (\n",
        "            tilop(self.scale_exponent),\n",
        "            tilop(self.scale_beta),\n",
        "            (self.edge_loss_relative_factor),\n",
        "            self.scale_mean,\n",
        "            self.scale_sigma,\n",
        "            tilop(self.orientation_gain),\n",
        "            tilop(self.orientation_loss),\n",
        "            self.yfactor,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Flat query coordinates: <1, n_query_coordinates, 4>\n",
        "        # We need to convert <b, s, o, y, x, d> \n",
        "        original_sums, diffs = inputs\n",
        "\n",
        "        se, sb, elrs, mu, sigma, og, ol, yfactor = self.getw()\n",
        "\n",
        "        horizontal_gap_gains = (tf.nn.relu(diffs[:, :, 0:1, :, :, :]) + eps) * og[:, :, 0:1, :, :, :]\n",
        "        vertical_gap_gains = (tf.nn.relu(diffs[:, :, 1:, :, :, :]) + eps) * 0. # (og[:, :, 1:, :, :, :]) # -og\n",
        "        gap_gains = (eps + tf.concat([horizontal_gap_gains, vertical_gap_gains], axis=2)) ** se\n",
        "\n",
        "        horizontal_edge_losses = (tf.nn.relu(-diffs[:, :, 0:1, :, :, :]) + eps) * (ol[:, :, 0:1, :, :, :]) #(-1.) # -ol\n",
        "        vertical_edge_losses = (tf.nn.relu(-diffs[:, :, 1:, :, :, :]) + eps) * 0. #(ol[:, :, 1:, :, :, :]) # -ol\n",
        "        edge_losses = (eps + tf.concat([horizontal_edge_losses, vertical_edge_losses], axis=2)) ** sb\n",
        "\n",
        "        # We don't want \n",
        "        # Small gaps are actually good, because they separate the two letters.\n",
        "        # It's not clear how exactly they accomplish that.\n",
        "\n",
        "        penalties = (\n",
        "          ((gap_gains + eps) + (edge_losses + eps)) / (tf.reduce_sum(tf.abs(edge_losses[:, :, :, :, :, 1:2]), axis=[1,2,3,4], keepdims=True)) # Try to discourage CSF from just shrinking it\n",
        "        )\n",
        "        return penalties\n",
        "\n",
        "class DistanceEstimator(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DistanceEstimator, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, x = inputs\n",
        "        xdelta = (x[:, 1:] - x[:, :-1])\n",
        "        ydelta = (y[:, 1:] - y[:, :-1]) # Positive when upward\n",
        "    \n",
        "        yrange = (tf.reduce_max(y, axis=[1], keepdims=True) - tf.reduce_min(y, axis=[1], keepdims=True)) + eps\n",
        "        estimate_validities = nd_softmax(1e3 * y/yrange, axis=[1])\n",
        "        estimated_distances = tf.reduce_sum(estimate_validities * x, axis=[1], name='estimated_distances')\n",
        "        return estimated_distances\n",
        "\n",
        "def get_model():\n",
        "    shifted_gd1_filtered_images = tf.keras.Input(shape=full_shape, name='shifted_gd1_filtered_images', dtype=tf.complex64)\n",
        "    shifted_gd2_filtered_images = tf.keras.Input(shape=full_shape, name='shifted_gd2_filtered_images', dtype=tf.complex64)\n",
        "\n",
        "    # Go from <b, s, o, h, w, d> to <b, s, o, p, h, w, d>\n",
        "    pr_gd1 = rectify_phases(shifted_gd1_filtered_images)\n",
        "    pr_gd2 = rectify_phases(shifted_gd2_filtered_images)\n",
        "    pr_pair = rectify_phases(shifted_gd1_filtered_images + shifted_gd2_filtered_images)\n",
        "\n",
        "    # Then, exponentiate with a power.\n",
        "    exp = Exponentiate()\n",
        "    pr_p_gd1 = tf.identity(exp([pr_gd1, tf.abs(shifted_gd1_filtered_images)]), name=\"pr_p_gd1\")\n",
        "    pr_p_gd2 = tf.identity(exp([pr_gd2, tf.abs(shifted_gd2_filtered_images)]), name=\"pr_p_gd2\")\n",
        "    pr_p_pair = tf.identity(exp([pr_pair, tf.abs(shifted_gd1_filtered_images + shifted_gd2_filtered_images)]), name=\"pr_p_pair\")\n",
        "\n",
        "    # Then, calculate the normalization pools\n",
        "    np = NormalizationPool()\n",
        "    pr_np_gd1 = tf.identity(np(pr_p_gd1), name=\"pr_np_gd1\")\n",
        "    pr_np_gd2 = tf.identity(np(pr_p_gd2), name=\"pr_np_gd2\")\n",
        "    pr_np_pair = tf.identity(np(pr_p_pair), name=\"pr_np_pair\")\n",
        "\n",
        "    # Then, perform the divisive normalization\n",
        "    dn = DivisiveNormalization()\n",
        "    pr_dn_gd1 = tf.identity(dn([pr_p_gd1, pr_np_gd1]), name=\"pr_dn_gd1\")\n",
        "    pr_dn_gd2 = tf.identity(dn([pr_p_gd2, pr_np_gd2]), name=\"pr_dn_gd2\")\n",
        "    pr_dn_pair = tf.identity(dn([pr_p_pair, pr_np_pair]), name=\"pr_dn_pair\")\n",
        "\n",
        "    # Then, compute the absolute energy values, because that's what we care about.\n",
        "    ba = BiasedAbs()\n",
        "    apply_csf = ApplyCsf()\n",
        "    penalize = PenalizeZero()\n",
        "\n",
        "    #eba_gd1 = ba(pr_dn_gd1) #((ba(shifted_gd1_filtered_images)))\n",
        "    #eba_gd2 = ba(pr_dn_gd2) #((ba(shifted_gd2_filtered_images)))\n",
        "    #eba_pair = tf.identity(ba(pr_dn_pair), \"bapr_dn_pair\") #((ba(shifted_gd1_filtered_images + shifted_gd2_filtered_images)))\n",
        "\n",
        "    eba_gd1 = ba(shifted_gd1_filtered_images)\n",
        "    eba_gd2 = ba(shifted_gd2_filtered_images)\n",
        "    eba_pair = ba(shifted_gd1_filtered_images + shifted_gd2_filtered_images)\n",
        "\n",
        "    #e_dn_gd1 = tf.identity(apply_csf(dn([eba_gd1, eba_gd1])), \"e_dn_gd1\")\n",
        "    #e_dn_gd2 = tf.identity(apply_csf(dn([eba_gd2, eba_gd2])), \"e_dn_gd2\")\n",
        "    #e_dn_pair = tf.identity(apply_csf(dn([eba_pair, eba_pair])), \"e_dn_pair\")\n",
        "    e_dn_gd1 = tf.identity(apply_csf(eba_gd1), \"e_dn_gd1\")\n",
        "    e_dn_gd2 = tf.identity(apply_csf(eba_gd2), \"e_dn_gd2\")\n",
        "    e_dn_pair = tf.identity(apply_csf(eba_pair), \"e_dn_pair\")\n",
        "\n",
        "    # Then, compute the differences between pair and (gd1 + gd2)\n",
        "\n",
        "    original_sums = tf.identity(e_dn_gd1 + e_dn_gd2, name=\"original_sums\")\n",
        "    diffs = tf.identity(e_dn_pair - original_sums, name=\"diffs\")\n",
        "    penalties = tf.identity(penalize([original_sums, diffs]), name=\"total_pixel_penalties\")\n",
        "\n",
        "    # Not used for anything\n",
        "    sample_distances = tf.keras.Input(shape=(n_sample_distances), name='sample_distances')\n",
        "    pair_images = tf.keras.Input(shape=(box_height, box_width, n_sample_distances), name='pair_images')\n",
        "    zero_indices = tf.keras.Input(shape=(), name='zero_indices')\n",
        "\n",
        "    return tf.keras.Model(inputs=[shifted_gd1_filtered_images,\n",
        "                                  shifted_gd2_filtered_images,\n",
        "                                  sample_distances, pair_images, zero_indices],\n",
        "                                  outputs=(penalties))\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(_target, penalties):\n",
        "    d = tf.reduce_sum(penalties, axis=[1, 2, 3, 4]) + eps\n",
        "\n",
        "    if False:\n",
        "        # Old zero-finding model, which minimizes the vertical distance from 0, plus some extra constraints for monotonicity\n",
        "        total_gap_gains = tf.reduce_sum(tf.nn.relu(penalties), axis=[1, 2, 3, 4])\n",
        "        total_edge_losses = tf.reduce_sum(tf.nn.relu(-penalties), axis=[1, 2, 3, 4])\n",
        "        gap_gain_increase = 2 + tf.nn.elu(total_gap_gains[:, 0] - total_gap_gains[:, 1]) + tf.nn.elu(total_gap_gains[:, 1] - total_gap_gains[:, 2])\n",
        "        edge_loss_decrease = 2 + tf.nn.elu(total_edge_losses[:, 1] - total_edge_losses[:, 0]) + tf.nn.elu(total_edge_losses[:, 2] - total_edge_losses[:, 1])\n",
        "        first_negative = 1 + tf.nn.elu(d[:, 0])\n",
        "        last_positive = 1 + tf.nn.elu(-d[:, 2])\n",
        "        first_increase = 1 + tf.nn.elu(d[:, 0] - d[:, 1])\n",
        "        last_increase = 1 + tf.nn.elu(d[:, 1] - d[:, 2])\n",
        "    \n",
        "        l = tf.identity(1000 * d[:, 1]**2 + first_negative + last_positive + first_increase + last_increase + gap_gain_increase + edge_loss_decrease, name=\"losses\") # Gap is negative\n",
        "        return tf.reduce_sum(l) \n",
        "    elif True: # newer zero-finding model, which tries to find the actual most likely x-zero-crossing\n",
        "        first_negative = tf.nn.relu(d[:, 0]) * 100\n",
        "        last_positive = tf.nn.relu(-d[:, 2]) * 100\n",
        "        constrained_slopes = (d[:, 1:] - d[:, 0:-1]) / 2 # right now delta-x is equal spacing\n",
        "        predicted_zeros = np.array([-2, 0]) - d[:, 0:-1] / constrained_slopes\n",
        "\n",
        "        first_crosses_up = tf.nn.sigmoid(1000 * d[:, 1]) * tf.nn.sigmoid(-1000 * d[:, 0])\n",
        "        second_crosses_up = tf.nn.sigmoid(1000 * d[:, 2]) * tf.nn.sigmoid(-1000 * d[:, 1])\n",
        "        first_above_zero = tf.nn.sigmoid(1000 * d[:, 0])\n",
        "        last_below_zero = tf.nn.sigmoid(-1000 * d[:, 2])\n",
        "        relevance_first = tf.nn.tanh(1000 * (first_crosses_up + first_above_zero))\n",
        "        relevance_last = tf.nn.tanh(1000 * (second_crosses_up + last_below_zero))\n",
        "\n",
        "        predicted_zero = (relevance_first * predicted_zeros)[:, 0] + (relevance_last * predicted_zeros)[:, 1]\n",
        "        return tf.reduce_sum(predicted_zero**2) # + first_negative + last_positive)\n",
        "    else: # Penalize based on whether or not the center one is the minimum\n",
        "        first_down = tf.nn.softplus(d[:, 1] - d[:, 0])  # 0.1 - 6.6\n",
        "        second_up = tf.nn.softplus(d[:, 1] - d[:, 2])\n",
        "        # It's only good when both \n",
        "        return tf.reduce_sum(first_down * tf.abs(first_down) + second_up * tf.abs(second_up))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCnQjSuTT_W",
        "colab_type": "text"
      },
      "source": [
        "## Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAaOa_qnTWC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.patches import Circle\n",
        "\n",
        "class MonitorProgressCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data_generator):\n",
        "        self.data_generator = data_generator\n",
        "        self.model_inputs = [l.input for l in model.layers if isinstance(l, tf.keras.layers.InputLayer)]\n",
        "        #print(\"MODEL INPUTS ARE\", [(l.name, l.input.shape) for l in model.layers if isinstance(l, tf.keras.layers.InputLayer)])\n",
        "        self.current_data = None\n",
        "\n",
        "    def get_val(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        #print(\"getting value\", l.name, l.output.shape)\n",
        "        #print(\"values\", tf.keras.backend.function(self.model_inputs, [l.output])(self.current_data))\n",
        "        output = tf.keras.backend.function(self.model_inputs, [l.output])(self.current_data)[0]\n",
        "        return output\n",
        "\n",
        "    def get_weights(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        return l.get_weights()\n",
        "\n",
        "    def print_weights(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        l.print_weights()\n",
        "\n",
        "    def on_test_batch_begin(self, batch_index, logs=None):\n",
        "        dataset = self.data_generator[batch_index]\n",
        "        current_data = dataset[0]\n",
        "        self.current_data = current_data\n",
        "        shifted_gd1_d1_filtered_images, shifted_gd2_d1_filtered_images, sample_distances, pair_images, zero_indices = current_data\n",
        "\n",
        "        iix = 0\n",
        "        \n",
        "        if False:\n",
        "            plt.imshow(pair_images[iix, :, :, 0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(pair_images[iix, :, :, zero_indices[iix]])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(pair_images[iix, :, :, 2])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "        if True:\n",
        "            self.print_weights(\"apply_csf\")\n",
        "            #self.print_weights(\"exponentiate\")\n",
        "            #self.print_weights(\"normalization_pool\")\n",
        "            #self.print_weights(\"divisive_normalization\")\n",
        "            self.print_weights(\"biased_abs\")\n",
        "            self.print_weights(\"penalize_zero\")\n",
        "\n",
        "        if False:\n",
        "            e_gd1 = self.get_val(\"e_gd1\")\n",
        "            e_pair = self.get_val(\"e_pair\")\n",
        "\n",
        "            i_gd1 = self.get_val(\"shifted_gd1_filtered_images\")\n",
        "            i_gd2 = self.get_val(\"shifted_gd2_filtered_images\")\n",
        "            i_total_angle = tf.math.angle(i_gd1 + i_gd2)\n",
        "\n",
        "            dn_gd1 = self.get_val(\"e_dn_gd1\")\n",
        "            dn_gd2 = self.get_val(\"e_dn_gd2\")\n",
        "            dn_pair = self.get_val(\"e_dn_pair\")\n",
        "            diffs = self.get_val(\"diffs\")\n",
        "            for si in range(n_sizes):\n",
        "                fig, ax = plt.subplots(nrows=1, ncols=5, gridspec_kw={'wspace':0, 'hspace':0}, figsize=(4 * 4 * box_width / 100, 4 * 1 * box_height / 100))\n",
        "                ax[0].imshow(tf.math.angle(i_gd1)[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[1].imshow(i_total_angle[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[2].imshow((dn_gd1 + dn_gd2)[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[3].imshow(dn_pair[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[4].imshow(diffs[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            increases = self.get_val(\"increases\")\n",
        "            dsvps = self.get_val(\"dsvps\")\n",
        "            #print(\"DPSS:\", dsvps)\n",
        "            pcs = self.get_val(\"total_penalties\")\n",
        "            for pix in range(batch_size):\n",
        "                plt.plot(np.arange(n_sample_distances) - zero_indices[pix], pcs[pix, :])\n",
        "                plt.plot(0.5 + np.arange(n_sample_distances - 1) - zero_indices[pix], dsvps[pix, :])\n",
        "                plt.plot(0.5 + np.arange(n_sample_distances - 1) - zero_indices[pix], increases[pix, :])\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            p = self.get_val(\"pr_dn_pair\")\n",
        "            pa = tf.math.angle(tf.complex(p[:, :, :, 0, :, :, :] - p[:, :, :, 2, :, :, :], p[:, :, :, 1, :, :, :] - p[:, :, :, 3, :, :, :]))\n",
        "            pv = tf.abs(tf.complex(p[:, :, :, 0, :, :, :] - p[:, :, :, 2, :, :, :], p[:, :, :, 1, :, :, :] - p[:, :, :, 3, :, :, :]))\n",
        "            for si in range(n_sizes):\n",
        "                plt.imshow((pa)[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                plt.imshow((pv)[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            if False:\n",
        "                va = self.get_val(\"pr_np_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    for pi in range(4):\n",
        "                        ax[si, pi].imshow(va[iix, si, 0, pi, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                        ax[si, pi].set_xticklabels([])\n",
        "                        ax[si, pi].set_yticklabels([])\n",
        "                plt.show()\n",
        "    \n",
        "                va = self.get_val(\"pr_dn_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    for pi in range(4):\n",
        "                        ax[si, pi].imshow(va[iix, si, 0, pi, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                        ax[si, pi].set_xticklabels([])\n",
        "                        ax[si, pi].set_yticklabels([])\n",
        "                plt.show()\n",
        "    \n",
        "                va = self.get_val(\"e_dn_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=1,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    ax[si].imshow(va[iix, si, 0, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                    ax[si].set_xticklabels([])\n",
        "                    ax[si].set_yticklabels([])\n",
        "                plt.show()\n",
        "\n",
        "            va = self.get_val(\"e_dn_pair\")\n",
        "            pp = self.get_val(\"e_dn_gd1\") + self.get_val(\"e_dn_gd2\")\n",
        "            size_factor = 5\n",
        "            #fig, ax = plt.subplots(nrows=n_sizes, ncols=1,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "            for si in range(n_sizes):\n",
        "                print(\"size\", si, \"pair then originalsums\")\n",
        "                plt.imshow(va[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                plt.imshow(pp[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            if True:\n",
        "                print(\"Normalization pool, pair, 2, 0:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_np_pair\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Normalizataion pool, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_np_pair\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Divisive Normalization, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_dn_pair\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Divisive normalization, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_dn_pair\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Original sums, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"original_sums\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Original sums, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"original_sums\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Diffs, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"diffs\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "        if False:\n",
        "            diffs = tf.math.real(self.get_val(\"diffs\"))\n",
        "            for di in range(n_sample_distances):\n",
        "                for si in range(n_sizes):\n",
        "                    print(\"distance\", di, \"size\", si)\n",
        "                    plt.imshow(diffs[iix, si, 0, :, :, di])\n",
        "                    plt.colorbar()\n",
        "                    plt.show()\n",
        "        if True:\n",
        "\n",
        "            if False:\n",
        "                # Visualize the normalization pools for each size/orientation with and without phase congruency\n",
        "                e_dn_gd1 = tf.reduce_sum(self.get_val(\"e_dn_gd1\"), axis=[2], keepdims=True)\n",
        "                e_dn_gd2 = tf.reduce_sum(self.get_val(\"e_dn_gd2\"), axis=[2], keepdims=True)\n",
        "                e_dn_pair = tf.reduce_sum(self.get_val(\"e_dn_pair\"), axis=[2], keepdims=True)\n",
        "    \n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=5,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * 5 * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                vm = tf.reduce_max(e_dn_pair[iix, :, :, :, :, zero_indices[iix]])\n",
        "                for si in range(n_sizes):\n",
        "                    ax[si, 0].imshow(e_dn_gd1[iix, si, 0, :, :, zero_indices[iix]], vmin=0, vmax=vm)\n",
        "                    ax[si, 0].set_xticklabels([])\n",
        "                    ax[si, 0].set_yticklabels([])\n",
        "                    ax[si, 1].imshow(e_dn_gd2[iix, si, 0, :, :, zero_indices[iix]], vmin=0, vmax=vm)\n",
        "                    ax[si, 1].set_xticklabels([])\n",
        "                    ax[si, 1].set_yticklabels([])\n",
        "                    ax[si, 2].imshow((e_dn_gd1 + e_dn_gd2)[iix, si, 0, :, :, zero_indices[iix]], vmin=0, vmax=vm)\n",
        "                    ax[si, 2].set_xticklabels([])\n",
        "                    ax[si, 2].set_yticklabels([])\n",
        "                    ax[si, 3].imshow(e_dn_pair[iix, si, 0, :, :, zero_indices[iix]], vmin=0, vmax=vm)\n",
        "                    ax[si, 3].set_xticklabels([])\n",
        "                    ax[si, 3].set_yticklabels([])\n",
        "                    ax[si, 4].imshow((e_dn_pair - e_dn_gd1 - e_dn_gd2)[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                    ax[si, 4].set_xticklabels([])\n",
        "                    ax[si, 4].set_yticklabels([])\n",
        "                plt.show()\n",
        "\n",
        "            print(\"Diffs, pair, total:\")\n",
        "            diffs = tf.math.real(self.get_val(\"diffs\"))\n",
        "            plt.imshow(-tf.reduce_sum(diffs[iix, :, :, :, :, zero_indices[iix]], axis=[0, 1]))\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"Penalties:\")\n",
        "            pixel_penalties = tf.math.real(self.get_val(\"total_pixel_penalties\"))\n",
        "            sigmas = get_sigmas()\n",
        "\n",
        "            plt.plot(sigmas, np.zeros_like(sigmas), color='k')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.show()\n",
        "\n",
        "            plt.plot(sigmas, np.zeros_like(sigmas), color='k')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.show()\n",
        "\n",
        "            d = tf.reduce_sum(pixel_penalties, [1,2,3,4])\n",
        "    \n",
        "            first_negative = tf.nn.relu(d[:, 0]) * 100\n",
        "            last_positive = tf.nn.relu(-d[:, 2]) * 100\n",
        "            constrained_slopes = (d[:, 1:] - d[:, 0:-1]) / 2 # right now delta-x is equal spacing\n",
        "            predicted_zeros = np.array([-2, 0]) - d[:, 0:-1] / constrained_slopes\n",
        "    \n",
        "            first_crosses_up = tf.nn.sigmoid(1000 * d[:, 1]) * tf.nn.sigmoid(-1000 * d[:, 0])\n",
        "            second_crosses_up = tf.nn.sigmoid(1000 * d[:, 2]) * tf.nn.sigmoid(-1000 * d[:, 1])\n",
        "            first_above_zero = tf.nn.sigmoid(1000 * d[:, 0])\n",
        "            last_below_zero = tf.nn.sigmoid(-1000 * d[:, 2])\n",
        "            relevance_first = tf.nn.tanh(1000 * (first_crosses_up + first_above_zero))\n",
        "            relevance_last = tf.nn.tanh(1000 * (second_crosses_up + last_below_zero))\n",
        "            print(\"relevance first\", relevance_first, \"relevance last\", relevance_last, \"firstcrosses up or above zero:\", first_crosses_up, first_above_zero, \"lastcrossesup or below zero\", second_crosses_up, last_below_zero)\n",
        "\n",
        "            predicted_zero = (relevance_first * predicted_zeros)[:, 0] + (relevance_last * predicted_zeros)[:, 1]\n",
        "\n",
        "            print(\"predicted zero\", predicted_zero, \"squared:\", predicted_zero**2, \"loss:\", predicted_zero**2 + first_negative + last_positive)\n",
        "\n",
        "            for di in range(n_sample_distances):\n",
        "                print(\"DISTANCE INDEX\", di)\n",
        "                if (di == zero_indices[iix]):\n",
        "                    print(\"best distance:\")\n",
        "                vm = max(tf.reduce_max(pixel_penalties[iix, :, :, :, :, di]), -tf.reduce_min(pixel_penalties[iix, :, :, :, :, di]))\n",
        "                print(\"max:\", vm, \"sum:\", tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di]))\n",
        "                if True:\n",
        "                    # Show the image with penalties overlaid:\n",
        "                    plt.imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                    plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di], [0, 1]), alpha=0.7)\n",
        "                    plt.colorbar()\n",
        "                    plt.show()\n",
        "                if di == 1 and False:\n",
        "                    for si in range(n_sizes):\n",
        "                        print(\"SIZE\", si)\n",
        "                        fig, ax = plt.subplots(1, 2,  gridspec_kw = {'wspace':0, 'hspace':0},  figsize=(5 * 2 * box_width / 100, 5 * box_height / 100))\n",
        "                        ax[0].imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                        pp = tf.reduce_sum(pixel_penalties[iix, si, :, :, :, di], [0])\n",
        "                        aim = ax[0].imshow(pp, alpha=0.7)\n",
        "                        max_y, max_x = np.unravel_index(pp.numpy().argmax(), pp.shape)\n",
        "                        min_y, min_x = np.unravel_index(pp.numpy().argmin(), pp.shape)\n",
        "                        ax[0].add_patch(Circle((max_x,max_y),sigmas[si]*4,edgecolor='w',facecolor=None,fill=False))\n",
        "                        ax[0].add_patch(Circle((min_x,min_y),sigmas[si]*4,edgecolor='w',facecolor=None,fill=False))\n",
        "                        fig.colorbar(aim, ax=ax[0])\n",
        "                        ax[1].imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                        dim = ax[1].imshow(tf.reduce_sum(diffs[iix, si, :, :, :, zero_indices[iix]], axis=[0]))\n",
        "                        fig.colorbar(dim, ax=ax[1])\n",
        "                        plt.show()\n",
        "\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di], [2, 3]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "        if False:\n",
        "            print(\"Sigmas\")\n",
        "            sigmas = self.get_weights(\"SpatialAverage\")[0]\n",
        "            plt.plot(sigmas[0, 0, :, 0, 0])\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            print(\"After s/o dense convolution: lines, edges\")\n",
        "            blurred = self.get_val(\"SpatialAverage\")\n",
        "            subbed = self.get_val(\"pow\")\n",
        "            hra_total = self.get_val(\"hr_atotal\")\n",
        "            size_factor = 2\n",
        "            fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "            for si in range(n_sizes):\n",
        "                ax[si, 0].imshow(tf.reduce_sum(abstotal, axis=[2])[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 0].set_xticklabels([])\n",
        "                ax[si, 0].set_yticklabels([])\n",
        "                ax[si, 1].imshow(blurred[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 1].set_xticklabels([])\n",
        "                ax[si, 1].set_yticklabels([])\n",
        "                ax[si, 2].imshow(subbed[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 2].set_xticklabels([])\n",
        "                ax[si, 2].set_yticklabels([])\n",
        "                ax[si, 3].imshow(hra_total[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 3].set_xticklabels([])\n",
        "                ax[si, 3].set_yticklabels([])\n",
        "\n",
        "                print(\"size\", si, tf.reduce_sum(hra_total[iix, si, :, :, zero_indices[iix]]))\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            hravars = self.get_weights(\"hr_atotal\")\n",
        "            print(\"hravars\", hravars)\n",
        "            print(\"Hyperbolic ratio variables.\")\n",
        "            print(\"Exponents\")\n",
        "            plt.plot(tf.nn.softplus(hravars[0][0, :, 0, 0, 0]))\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"Alphas\")\n",
        "            plt.plot(tf.nn.softplus(hravars[1][0, :, 0, 0, 0]))\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"m-scale\")\n",
        "            plt.plot(hravars[2][0, :, 0, 0, 0])\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "    \n",
        "            print(\"Total for each pixel\")\n",
        "            out = self.get_val(\"Sum_1\")\n",
        "            plt.imshow(out[iix, :, :, 0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(out[iix, :, :, zero_indices[iix]])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(out[iix, :, :, -1])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            #for i in range(n_sample_distances):\n",
        "            #    plt.imshow(tf.reduce_sum(out, axis=[3,4], keepdims=True)[iix, :, :, 0, 0, i])\n",
        "            #    plt.colorbar()\n",
        "            #    plt.show()\n",
        "            #    print(\"Index:\", i, \"total is\", tf.reduce_sum(out, axis=[1,2,3,4])[iix, i])\n",
        "        if False:\n",
        "            # Display what the penalties look like\n",
        "            pass\n",
        "\n",
        "        if True:\n",
        "            self.print_weights(\"penalize_zero\")\n",
        "\n",
        "        if False:             \n",
        "            print(\"Diglyphiness values\")\n",
        "            out = self.get_val(\"Sum\")\n",
        "            pred = self.get_val(\"distance_estimator\")\n",
        "            print(\"OUT PRED\", out.shape, pred.shape)\n",
        "            plt.plot(sample_distances[iix, :], out[iix, :])\n",
        "            print(\"Pred distance is\", pred[iix], \"correct distance is\", sample_distances[iix, zero_indices[iix]])\n",
        "            plt.scatter([pred[iix]], [0])\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIbPWIeyH2Xh",
        "colab_type": "text"
      },
      "source": [
        "## Model fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRnxgssIHQGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "ig = InputGenerator(batch_size)\n",
        "if True:\n",
        "    model = get_model()\n",
        "    model.compile(loss=compute_loss,\n",
        "                optimizer=tf.keras.optimizers.Adagrad(0.05))\n",
        "    #model.summary()\n",
        "    history = model.fit_generator(ig,\n",
        "                                callbacks=[MonitorProgressCallback(ig)],\n",
        "                                validation_data=ig,\n",
        "                                validation_steps=1,\n",
        "                                validation_freq=40,\n",
        "                                epochs=1000,\n",
        "                                steps_per_epoch=20, use_multiprocessing=False)\n",
        "ig.kill()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4Pep9M0R2N",
        "colab_type": "text"
      },
      "source": [
        "## Use trained model to predict letter distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7mptUCT0QmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distances = [-3, 0, 2, 4, 6, 8, 10, 12, 15, 20]\n",
        "distances = [0, 8, 16]\n",
        "predicted_kerning_dict = {}\n",
        "glyph_data = {}\n",
        "for glyph_char in tqdm(glyph_char_list):\n",
        "    glyph_data[glyph_char] = get_glyph_data_with_filtered_as_dict(glyph_char)\n",
        "\n",
        "for gl in glyph_char_list:\n",
        "    for gr in glyph_char_list:\n",
        "        with tf.device('/CPU:0'):\n",
        "            cpd = shift_and_overlay_pair_data(glyph_data[gl], glyph_data[gr], distances)\n",
        "            if False:\n",
        "                print(\"dist 0\")\n",
        "                plt.imshow(cpd['pair_images'][:, :, 0])\n",
        "                plt.show()\n",
        "                plt.imshow(cpd['pair_images'][:, :, 1])\n",
        "                plt.show()\n",
        "                plt.imshow(cpd['pair_images'][:, :, 2])\n",
        "                plt.show()\n",
        "                print(cpd['pair_images'][None, :, :, :].shape) \n",
        "            inputs = [\n",
        "                cpd['shifted_gd1_d1_filtered_images'][None, :, :, :, :, :],\n",
        "                cpd['shifted_gd2_d1_filtered_images'][None, :, :, :, :, :],\n",
        "                cpd['sample_distances'][None, :],\n",
        "                cpd['pair_images'][None, :, :, :],\n",
        "                np.array(cpd['zero_index'])[None],\n",
        "            ]   \n",
        "\n",
        "            d = tf.reduce_sum(model.predict(inputs), axis=[1,2,3,4])\n",
        "            print(\"RESULT:\", gl, gr, d[0, :].numpy())\n",
        "\n",
        "            first_negative = tf.nn.relu(d[:, 0]) * 100\n",
        "            last_positive = tf.nn.relu(-d[:, 2]) * 100\n",
        "            constrained_slopes = (d[:, 1:] - d[:, 0:-1]) / 8 # right now delta-x is equal spacing\n",
        "            predicted_zeros = np.array([-8, 0]) - d[:, 0:-1] / constrained_slopes\n",
        "    \n",
        "            first_crosses_up = tf.nn.sigmoid(1000 * d[:, 1]) * tf.nn.sigmoid(-1000 * d[:, 0])\n",
        "            second_crosses_up = tf.nn.sigmoid(1000 * d[:, 2]) * tf.nn.sigmoid(-1000 * d[:, 1])\n",
        "            first_above_zero = tf.nn.sigmoid(1000 * d[:, 0])\n",
        "            last_below_zero = tf.nn.sigmoid(-1000 * d[:, 2])\n",
        "            relevance_first = tf.nn.tanh(1000 * (first_crosses_up + first_above_zero))\n",
        "            relevance_last = tf.nn.tanh(1000 * (second_crosses_up + last_below_zero))\n",
        "\n",
        "            predicted_zero = (relevance_first * predicted_zeros)[:, 0] + (relevance_last * predicted_zeros)[:, 1]\n",
        "            predicted_kerning_dict[gl, gr] = int(predicted_zero) + 8 - f.minimum_ink_distance(gl, gr)\n",
        "            print(\"Predicted dist,\", gl, gr, \":\", predicted_kerning_dict[gl, gr])\n",
        "            print(\"Predicted zero was:\", predicted_zero, \"mindist:\", f.minimum_ink_distance(gl, gr))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-RZLdLz6Enc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Font scaling factor is\", f.scale_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQ3_GtPPHqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ufo = defcon.Font()\n",
        "extractor.extractUFO(filename, ufo, doKerning=False)\n",
        "\n",
        "print(\"Adding kerning data ...\", gl)\n",
        "import string\n",
        "for gl in string.ascii_lowercase:\n",
        "    ufo.layers.defaultLayer[gl].leftMargin = 0 #lsb\n",
        "    ufo.layers.defaultLayer[gl].rightMargin = 0 #rsb\n",
        "    for gr in string.ascii_lowercase:\n",
        "        ufo.kerning[(gl, gr)] = predicted_kerning_dict[(gl, gr)] / f.scale_factor\n",
        "print(\"Compiling to OTF ...\")\n",
        "otf = compileOTF(ufo)\n",
        "otf.save('example-output.otf')\n",
        "print(\"Compilation done.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}