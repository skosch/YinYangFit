{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YinYangFit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skosch/YinYangFit/blob/master/YinYangFit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XZBAz3JvUbx",
        "colab_type": "text"
      },
      "source": [
        "## Set up TF2 and import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHFF5hat2n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name != '/device:GPU:0':\n",
        "      raise SystemError('GPU device not found')\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    \n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5H5-Eq0vAve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "pi = np.pi\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.ndimage.interpolation import affine_transform\n",
        "import tensorflow as tf\n",
        "import random; random.seed()\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm as tqdm\n",
        "import sys\n",
        "from functools import reduce\n",
        "import random\n",
        "from itertools import cycle, islice, product\n",
        "import operator\n",
        "\n",
        "!pip install --quiet --upgrade git+git://github.com/simoncozens/tensorfont.git\n",
        "!pip install --quiet fonttools\n",
        "!pip install --upgrade git+git://github.com/simoncozens/fontParts.git@d444bde6e2a0adbcd9a16593a615a99823089c70\n",
        "!pip install booleanOperations\n",
        "import fontParts\n",
        "\n",
        "from tensorfont import Font\n",
        "\n",
        "print(\"✓ Dependencies imported.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAPx3wjHvzBt",
        "colab_type": "text"
      },
      "source": [
        "### Download font files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1E26X5XvoAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -q -O OpenSans-Regular.ttf https://github.com/googlefonts/opensans/blob/master/ttfs/OpenSans-Regular.ttf?raw=true\n",
        "#!wget -q -O Roboto.ttf https://github.com/google/fonts/blob/master/apache/roboto/Roboto-Regular.ttf?raw=true\n",
        "#!wget -q -O Roboto.otf https://github.com/AllThingsSmitty/fonts/blob/master/Roboto/Roboto-Regular/Roboto-Regular.otf?raw=true\n",
        "#!wget -q -O DroidSerif.ttf https://github.com/datactivist/sudweb/blob/master/fonts/droid-serif-v6-latin-regular.ttf?raw=true\n",
        "#!wget -q -O CrimsonItalic.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Italic.otf?raw=true\n",
        "#!wget -q -O CrimsonBold.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Bold.otf?raw=true \n",
        "#!wget -q -O CrimsonRoman.otf https://github.com/alif-type/amiri/blob/master/Amiri-Regular.ttf?raw=true\n",
        "\n",
        "!wget -q -O CrimsonRoman.otf https://github.com/skosch/Crimson/blob/master/Desktop%20Fonts/OTF/Crimson-Roman.otf?raw=true\n",
        "print(\"✓ Font file(s) downloaded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3n6gilxXPl",
        "colab_type": "text"
      },
      "source": [
        "## Load font data and set up global parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lod0tNeLxaDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glyph_char_list = \"abcdeghijlmnopqrstuzywvxkf\"\n",
        "#glyph_char_list = \"abgjqrst\"\n",
        "glyph_char_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "#glyph_char_list = \"OO\"\n",
        "\n",
        "# ==== Create Font ====\n",
        "factor = 1.14 #1.539  # This scales the size of everything\n",
        "f = Font(\"CrimsonRoman.otf\", 24 * factor) # Roboto.ttf CrimsonRoman.otf # 34 for lowercase\n",
        "box_height = f.full_height_px\n",
        "box_width = int(141 * factor)\n",
        "box_width += (box_width + 1) % 2\n",
        "print(\"Box size:\", box_height, \"×\", box_width)\n",
        "\n",
        "# 37067520 allocated, maxAllocSize: 873707520\n",
        "\n",
        "batch_size = 1  # must be divisible by 8 to work on TPU\n",
        "n_sample_distances = 3 # should be an odd number\n",
        "\n",
        "n_sizes = 14\n",
        "n_pseudo_orientations = 4\n",
        "n_orientations = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd6aKEDCv3g_",
        "colab_type": "text"
      },
      "source": [
        "## Create Gabor filter bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x7Irsj_wLJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sigmas(skip_scales=0):\n",
        "    sigmas = []\n",
        "    for s in range(n_sizes):\n",
        "        min_sigma = 1.5\n",
        "        max_sigma = box_width / 12\n",
        "        #sigmas.append((max_sigma - min_sigma) * (s + skip_scales)**2 / (n_sizes - 1)**2 + min_sigma)\n",
        "        sigmas.append((max_sigma - min_sigma) * s / n_sizes + min_sigma)\n",
        "    return np.array(sigmas)\n",
        "print(\"Sigmas are\", get_sigmas())\n",
        "\n",
        "def get_3n_filters(skip_scales, display_filters=False):\n",
        "    def rotated_mgrid(oi):\n",
        "        \"\"\"Generate a meshgrid and rotate it by RotRad radians.\"\"\"\n",
        "        rotation = np.array([[ np.cos(pi*oi/n_orientations), np.sin(pi*oi/n_orientations)],\n",
        "                             [-np.sin(pi*oi/n_orientations), np.cos(pi*oi/n_orientations)]])\n",
        "        hh = box_height # / 2\n",
        "        bw = box_width # / 2\n",
        "        y, x = np.mgrid[-hh:hh, -bw:bw].astype(np.float32)\n",
        "        y += 0.5 # 0 if box_height % 2 == 0 else 0.5\n",
        "        x += 0.5 # 0 if box_width % 2 == 0 else 0.5\n",
        "        return np.einsum('ji, mni -> jmn', rotation, np.dstack([x, y]))\n",
        "\n",
        "    def get_filter(sigma, theta):\n",
        "        x, y = rotated_mgrid(theta)\n",
        "\n",
        "        # To minimize ringing etc., we create the filter as is, then run it through the DFT.\n",
        "        a1 = 0.25 # See Georgeson et al. 2007\n",
        "        s1 = sigma #a1 * sigma\n",
        "        d1_space = -np.exp(-(x**2+y**2)/(2*s1**2))*x/(2*pi*s1**4)\n",
        "        d1 = np.fft.fft2(d1_space + 1j * np.zeros_like(d1_space)) #, [box_height, box_width])\n",
        "\n",
        "        # Second derivative:\n",
        "        s2 = sigma #np.sqrt(1. - a1**2) * sigma # See Georgeson et al. (2007)\n",
        "        d2_space = np.exp(-(x**2+y**2)/(2*s2**2))/(2*pi*s2**4) - np.exp(-(x**2+y**2)/(2*s2**2))*x**2/(2*pi*s2**6)\n",
        "        d2 = sigma**1.5 * np.fft.fft2(d2_space + 1j * np.zeros_like(d2_space)) #, [box_height, box_width])\n",
        "\n",
        "        # For now: d1 is complex(-d2,d1)\n",
        "        d1c = 1j * (d2 + 1j*d1) #/ sigma #np.sqrt(sigma)\n",
        "        return (d1c, d2)\n",
        "\n",
        "    d1_bank = np.zeros((n_sizes, n_orientations, 2*box_height, 2*box_width)).astype(np.complex64)\n",
        "    d2_bank = np.zeros((n_sizes, n_orientations, 2*box_height, 2*box_width)).astype(np.complex64)\n",
        "\n",
        "    if display_filters:\n",
        "        sizediv = 60\n",
        "        fig, ax = plt.subplots(nrows=n_sizes*2, ncols=n_orientations, gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(box_width * n_orientations / sizediv, box_height * n_sizes * 2 / sizediv))\n",
        "\n",
        "    sigmas = get_sigmas()\n",
        "    for s in range(n_sizes):\n",
        "        sigma = sigmas[s]\n",
        "        for o in range(n_orientations):\n",
        "            (d1, d2) = get_filter(sigma, o)\n",
        "            if display_filters:\n",
        "                #ax[s*2, o].imshow(np.real(np.fft.ifft2(d1)), cmap=\"inferno\")\n",
        "                ax[s*2, o].imshow(np.abs(np.fft.fftshift(d1)), cmap=\"inferno\")\n",
        "                ax[s*2, o].set_aspect(\"auto\")\n",
        "                ax[s*2, o].set_yticklabels([])\n",
        "                ax[s*2+1, o].imshow(np.imag(np.fft.ifft2(d1)), cmap=\"inferno\")\n",
        "                ax[s*2+1, o].set_aspect(\"auto\")\n",
        "                ax[s*2+1, o].set_yticklabels([])\n",
        "            d1_bank[s, o, :, :] = d1\n",
        "            d2_bank[s, o, :, :] = d2\n",
        "\n",
        "    if display_filters:\n",
        "        plt.show()\n",
        "\n",
        "    return (d1_bank.astype(np.complex64), d2_bank)\n",
        "\n",
        "d1_filter_bank, d2_filter_bank = get_3n_filters(0, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojRjQUML16hz",
        "colab_type": "text"
      },
      "source": [
        "## Rasterize the glyphs into numpy arrays, and extract their ink widths in pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crNhHm8d2RQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_filter_bank(input_image, filter_bank):\n",
        "    \"\"\"\n",
        "    Input image should have dimensions <h, w> or <s, o, h, w> or <b, s, o, h, w, d>.\n",
        "    Filter bank should have dimensions <s, o, h, w>\n",
        "    \"\"\"\n",
        "    if len(input_image.shape) == 2:\n",
        "        bdsohw_input_image = input_image[None, None, None, None, :, :]\n",
        "    elif len(input_image.shape) == 4:\n",
        "        bdsohw_input_image = input_image[None, None, :, :, :, :]\n",
        "    elif len(input_image.shape) == 6:\n",
        "        bdsohw_input_image = tf.einsum(\"bsohwd->bdsohw\", input_image)\n",
        "\n",
        "    # pad image to filter size, which is 2*box_height, 2*box_width (to prevent too much wrapping)\n",
        "    padded_input = tf.pad(bdsohw_input_image, [[0, 0], [0, 0], [0, 0], [0, 0],\n",
        "                            [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "                            [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT')\n",
        "\n",
        "    input_in_freqdomain = tf.signal.fft2d(tf.dtypes.cast(tf.complex(padded_input, tf.zeros_like(padded_input)), tf.complex64))\n",
        "\n",
        "    padded_result = (tf.signal.ifft2d(input_in_freqdomain * filter_bank[None, None, :, :, :, :]))\n",
        "\n",
        "    if len(input_image.shape) == 2:\n",
        "        presult = tf.signal.fftshift(padded_result[0, 0, :, :, :, :], axes=[2, 3])\n",
        "        # Return <s, o, h, w>\n",
        "        return presult[:, :, int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))]\n",
        "    elif len(input_image.shape) == 4:\n",
        "        presult = tf.signal.fftshift(padded_result[0, 0, :, :, :, :], axes=[2, 3])\n",
        "        # Return <s, o, h, w>\n",
        "        return presult[:, :, int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))]\n",
        "    elif len(input_image.shape) == 6:\n",
        "        presult = tf.einsum(\"bdsohw->bsohwd\", tf.signal.fftshift(padded_result, axes=[2, 3]))\n",
        "        return presult[:, :, :, :,\n",
        "                           int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                           int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))]\n",
        "\n",
        "def get_glyph_data_with_filtered_as_dict(glyph_char):\n",
        "    \"\"\"\n",
        "    Returns a dict containing relevant glyph data, including filtered images.\n",
        "    @param glyph_char: string of length 1\n",
        "    \"\"\"\n",
        "    glyph_image = f.glyph(glyph_char).as_matrix(normalize=True).with_padding_to_constant_box_width(box_width).astype(np.float32)\n",
        "\n",
        "    return {\n",
        "        'glyph_char': glyph_char,\n",
        "        'glyph_image': glyph_image,\n",
        "        'glyph_ink_width': f.glyph(glyph_char).ink_width,\n",
        "        'glyph_d1_filtered_images': apply_filter_bank(glyph_image, d1_filter_bank),\n",
        "    }\n",
        "\n",
        "def get_sample_distances_and_translations(gd1, gd2, target_ink_distance):\n",
        "    \"\"\"Returns a list of distances at which the box images have to be shifted left and right before they can be overlaid to sample their normalized interaction\"\"\"\n",
        "    total_width_at_minimum_ink_distance = gd1['glyph_ink_width'] + gd2['glyph_ink_width'] - f.minimum_ink_distance(gd1['glyph_char'], gd2['glyph_char'])\n",
        "    \n",
        "    relative_sample_distances = [0]\n",
        "    simax = int((n_sample_distances - 1)/2)\n",
        "    \n",
        "    pos_ad = +2 #/ 2.\n",
        "    neg_ad = -2 #(target_ink_distance - 2)\n",
        "    next_pos = pos_ad\n",
        "    next_neg = neg_ad\n",
        "    for si in range(simax):\n",
        "        # always append a positive, and then ...\n",
        "        relative_sample_distances.append(next_pos)\n",
        "        next_pos += pos_ad\n",
        "        pos_ad *= 1.\n",
        "        # ... append a negative only if there is room\n",
        "        if target_ink_distance + next_neg >= -3 or True:\n",
        "            relative_sample_distances.append(next_neg)\n",
        "            next_neg += neg_ad\n",
        "        else:\n",
        "            relative_sample_distances.append(next_pos)\n",
        "            next_pos += pos_ad\n",
        "            pos_ad *= 1.\n",
        "    \n",
        "    relative_sample_distances.sort()\n",
        "    \n",
        "    zero_index_val = relative_sample_distances.index(0)\n",
        "    desired_penalty_slope_sign = np.ones((n_sample_distances - 1))\n",
        "    desired_penalty_slope_sign[:zero_index_val] = -1\n",
        "\n",
        "    sample_distances = (np.array(relative_sample_distances) + target_ink_distance)\n",
        "    sample_distances_left = np.ceil(sample_distances / 2)\n",
        "    sample_distances_right = np.floor(sample_distances / 2)\n",
        "    \n",
        "    total_ink_width = gd1['glyph_ink_width'] + gd2['glyph_ink_width']\n",
        "    ink_width_left = np.floor(total_ink_width / 4)\n",
        "    ink_width_right = np.ceil(total_ink_width / 4)\n",
        "    \n",
        "    left_translations = (-(np.ceil(total_width_at_minimum_ink_distance/2) + sample_distances_left) - (-ink_width_left)).astype(np.int32)\n",
        "    right_translations = ((np.floor(total_width_at_minimum_ink_distance/2) + sample_distances_right) - ink_width_right).astype(np.int32)\n",
        "\n",
        "    return {\n",
        "        'sample_distances': sample_distances,\n",
        "        'relative_sample_distances': relative_sample_distances,\n",
        "        'left_translations': left_translations,\n",
        "        'right_translations': right_translations,\n",
        "        'zero_index': zero_index_val,\n",
        "        'desired_penalty_slope_sign': desired_penalty_slope_sign,\n",
        "    }\n",
        "\n",
        "def shift_sohw1_into_sohwd(input_images, translations):\n",
        "    \"\"\"Shifts images to left/right and back-fills with zeros.\n",
        "    @param images: <sizes, orientations, height, width, 1>\n",
        "    @param translations: <len(translations)>\n",
        "    @output        <sizes, orientations, height, width, len(translations)>\n",
        "    \"\"\"\n",
        "    images = tf.tile(input_images, [1, 1, 1, 1, translations.shape[0]]) # create len(shifts) channel copies\n",
        "    fill_constant = 0\n",
        "    left = tf.maximum(0, tf.reduce_max(translations)) # positive numbers are shifts to the right, for which we need to add zeros on the left\n",
        "    right = -tf.minimum(0, tf.reduce_min(translations)) # negative numbers are shifts to the left, for which we need to add zeros on the right\n",
        "    left_mask = tf.ones(shape=(tf.shape(images)[0], tf.shape(images)[1], tf.shape(images)[2], left, tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    right_mask = tf.ones(shape=(tf.shape(images)[0], tf.shape(images)[1], tf.shape(images)[2], right, tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    padded_images = tf.concat([left_mask, images, right_mask], axis=3) # pad on axis 3 (i.e. width-wise)\n",
        "\n",
        "    # Now that the images are all padded, we need to crop them to implement the shifts.\n",
        "    def crop_image_widthwise(image_and_shift):\n",
        "        image = image_and_shift[0] # sohw\n",
        "        shift = image_and_shift[1] # \n",
        "        return image[:, :, :, left-shift:left-shift+input_images.shape[3]] # positive shift: left-shift\n",
        "\n",
        "    return tf.einsum(\"dsohw->sohwd\", tf.map_fn(\n",
        "        crop_image_widthwise,\n",
        "        (tf.einsum(\"sohwd->dsohw\", padded_images), translations),\n",
        "        dtype=images.dtype))\n",
        "\n",
        "def shift_hwso1_into_hwsod(input_images, translations):\n",
        "    \"\"\"Shifts images to left/right and back-fills with zeros.\n",
        "    @param images: <height, width, sizes, orientations, 1>\n",
        "    @param translations: <len(translations)>\n",
        "    @output        <height, width, sizes, orientations, len(translations)>\n",
        "    \"\"\"\n",
        "    images = tf.tile(input_images, [1, 1, 1, 1, translations.shape[0]]) # create len(shifts) channel copies\n",
        "    fill_constant = 0\n",
        "    left = tf.maximum(0, tf.reduce_max(translations)) # positive numbers are shifts to the right, for which we need to add zeros on the left\n",
        "    right = -tf.minimum(0, tf.reduce_min(translations)) # negative numbers are shifts to the left, for which we need to add zeros on the right\n",
        "    left_mask = tf.ones(shape=(tf.shape(images)[0], left, tf.shape(images)[2], tf.shape(images)[3], tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    right_mask = tf.ones(shape=(tf.shape(images)[0], right, tf.shape(images)[2], tf.shape(images)[3], tf.shape(images)[4]), dtype=images.dtype) * fill_constant\n",
        "    padded_images = tf.concat([left_mask, images, right_mask], axis=1) # pad on axis 2 (i.e. width-wise)\n",
        "\n",
        "    # Now that the images are all padded, we need to crop them to implement the shifts.\n",
        "    def crop_image_widthwise(image_and_shift):\n",
        "        image = image_and_shift[0]\n",
        "        shift = image_and_shift[1]\n",
        "        return image[:, left-shift:left-shift+input_images.shape[1], :, :] # positive shift: left-shift\n",
        "\n",
        "    return tf.einsum(\"dhwso->hwsod\", tf.map_fn(\n",
        "        crop_image_widthwise,\n",
        "        (tf.einsum(\"hwsod->dhwso\", padded_images), translations),\n",
        "        dtype=images.dtype))\n",
        "\n",
        "def shift_and_overlay_pair_data(gd1, gd2):\n",
        "    \"\"\"\n",
        "    Returns a 5D tensor <box_height, box_width, sizes, orientations, distances>.\n",
        "    \"\"\"\n",
        "\n",
        "    target_ink_distance = int(f.pair_distance(gd1['glyph_char'], gd2['glyph_char']) + f.minimum_ink_distance(gd1['glyph_char'], gd2['glyph_char']))\n",
        "    sdt = get_sample_distances_and_translations(gd1, gd2, target_ink_distance)\n",
        "\n",
        "    shifted_gd1_d1_filtered_images = shift_sohw1_into_sohwd(gd1['glyph_d1_filtered_images'][..., None], sdt['left_translations'])\n",
        "    shifted_gd2_d1_filtered_images = shift_sohw1_into_sohwd(gd2['glyph_d1_filtered_images'][..., None], sdt['right_translations'])\n",
        "\n",
        "    # We want to shift both the original images (for display purposes only), as well as the filtered images.\n",
        "    pair_images = (shift_hwso1_into_hwsod(gd1['glyph_image'][..., None, None, None], sdt['left_translations']) + \n",
        "                   shift_hwso1_into_hwsod(gd2['glyph_image'][..., None, None, None], sdt['right_translations']))[:, :, 0, 0, :]\n",
        "    zero_index = sdt['zero_index']\n",
        "    desired_penalty_slope_sign = sdt['desired_penalty_slope_sign']\n",
        "    sample_distances = sdt['sample_distances']\n",
        "\n",
        "    return  {\n",
        "        'shifted_gd1_d1_filtered_images': shifted_gd1_d1_filtered_images,\n",
        "        'shifted_gd2_d1_filtered_images': shifted_gd2_d1_filtered_images,\n",
        "        'ink_distance': target_ink_distance,\n",
        "        'pair_images': pair_images,\n",
        "        'zero_index': zero_index,\n",
        "        'desired_penalty_slope_sign': desired_penalty_slope_sign,\n",
        "        'sample_distances': sample_distances,\n",
        "    }\n",
        "\n",
        "class InputGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        print(\"Creating glyph images ...\", flush=True)\n",
        "        self.glyph_data = []\n",
        "        for glyph_char in tqdm(glyph_char_list):\n",
        "            self.glyph_data.append(get_glyph_data_with_filtered_as_dict(glyph_char))\n",
        "        self.n_pairs = len(glyph_char_list) ** 2\n",
        "        self.cached_pair_data = {}\n",
        "\n",
        "    def kill(self):\n",
        "        del self.glyph_data\n",
        "        del self.cached_pair_data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches in the dataset\"\"\"\n",
        "        return math.ceil(self.n_pairs / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return the content of batch idx.\n",
        "        Instead of always providing the same data for batch i,\n",
        "        we just pick batch_size random glyph pairs and return their glyph data.\n",
        "\n",
        "        This is run on the CPU, because otherwise the calculations sit in GPU memory,\n",
        "        are never released, and lead to annoying out-of-memory issues all the time.\n",
        "\n",
        "        Output: \n",
        "        ([shifted_gd1_filtered_images, shifted_gd2_filtered_images, sample_distances], [ink_distance, pair_images, zero_index])\n",
        "        \"\"\"\n",
        "        with tf.device('/CPU:0'):\n",
        "            g_shifted_gd1_d1_filtered_images = []\n",
        "            g_shifted_gd2_d1_filtered_images = []\n",
        "            g_sample_distances = []\n",
        "            g_ink_distance = []\n",
        "            g_pair_images = []\n",
        "            g_zero_index = []\n",
        "            g_desired_penalty_slope_sign = []\n",
        "            for i in range(batch_size):\n",
        "                g1 = random.choice(self.glyph_data)\n",
        "                g2 = random.choice(self.glyph_data)\n",
        "    \n",
        "                #if (g1['glyph_char'] + g2['glyph_char']) not in self.cached_pair_data:\n",
        "                #    self.cached_pair_data[g1['glyph_char'] + g2['glyph_char']] = shift_and_overlay_pair_data(g1, g2)\n",
        "                #cpd = self.cached_pair_data[g1['glyph_char'] + g2['glyph_char']]\n",
        "\n",
        "                # Don't cache the data -- does that save RAM?\n",
        "                cpd = shift_and_overlay_pair_data(g1, g2)\n",
        "\n",
        "                g_shifted_gd1_d1_filtered_images.append(cpd['shifted_gd1_d1_filtered_images'])\n",
        "                g_shifted_gd2_d1_filtered_images.append(cpd['shifted_gd2_d1_filtered_images'])\n",
        "                g_sample_distances.append(cpd['sample_distances'])\n",
        "                g_ink_distance.append(cpd['ink_distance'] * 1.0)\n",
        "                g_pair_images.append(cpd['pair_images'])\n",
        "                g_zero_index.append(cpd['zero_index'])\n",
        "                g_desired_penalty_slope_sign.append(cpd['desired_penalty_slope_sign'])\n",
        "    \n",
        "            inputs = [\n",
        "                tf.stack(g_shifted_gd1_d1_filtered_images),\n",
        "                tf.stack(g_shifted_gd2_d1_filtered_images),\n",
        "                tf.stack(g_sample_distances),\n",
        "                tf.stack(g_pair_images),\n",
        "                tf.stack(g_zero_index),\n",
        "                tf.stack(g_desired_penalty_slope_sign),\n",
        "            ]   \n",
        "            outputs = tf.stack(g_ink_distance)\n",
        "    \n",
        "            return inputs, outputs \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AMRfFWJwNOj",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upbQl3OvHtz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "full_shape = ( n_sizes, n_orientations, box_height, box_width, n_sample_distances)\n",
        "\n",
        "eps = np.finfo(np.float32).tiny\n",
        "\n",
        "@tf.function\n",
        "def nd_softmax(target, axis, name=None):\n",
        "    max_axis = tf.reduce_max(target, axis, keepdims=True)\n",
        "    target_exp = tf.exp(target - max_axis)\n",
        "    normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n",
        "    softmax = target_exp / (normalize + eps)\n",
        "    return softmax\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def tilo(t):\n",
        "    return tf.concat([t[:, :, 0:1, :, :, :], t[:, :, 1:2, :, :, :], t[:, :, 2:3, :, :, :], t[:, :, 1:2, :, :, :]], axis=2)\n",
        "@tf.function\n",
        "def tilop(t):\n",
        "    return tf.nn.softplus(tilo(t))\n",
        "@tf.function\n",
        "def ptilo(t):\n",
        "    return tf.concat([t[:, :, 0:1, :, :, :, :], t[:, :, 1:2, :, :, :, :], t[:, :, 2:3, :, :, :, :], t[:, :, 1:2, :, :, :, :]], axis=2)\n",
        "@tf.function\n",
        "def ptilop(t):\n",
        "    return tf.nn.softplus(ptilo(t))\n",
        "@tf.function\n",
        "def tiloa(t):\n",
        "    return tf.concat([t[:, :, :, :, 0:1, :, :], t[:, :, :, :, 1:2, :, :], t[:, :, :, :, 2:3, :, :], t[:, :, :, :, 1:2, :, :]], axis=4)\n",
        "@tf.function\n",
        "def tiloap(t):\n",
        "    return tf.nn.softplus(tiloa(t))\n",
        "\n",
        "def rectify_phases(inputs):\n",
        "    # Inputs: <b, s, o, h, w, d>\n",
        "    # Output: <b, s, o, p, h, w, d> where p is [0, 1, 2, 3]\n",
        "    return tf.stack([tf.nn.relu(tf.math.real(inputs)),\n",
        "                     tf.nn.relu(tf.math.imag(inputs)),\n",
        "                     tf.nn.relu(-tf.math.real(inputs)),\n",
        "                     tf.nn.relu(-tf.math.imag(inputs))], axis=3)\n",
        "\n",
        "class BiasedAbs(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BiasedAbs, self).__init__(**kwargs)\n",
        "\n",
        "        self.bias_weights = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(0.),\n",
        "                                       name='bias_weights',\n",
        "                                       trainable=True)\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(tf.nn.sigmoid(tilo(self.bias_weights))[0, :, :, 0, 0, 0])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        squashed_bias_weights = tf.nn.sigmoid(tilo(self.bias_weights))\n",
        "        # 0.0 -> abs(real)\n",
        "        # 0.5 -> abs(inputs)\n",
        "        # 1.0 -> abs(imag)\n",
        "        fr = 1. - squashed_bias_weights\n",
        "        fi = squashed_bias_weights\n",
        "        biased_abs = tf.sqrt(eps + fr * tf.math.real(inputs)**2 + fi * tf.math.imag(inputs)**2) * tf.sqrt(2.)\n",
        "        #biased_abs = tf.sqrt(eps + fr * (inputs[:, :, :, 0, :, :, :] - inputs[:, :, :, 2, :, :, :])**2 + fi * (inputs[:, :, :, 1, :, :, :] - inputs[:, :, :, 3, :, :, :])**2) * tf.sqrt(2.)\n",
        "\n",
        "        return biased_abs\n",
        "\n",
        "class NormalizationPool(tf.keras.layers.Layer):\n",
        "    # This layer computes the unscaled normalization pool for each neuron.\n",
        "    # It effectively performs a blur over space, frequency, and orientation, using\n",
        "    # Gaussian blur via 3D FFT (instead of a convolutional layer) followed by\n",
        "    # a matrix multiplication over the orientation axis (arbitrary kernel).\n",
        "    #\n",
        "    # The input is a 7D tensor of reals <b, s, o, p, y, x, d> (scale_index, orientation_index, phase_index, vertical coordinate, horizontal coordinate)\n",
        "    # The output is a 7D tensor of reals <b, s, o, p, y, x, d> (scale_index, orientation_index, phase_index, vertical coordinate, horizontal coordinate)\n",
        "    #\n",
        "    # For more information, see Sawada & Petrov (2017)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NormalizationPool, self).__init__(**kwargs)\n",
        "\n",
        "        self.sigmas = get_sigmas()\n",
        "        self.wavelengths = self.get_wavelengths()\n",
        "        self.r2grid, self.sgrid = self.get_distgrids()\n",
        "\n",
        "        self.spatial_pool_size_factor = self.add_weight(shape=(),\n",
        "                                                        initializer=tf.keras.initializers.Constant(1.),\n",
        "                                                        name='spatial_pool_size_factor',\n",
        "                                                        trainable=True)\n",
        "        self.scale_pool_size_factor = self.add_weight(shape=(),\n",
        "                                                      initializer=tf.keras.initializers.Constant(1.),\n",
        "                                                      name='scale_pool_size_factor',\n",
        "                                                      trainable=True)\n",
        "\n",
        "        self.rescale_factor = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(-0.0),\n",
        "                                       name='npool_rescale_factor',\n",
        "                                       trainable=True)\n",
        "        #self.phase_congruency_coefficient = self.add_weight(shape=(),\n",
        "        #                                                      initializer=tf.keras.initializers.Constant(2.),\n",
        "        #                                                      name=\"phase_congruency_coefficient\",\n",
        "        #                                                      trainable=True)\n",
        "\n",
        "        # If this turns out to be symmetric, replace it with a von-Mises distribution\n",
        "        # factor = exp(1.22 * cos(angle-diff)**2)  -- 1.22 coefficient taken from Sawada\n",
        "        ro = np.eye(n_orientations)\n",
        "        self.orientation_inhibition_matrix = self.add_weight(shape=(n_orientations, n_orientations),\n",
        "                                                             initializer=tf.keras.initializers.Constant(ro),\n",
        "                                                             name='orientation_inhibition_matrix',\n",
        "                                                             trainable=True)\n",
        "        po = 2. - np.eye(4)\n",
        "        self.phase_inhibition_matrix = self.add_weight(shape=(4, 4),\n",
        "                                                             initializer=tf.keras.initializers.Constant(po),\n",
        "                                                             name='phase_inhibition_matrix',\n",
        "                                                             trainable=True)\n",
        "        \n",
        "        # Instead of an orientation inhibition matrix, we use a combined spatial/orientation filter.\n",
        "        # Inhibition should be highest for orientations either in the opposition direction, or in the same direction but\n",
        "        # aligned parallel.\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(\"Spatial pool size factor\", tf.nn.softplus(self.spatial_pool_size_factor.numpy()))\n",
        "        print(\"Scale pool size factor\", tf.nn.softplus(self.scale_pool_size_factor.numpy()))\n",
        "        print(\"Rescale factor\", self.rescale_factor)\n",
        "        print(\"Orientation inhibition matrix\")\n",
        "        plt.imshow(tf.nn.softplus(self.orientation_inhibition_matrix.numpy()))\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        print(\"Phase inhibition matrix\")\n",
        "        plt.imshow(tf.nn.softplus(self.phase_inhibition_matrix.numpy()))\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def get_wavelengths(self):\n",
        "        sigmas = self.sigmas\n",
        "        # We are padding to twice n_sizes\n",
        "        padded_sigmas = [sigmas[0]] * int(np.ceil(n_sizes / 2)) + list(sigmas) + [sigmas[-1]] * int(n_sizes / 2)\n",
        "        # Used for the distgrids, which are sorted <b, d, o, p, s, y, x> (because FFT works on innermost axes)\n",
        "        return np.array(padded_sigmas).astype(np.float32)[None, None, None, None, :, None, None]\n",
        "    \n",
        "    def get_distgrids(self):\n",
        "        # Computes the distance, spatially and in terms of log-wavelength, between two points\n",
        "        # in <b, d, o, p, s, y, x> space, on a grid that is spatially twice the size (will be zero-padded)\n",
        "        # in x and y, and twice the size in terms of scale as well (will be same-padded).\n",
        "        y, x = np.mgrid[-box_height:box_height,\n",
        "                        -box_width:box_width].astype(np.float32)\n",
        "        r2grid = (y**2 + x**2)[None, None, None, None, None, :, :]\n",
        "        sd = np.mgrid[-n_sizes:n_sizes].astype(np.float32)[None, None, None, None, :, None, None]\n",
        "        sgrid = sd**2 \n",
        "            # TODO: this may be incorrect; larger wavelengths may be less susceptible to\n",
        "            # the neighbouring octaves (the absolute difference may count more than the logarithmic)\n",
        "        return r2grid, sgrid\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Create the 4d Gaussian blur filter.\n",
        "        # The total distance is computed based on r2grid *and* sgrid.\n",
        "        # Sawada assumes that we can take the product of both filters.\n",
        "        psppsf = tf.nn.softplus(self.spatial_pool_size_factor)\n",
        "        s_spatial_filter = (tf.exp(-self.r2grid/(eps + psppsf*self.wavelengths**2)) / \n",
        "                            (tf.sqrt(2*np.pi)*(psppsf*self.wavelengths)**2 + eps))\n",
        "                            # SPATIAL FILTER: self.wavelengths need to be twice as big\n",
        "\n",
        "        pscpsf = tf.nn.softplus(self.scale_pool_size_factor)\n",
        "        s_scale_filter = (tf.exp(-self.sgrid/(eps + pscpsf**2)) / (tf.sqrt(2*np.pi)*pscpsf**2) + eps)\n",
        "                            \n",
        "        s_filters = s_spatial_filter * s_scale_filter\n",
        "\n",
        "        # The same filters, in frequency space\n",
        "        f_filters = tf.signal.fft3d(tf.signal.fftshift(tf.complex(s_filters, tf.zeros_like(s_filters))))\n",
        "\n",
        "        # Reshape inputs, so that <s, y, x> are the innermost dimensions, because fft3d works\n",
        "        # on the innermost dims\n",
        "\n",
        "        sigma_scale_factors = tf.exp(self.sigmas * self.rescale_factor)[None, :, None, None, None, None, None]\n",
        "        rescaled_inputs = inputs * sigma_scale_factors\n",
        "\n",
        "        r_s_inputs = tf.einsum(\"bsopyxd->bdopsyx\", rescaled_inputs)\n",
        "\n",
        "        # Pad the inputs (except for the phases, which we don't mind if the DFT wraps them)\n",
        "        #pr_s_inputs1 = tf.pad(r_s_inputs,\n",
        "        #                    [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0],\n",
        "        #                    [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "        #                    [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT')\n",
        "        #pr_s_inputs = tf.pad(pr_s_inputs1,\n",
        "        #                    [[0, 0], [0, 0], [0, 0], [0, 0],\n",
        "        #                    [int(np.ceil(n_sizes / 2)), int(n_sizes / 2)],\n",
        "        #                    [0, 0], [0, 0]], mode='CONSTANT') # we would use edge, but tf only has symmetric\n",
        "\n",
        "        # Can use the above again as soon as TF2.1 comes out and supports padding above 6 dimensions ... jeez.\n",
        "        # For now, just use the first entry on dimension 0, relying on batch_size = 1\n",
        "\n",
        "        pr_s_inputs1 = tf.pad(r_s_inputs[0, :, :, :, :, :, :],\n",
        "                           [[0, 0], [0, 0], [0, 0], [0, 0], \n",
        "                            [int(np.ceil(box_height / 2)), int(box_height / 2)],\n",
        "                            [int(np.ceil(box_width / 2)), int(box_width / 2)]], mode='CONSTANT') #[None, :, :, :, :, :, :]\n",
        "        pr_s_inputs = tf.pad(pr_s_inputs1, #[0, :, :, :, :, :, :],\n",
        "                            [[0, 0], [0, 0], [0, 0],\n",
        "                            [int(np.ceil(n_sizes / 2)), int(n_sizes / 2)],\n",
        "                            [0, 0], [0, 0]], mode='CONSTANT')[None, :, :, :, :, :, :]\n",
        "\n",
        "        # Convert inputs to frequency domain\n",
        "        pr_f_inputs = tf.signal.fft3d(tf.complex(pr_s_inputs, tf.zeros_like(pr_s_inputs)))\n",
        "\n",
        "        # Perform the filtering and convert back to space domain\n",
        "        pr_s_filtered = tf.math.real(tf.signal.ifft3d(pr_f_inputs * f_filters))\n",
        "\n",
        "        # Crop away the padding\n",
        "        r_s_filtered = pr_s_filtered[:, :, :, :,\n",
        "                                     int(np.ceil(n_sizes / 2)):int(n_sizes + np.ceil(n_sizes / 2)),\n",
        "                                     int(np.ceil(box_height / 2)):int(box_height + np.ceil(box_height / 2)),\n",
        "                                     int(np.ceil(box_width / 2)):int(box_width + np.ceil(box_width / 2))]\n",
        "        \n",
        "        # Perform cross-orientation blurring\n",
        "        r_s_ob_filtered = tf.einsum(\"bdkpsyx,kq->bdqpsyx\", r_s_filtered,\n",
        "                                    tf.nn.softplus(self.orientation_inhibition_matrix))\n",
        "        r_s_obpb_filtered = tf.einsum(\"bdoksyx,kq->bdoqsyx\", r_s_ob_filtered,\n",
        "                                    tf.nn.softplus(self.phase_inhibition_matrix))\n",
        "\n",
        "        # Reorder dimensions\n",
        "        s_obpb_filtered = tf.einsum(\"bdopsyx->bsopyxd\", r_s_obpb_filtered)\n",
        "         \n",
        "        return s_obpb_filtered\n",
        "\n",
        "class ApplyCsf(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ApplyCsf, self).__init__(**kwargs)\n",
        "\n",
        "        self.a = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(5.0),\n",
        "                                       name='a',\n",
        "                                       trainable=True)\n",
        "        self.b = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(5.0),\n",
        "                                       name='b',\n",
        "                                       trainable=True)\n",
        "        self.orientation_factors = self.add_weight(shape=(1, 1, n_orientations - 1, 1, 1, 1),\n",
        "                                                   initializer=tf.keras.initializers.Constant(0.62),\n",
        "                                                   name='orientation_factors', trainable=True)\n",
        "        self.sigmas = get_sigmas()[None, :, None, None, None, None]\n",
        "\n",
        "    def print_weights(self):\n",
        "        a, b, of, s = tf.nn.softplus(self.a), tf.nn.softplus(self.b), tilop(self.orientation_factors), 10./self.sigmas\n",
        "        factors = tf.exp(-((s-a)/b + tf.exp(-(s-a)/b) )) * of / of[0, 0, 0, 0, 0, 0]\n",
        "\n",
        "        fig, ax = plt.subplots(1, 4)\n",
        "        for oi in range(n_orientations):\n",
        "            ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], factors[0, :, oi, 0, 0, 0])\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        a, b, of, s = tf.nn.softplus(self.a), tf.nn.softplus(self.b), tilop(self.orientation_factors), 10./self.sigmas\n",
        "        factors = tf.exp(-((s-a)/b + tf.exp(-(s-a)/b) )) * of / of[0, 0, 0, 0, 0, 0]\n",
        "\n",
        "        #factors = (a/b) * (self.sigmas/b)**(a-1) * tf.exp(-(self.sigmas/b)**a)\n",
        "        #factors = tf.exp(-((self.sigmas-a)/b + tf.exp(-(self.sigmas-a)/b) ))\n",
        "        print(factors.shape, \"factorsshape\")\n",
        "\n",
        "        return inputs * factors\n",
        "\n",
        "class Exponentiate(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Exponentiate, self).__init__(**kwargs)\n",
        "\n",
        "        #self.exponents = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1, 1),\n",
        "        self.exponents = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(2.5),\n",
        "                                       name='exponents',\n",
        "                                       trainable=False)\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(\"Exponents:\")\n",
        "        plt.imshow(tilop(self.exponents)[0, :, :, 0, 0, 0])\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #rectified_inputs, absvals = inputs\n",
        "        #pabsvals = absvals[:, :, :, None, :, :, :]\n",
        "        #factor = (pabsvals + 1.e-6) ** ptilop(self.exponents) / (1.e-6 + pabsvals)\n",
        "        #return (rectified_inputs + 1.e-6) * factor\n",
        "        return (inputs + eps) ** tilop(self.exponents)\n",
        "\n",
        "class DivisiveNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        super(DivisiveNormalization, self).__init__(**kwargs)\n",
        "\n",
        "        self.factors = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(2.46),\n",
        "                                       name='dn_factors',\n",
        "                                       trainable=True)\n",
        "        self.beta = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1, 1),\n",
        "                                       initializer=tf.keras.initializers.Constant(2.46),\n",
        "                                       name='dn_beta',\n",
        "                                       trainable=True)\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(\"Factors:\")\n",
        "        plt.imshow(ptilop(self.factors)[0, :, :, 0, 0, 0, 0])\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Beta:\")\n",
        "        plt.imshow(ptilop(self.beta)[0, :, :, 0, 0, 0, 0])\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        stimulus, normalization_pool = inputs\n",
        "        return ptilop(self.factors) * stimulus / (eps + ptilop(self.beta) + normalization_pool)\n",
        "\n",
        "\n",
        "\n",
        "class PenalizeZero(tf.keras.layers.Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        super(PenalizeZero, self).__init__(**kwargs)\n",
        "\n",
        "        self.scale_exponent = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                              initializer=tf.keras.initializers.Constant(0.62),\n",
        "                                              name='scale_exponent', trainable=False) \n",
        "        self.scale_beta = self.add_weight(shape=(1, n_sizes, n_orientations - 1, 1, 1, 1),\n",
        "                                          initializer=tf.keras.initializers.Constant(0.62),\n",
        "                                          name='scale_beta', trainable=False) \n",
        "        self.edge_loss_relative_factor = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(0.62),\n",
        "                                          name='edge_loss_relative_factor', trainable=False) \n",
        "        self.scale_mean = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(6),\n",
        "                                          name='scale_mean', trainable=False) \n",
        "        self.scale_sigma = self.add_weight(shape=(),\n",
        "                                          initializer=tf.keras.initializers.Constant(2),\n",
        "                                          name='scale_sigma', trainable=False) \n",
        "\n",
        "        self.sigmas = get_sigmas().astype(np.float32)[None, :, None, None, None, None]\n",
        "\n",
        "    def print_weights(self):\n",
        "        se, sb, elrs, mu, sigma = self.getw()\n",
        "        print(\"Diff exponent:\")\n",
        "        fig, ax = plt.subplots(1, 4)\n",
        "        for oi in range(n_orientations):\n",
        "        #    gaussian_mask = tf.exp(-(self.sigmas - mu)**2/(2*tf.nn.softplus(sigma)**2))\n",
        "        #    ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], gaussian_mask[0, :, 0, 0, 0, 0], color='b')\n",
        "        #    ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], sb[0, :, oi, 0, 0, 0])\n",
        "            ax[oi].plot(self.sigmas[0, :, 0, 0, 0, 0], se[0, :, oi, 0, 0, 0], linestyle='dotted')\n",
        "        plt.show()\n",
        "#\n",
        "        #print(\"edge loss relative factor:\", elrs.numpy())\n",
        "\n",
        "    def getw(self):\n",
        "        return (\n",
        "            tilop(self.scale_exponent),\n",
        "            tilop(self.scale_beta),\n",
        "            (self.edge_loss_relative_factor),\n",
        "            self.scale_mean,\n",
        "            self.scale_sigma,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Flat query coordinates: <1, n_query_coordinates, 4>\n",
        "        # We need to convert <b, s, o, y, x, d> \n",
        "        original_sums, diffs = inputs\n",
        "\n",
        "        se, sb, elrs, mu, sigma = self.getw()\n",
        "\n",
        "        gap_gains = (tf.nn.relu(diffs) + eps)\n",
        "        edge_losses = (tf.nn.relu(-diffs) + eps)\n",
        "\n",
        "        # Both gains and losses are masked with the same Gaussian.\n",
        "        #gaussian_mask = tf.exp(-(self.sigmas - mu)**2/(2*tf.nn.softplus(sigma)**2))\n",
        "\n",
        "        #gap_gains_hra = gaussian_mask * (gap_gains + eps) ** se #/ (eps + sb ** se + (gap_gains + eps) ** se)\n",
        "        #edge_losses_hra = elrs * gaussian_mask * (edge_losses + eps) ** se #/ (eps + sb ** se + (edge_losses + eps) ** se)\n",
        "\n",
        "        #no_vertical_gap = np.array([1., 0.5, 0., 0.5])[None, None, :, None, None, None]\n",
        "        #sf_gaussian = tf.exp(-(self.sigmas - sf[:, 0:1, :, :, :, :])**2/(2*tf.nn.softplus(sf[:, 1:2, :, :, :, :]**2)))*tf.nn.softplus(sf[:, 2:3, :, :, :, :])\n",
        "        #sf_gaussian = tf.exp(-(self.sigmas - self.sigmas[0, 6, 0, 0, 0, 0])**2/(2*tf.nn.softplus(sf[:, 1:2, :, :, :, :]**2)))*tf.nn.softplus(sf[:, 2:3, :, :, :, :])\n",
        "        #gap_gaussian = tf.exp(-(self.sigmas - sf[:, 0:1, :, :, :, :])**2/(2*tf.nn.softplus(sf[:, 1:2, :, :, :, :]**2)))*tf.nn.softplus(gsf[:, 2:3, :, :, :, :])\n",
        "        #gap_gaussian = tf.exp(-(self.sigmas - self.sigmas[0, 6, 0, 0, 0, 0])**2/(2*tf.nn.softplus(gsf[:, 1:2, :, :, :, :]**2)))*tf.nn.softplus(gsf[:, 2:3, :, :, :, :])\n",
        "\n",
        "        penalties = (\n",
        "        #    gap_gains_hra - edge_losses_hra\n",
        "          (gap_gains + eps) - (edge_losses + eps)\n",
        "        )\n",
        "        return penalties\n",
        "\n",
        "class TotalPenalty(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TotalPenalty, self).__init__(**kwargs)\n",
        "\n",
        "        self.total_factor = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(1.),\n",
        "                                       name='total_factor', trainable=False)\n",
        "        self.total_exponent = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(1.1),\n",
        "                                       name='total_exponent', trainable=False)\n",
        "        self.total_beta = self.add_weight(shape=(),\n",
        "                                       initializer=tf.keras.initializers.Constant(1.0),\n",
        "                                       name='total_beta', trainable=False)\n",
        "\n",
        "    def print_weights(self):\n",
        "        tfa, te, tb = self.getw()\n",
        "        print(\"Total factor:\", tfa)\n",
        "        print(\"Total exponent:\", te)\n",
        "        print(\"Total beta:\", tb)\n",
        "\n",
        "    def getw(self):\n",
        "        return (\n",
        "            tf.nn.softplus(self.total_factor),\n",
        "            tf.nn.softplus(self.total_exponent),\n",
        "            tf.nn.softplus(self.total_beta),\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Now we want to add up all of the inputs, and feed them through a HRA\n",
        "        total_penalty = tf.reduce_sum(inputs, axis=[1, 2, 3, 4]) + eps\n",
        "        tfa, te, tb = self.getw()\n",
        "\n",
        "        return total_penalty #tfa * (total_penalty + eps) ** te / (eps + tb ** te + (total_penalty + eps) ** te)\n",
        "\n",
        "\n",
        "class DistanceEstimator(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DistanceEstimator, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, x = inputs\n",
        "        xdelta = (x[:, 1:] - x[:, :-1])\n",
        "        ydelta = (y[:, 1:] - y[:, :-1]) # Positive when upward\n",
        "    \n",
        "        yrange = (tf.reduce_max(y, axis=[1], keepdims=True) - tf.reduce_min(y, axis=[1], keepdims=True)) + eps\n",
        "        estimate_validities = nd_softmax(1e3 * y/yrange, axis=[1])\n",
        "        estimated_distances = tf.reduce_sum(estimate_validities * x, axis=[1], name='estimated_distances')\n",
        "        return estimated_distances\n",
        "\n",
        "def get_model():\n",
        "    shifted_gd1_filtered_images = tf.keras.Input(shape=full_shape, name='shifted_gd1_filtered_images', dtype=tf.complex64)\n",
        "    shifted_gd2_filtered_images = tf.keras.Input(shape=full_shape, name='shifted_gd2_filtered_images', dtype=tf.complex64)\n",
        "\n",
        "    # Go from <b, s, o, h, w, d> to <b, s, o, p, h, w, d>\n",
        "    pr_gd1 = rectify_phases(shifted_gd1_filtered_images)\n",
        "    pr_gd2 = rectify_phases(shifted_gd2_filtered_images)\n",
        "    pr_pair = rectify_phases(shifted_gd1_filtered_images + shifted_gd2_filtered_images)\n",
        "\n",
        "    # Then, exponentiate with a power.\n",
        "    exp = Exponentiate()\n",
        "    #pr_p_gd1 = tf.identity(exp([pr_gd1, tf.abs(shifted_gd1_filtered_images)]), name=\"pr_p_gd1\")\n",
        "    #pr_p_gd2 = tf.identity(exp([pr_gd2, tf.abs(shifted_gd2_filtered_images)]), name=\"pr_p_gd2\")\n",
        "    #pr_p_pair = tf.identity(exp([pr_pair, tf.abs(shifted_gd1_filtered_images + shifted_gd2_filtered_images)]), name=\"pr_p_pair\")\n",
        "\n",
        "    # Then, calculate the normalization pools\n",
        "    #np = NormalizationPool()\n",
        "    #pr_np_gd1 = tf.identity(np(pr_p_gd1), name=\"pr_np_gd1\")\n",
        "    #pr_np_gd2 = tf.identity(np(pr_p_gd2), name=\"pr_np_gd2\")\n",
        "    #pr_np_pair = tf.identity(np(pr_p_pair), name=\"pr_np_pair\")\n",
        "\n",
        "    # Then, perform the divisive normalization\n",
        "    #dn = DivisiveNormalization()\n",
        "    #pr_dn_gd1 = tf.identity(dn([pr_p_gd1, pr_np_gd1]), name=\"pr_dn_gd1\")\n",
        "    #pr_dn_gd2 = tf.identity(dn([pr_p_gd2, pr_np_gd2]), name=\"pr_dn_gd2\")\n",
        "    #pr_dn_pair = tf.identity(dn([pr_p_pair, pr_np_pair]), name=\"pr_dn_pair\")\n",
        "\n",
        "    # Then, compute the absolute energy values, because that's what we care about.\n",
        "    ba = BiasedAbs()\n",
        "\n",
        "    apply_csf = ApplyCsf()\n",
        "    #e_dn_gd1 = tf.identity(ba(pr_dn_gd1), \"e_dn_gd1\")\n",
        "    #e_dn_gd2 = tf.identity(ba(pr_dn_gd2), \"e_dn_gd2\")\n",
        "    #e_dn_pair = tf.identity(ba(pr_dn_pair), \"e_dn_pair\")\n",
        "    e_dn_gd1 = tf.identity(apply_csf(exp(ba((shifted_gd1_filtered_images)))), \"e_dn_gd1\")\n",
        "    e_dn_gd2 = tf.identity(apply_csf(exp(ba((shifted_gd2_filtered_images)))), \"e_dn_gd2\")\n",
        "    e_dn_pair = tf.identity(apply_csf(exp(ba((shifted_gd1_filtered_images + shifted_gd2_filtered_images)))), \"e_dn_pair\")\n",
        "\n",
        "    #le_dn_gd1 = tf.math.imag(pr_dn_gd1)\n",
        "    #le_dn_gd2 = tf.math.imag(pr_dn_gd2)\n",
        "    #le_dn_pair = tf.math.imag(pr_dn_pair)\n",
        "    #ee_dn_gd1 = tf.math.real(pr_dn_gd1)\n",
        "    #ee_dn_gd2 = tf.math.real(pr_dn_gd2)\n",
        "    #ee_dn_pair = tf.math.real(pr_dn_pair)\n",
        "\n",
        "    #line_original_sums = tf.identity(tf.abs(le_dn_gd1 + le_dn_gd2), name=\"line_original_sums\")\n",
        "    #line_diffs = tf.identity(tf.abs(le_dn_pair) - line_original_sums, name=\"line_original_sums\")\n",
        "\n",
        "    # Then, compute the differences between pair and (gd1 + gd2)\n",
        "\n",
        "    original_sums = tf.identity(e_dn_gd1 + e_dn_gd2, name=\"original_sums\")\n",
        "    diffs = tf.identity(e_dn_pair - original_sums, name=\"diffs\")\n",
        "\n",
        "    # Then, use a 4D polyharmonic spline to figure out the best way to penalize certain diffs\n",
        "    # (Use <s, o, original_sum, diffs>) TODO: perhaps use 5D by including y-factor\n",
        "    penalize = PenalizeZero()\n",
        "    penalties = tf.identity(penalize([original_sums, diffs]), name=\"total_pixel_penalties\")\n",
        "\n",
        "    totalPenalty = TotalPenalty()\n",
        "    d = tf.identity(totalPenalty(penalties), name=\"total_penalties\")\n",
        "    #d = tf.identity(tf.nn.elu((d[:, 1] - d[:, 0])) + tf.nn.elu((d[:, 1] - d[:, 2])), name=\"losses\")\n",
        "\n",
        "    total_gap_gains = tf.reduce_sum(tf.nn.relu(penalties), axis=[1, 2, 3, 4])\n",
        "    total_edge_losses = tf.reduce_sum(tf.nn.relu(-penalties), axis=[1, 2, 3, 4])\n",
        "\n",
        "    gap_gain_increase = tf.nn.elu(total_gap_gains[:, 0] - total_gap_gains[:, 1]) + tf.nn.elu(total_gap_gains[:, 1] - total_gap_gains[:, 2])\n",
        "    edge_loss_decrease = tf.nn.elu(total_edge_losses[:, 1] - total_edge_losses[:, 0]) + tf.nn.elu(total_edge_losses[:, 2] - total_edge_losses[:, 1])\n",
        "\n",
        "    d = tf.identity(d[:, 1]**2 + tf.nn.elu(d[:, 0] - d[:, 1]) + tf.nn.elu(d[:, 1] - d[:, 2]) + gap_gain_increase + edge_loss_decrease, name=\"losses\") # Gap is negative\n",
        "    #d = tf.reduce_sum(penalties, axis=[1, 2], name=\"pixel_penalties\") # sum over orientations: <b, s1, o, h, w, d>\n",
        "    #d = tf.identity(tf.reduce_sum(d, axis=[1, 2]), name=\"total_penalties\") # / tf.reduce_sum(tf.nn.relu(-penalties), axis=[1,2,3,4]) # <b, h, w, d> → <b, d>\n",
        "\n",
        "    sample_distances = tf.keras.Input(shape=(n_sample_distances), name='sample_distances')\n",
        "    pair_images = tf.keras.Input(shape=(box_height, box_width, n_sample_distances), name='pair_images')\n",
        "    zero_indices = tf.keras.Input(shape=(), name='zero_indices')\n",
        "    desired_penalty_slope_sign = tf.keras.Input(shape=(n_sample_distances - 1), name='desired_penalty_slope_sign')\n",
        "\n",
        "\n",
        "    # Format: [n_batch_size, n_distances]\n",
        "    #increases = tf.identity(d[:, 1:] - d[:, :-1], name=\"increases\") # The increase during every interval.\n",
        "    # We need increases to be negative *before* the zero index, and positive after\n",
        "    # zero_indices is shape [batch_size]\n",
        "    #ki = (increases * [1, 1, 1, -1, -1, -1])\n",
        "    #desired_slope_variation_penalty = tf.identity(tf.nn.elu(ki) + 1., name=\"dsvps\") #(-desired_penalty_slope_sign\n",
        "\n",
        "    # Step 10. Estimate best distance\n",
        "    #predicted_ink_distances = DistanceEstimator()([d, sample_distances])\n",
        "\n",
        "    # Step 10. Deviation from zero\n",
        "\n",
        "    return tf.keras.Model(inputs=[shifted_gd1_filtered_images,\n",
        "                                  shifted_gd2_filtered_images,\n",
        "                                  sample_distances, pair_images, zero_indices, desired_penalty_slope_sign],\n",
        "                                  outputs=(d))\n",
        "                          #outputs=predicted_ink_distances)\n",
        "\n",
        "#@tf.function\n",
        "#def compute_loss(target_ink_distance, predicted_ink_distance):\n",
        "#    return tf.sqrt(1.e-7 + tf.reduce_sum((target_ink_distance - predicted_ink_distance) ** 2) / batch_size, name='sqrt') #* 100 / factor\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(_target, deviation):\n",
        "    return tf.reduce_sum(deviation) #(tf.reduce_sum(deviation**2)) / batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCnQjSuTT_W",
        "colab_type": "text"
      },
      "source": [
        "## Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAaOa_qnTWC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.patches import Circle\n",
        "\n",
        "class MonitorProgressCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data_generator):\n",
        "        self.data_generator = data_generator\n",
        "        self.model_inputs = [l.input for l in model.layers if isinstance(l, tf.keras.layers.InputLayer)]\n",
        "        #print(\"MODEL INPUTS ARE\", [(l.name, l.input.shape) for l in model.layers if isinstance(l, tf.keras.layers.InputLayer)])\n",
        "        self.current_data = None\n",
        "\n",
        "    def get_val(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        #print(\"getting value\", l.name, l.output.shape)\n",
        "        #print(\"values\", tf.keras.backend.function(self.model_inputs, [l.output])(self.current_data))\n",
        "        output = tf.keras.backend.function(self.model_inputs, [l.output])(self.current_data)[0]\n",
        "        return output\n",
        "\n",
        "    def get_weights(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        return l.get_weights()\n",
        "\n",
        "    def print_weights(self, name):\n",
        "        l = [l for l in model.layers if l.name.endswith(name)][0]\n",
        "        l.print_weights()\n",
        "\n",
        "    def on_test_batch_begin(self, batch_index, logs=None):\n",
        "        dataset = self.data_generator[batch_index]\n",
        "        current_data = dataset[0]\n",
        "        self.current_data = current_data\n",
        "        shifted_gd1_d1_filtered_images, shifted_gd2_d1_filtered_images, sample_distances, pair_images, zero_indices, desired_penalty_slope_sign = current_data\n",
        "\n",
        "        iix = 0\n",
        "        \n",
        "        if False:\n",
        "            plt.imshow(pair_images[iix, :, :, 0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(pair_images[iix, :, :, zero_indices[iix]])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(pair_images[iix, :, :, 2])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "        if True:\n",
        "            self.print_weights(\"apply_csf\")\n",
        "            self.print_weights(\"exponentiate\")\n",
        "            #self.print_weights(\"normalization_pool\")\n",
        "            #self.print_weights(\"divisive_normalization\")\n",
        "            #self.print_weights(\"biased_abs\")\n",
        "            self.print_weights(\"total_penalty\")\n",
        "\n",
        "        if False:\n",
        "            e_gd1 = self.get_val(\"e_gd1\")\n",
        "            e_pair = self.get_val(\"e_pair\")\n",
        "\n",
        "            i_gd1 = self.get_val(\"shifted_gd1_filtered_images\")\n",
        "            i_gd2 = self.get_val(\"shifted_gd2_filtered_images\")\n",
        "            i_total_angle = tf.math.angle(i_gd1 + i_gd2)\n",
        "\n",
        "            dn_gd1 = self.get_val(\"e_dn_gd1\")\n",
        "            dn_gd2 = self.get_val(\"e_dn_gd2\")\n",
        "            dn_pair = self.get_val(\"e_dn_pair\")\n",
        "            diffs = self.get_val(\"diffs\")\n",
        "            for si in range(n_sizes):\n",
        "                fig, ax = plt.subplots(nrows=1, ncols=5, gridspec_kw={'wspace':0, 'hspace':0}, figsize=(4 * 4 * box_width / 100, 4 * 1 * box_height / 100))\n",
        "                ax[0].imshow(tf.math.angle(i_gd1)[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[1].imshow(i_total_angle[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[2].imshow((dn_gd1 + dn_gd2)[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[3].imshow(dn_pair[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                ax[4].imshow(diffs[iix, si, 0, :, :, zero_indices[iix]])\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            increases = self.get_val(\"increases\")\n",
        "            dsvps = self.get_val(\"dsvps\")\n",
        "            #print(\"DPSS:\", dsvps)\n",
        "            pcs = self.get_val(\"total_penalties\")\n",
        "            for pix in range(batch_size):\n",
        "                plt.plot(np.arange(n_sample_distances) - zero_indices[pix], pcs[pix, :])\n",
        "                plt.plot(0.5 + np.arange(n_sample_distances - 1) - zero_indices[pix], dsvps[pix, :])\n",
        "                plt.plot(0.5 + np.arange(n_sample_distances - 1) - zero_indices[pix], increases[pix, :])\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            p = self.get_val(\"pr_dn_pair\")\n",
        "            pa = tf.math.angle(tf.complex(p[:, :, :, 0, :, :, :] - p[:, :, :, 2, :, :, :], p[:, :, :, 1, :, :, :] - p[:, :, :, 3, :, :, :]))\n",
        "            pv = tf.abs(tf.complex(p[:, :, :, 0, :, :, :] - p[:, :, :, 2, :, :, :], p[:, :, :, 1, :, :, :] - p[:, :, :, 3, :, :, :]))\n",
        "            for si in range(n_sizes):\n",
        "                plt.imshow((pa)[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                plt.imshow((pv)[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            if False:\n",
        "                va = self.get_val(\"pr_np_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    for pi in range(4):\n",
        "                        ax[si, pi].imshow(va[iix, si, 0, pi, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                        ax[si, pi].set_xticklabels([])\n",
        "                        ax[si, pi].set_yticklabels([])\n",
        "                plt.show()\n",
        "    \n",
        "                va = self.get_val(\"pr_dn_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    for pi in range(4):\n",
        "                        ax[si, pi].imshow(va[iix, si, 0, pi, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                        ax[si, pi].set_xticklabels([])\n",
        "                        ax[si, pi].set_yticklabels([])\n",
        "                plt.show()\n",
        "    \n",
        "                va = self.get_val(\"e_dn_gd1\")\n",
        "                size_factor = 5\n",
        "                fig, ax = plt.subplots(nrows=n_sizes, ncols=1,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "                for si in range(n_sizes):\n",
        "                    ax[si].imshow(va[iix, si, 0, :, :, zero_indices[iix]], cmap='RdBu') #, vmin=-lvmax, vmax=lvmax)\n",
        "                    ax[si].set_xticklabels([])\n",
        "                    ax[si].set_yticklabels([])\n",
        "                plt.show()\n",
        "\n",
        "            va = self.get_val(\"e_dn_pair\")\n",
        "            pp = self.get_val(\"e_dn_gd1\") + self.get_val(\"e_dn_gd2\")\n",
        "            size_factor = 5\n",
        "            #fig, ax = plt.subplots(nrows=n_sizes, ncols=1,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "            for si in range(n_sizes):\n",
        "                print(\"size\", si, \"pair then originalsums\")\n",
        "                plt.imshow(va[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                plt.imshow(pp[iix, si, 0, :, :, zero_indices[iix]]) #, vmin=-lvmax, vmax=lvmax)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "        if False:\n",
        "            if True:\n",
        "                print(\"Normalization pool, pair, 2, 0:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_np_pair\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Normalizataion pool, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_np_pair\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Divisive Normalization, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_dn_pair\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Divisive normalization, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"e_dn_pair\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Original sums, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"original_sums\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                print(\"Original sums, pair, total:\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"original_sums\"))\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, 0, :, :, zero_indices[iix]], axis=[0]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "            if True:\n",
        "                print(\"Diffs, pair, 2, 0\")\n",
        "                pixel_penalties = tf.math.real(self.get_val(\"diffs\"))\n",
        "                plt.imshow(pixel_penalties[iix, 2, 0, :, :, zero_indices[iix]])\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "        if False:\n",
        "            diffs = tf.math.real(self.get_val(\"diffs\"))\n",
        "            for di in range(n_sample_distances):\n",
        "                for si in range(n_sizes):\n",
        "                    print(\"distance\", di, \"size\", si)\n",
        "                    plt.imshow(diffs[iix, si, 0, :, :, di])\n",
        "                    plt.colorbar()\n",
        "                    plt.show()\n",
        "        if True:\n",
        "            print(\"Diffs, pair, total:\")\n",
        "            diffs = tf.math.real(self.get_val(\"diffs\"))\n",
        "            plt.imshow(-tf.reduce_sum(diffs[iix, :, :, :, :, zero_indices[iix]], axis=[0, 1]))\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"Penalties:\")\n",
        "            pixel_penalties = tf.math.real(self.get_val(\"total_pixel_penalties\"))\n",
        "            total_penalties = tf.math.real(self.get_val(\"total_penalties\"))\n",
        "            losses = tf.math.real(self.get_val(\"losses\"))\n",
        "            sigmas = get_sigmas()\n",
        "\n",
        "            plt.plot(sigmas, np.zeros_like(sigmas), color='k')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(diffs[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-diffs[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.show()\n",
        "\n",
        "            plt.plot(sigmas, np.zeros_like(sigmas), color='k')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 0]), axis=[1,2,3]), color='r')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 1]), axis=[1,2,3]), color='b')\n",
        "            plt.plot(sigmas, tf.reduce_sum(tf.nn.relu(pixel_penalties[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.plot(sigmas, tf.reduce_sum(-tf.nn.relu(-pixel_penalties[iix, :, :, :, :, 2]), axis=[1,2,3]), color='g')\n",
        "            plt.show()\n",
        "\n",
        "            for di in range(n_sample_distances):\n",
        "                print(\"DISTANCE INDEX\", di)\n",
        "                if (di == zero_indices[iix]):\n",
        "                    print(\"best distance:\")\n",
        "                vm = max(tf.reduce_max(pixel_penalties[iix, :, :, :, :, di]), -tf.reduce_min(pixel_penalties[iix, :, :, :, :, di]))\n",
        "                print(\"max:\", vm, \"sum:\", tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di]), \"total_sum:\", total_penalties[iix, di], \"losses:\", losses[iix])\n",
        "                plt.imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di], [0, 1]), alpha=0.7)\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "                if di == 1 and True:\n",
        "                    for si in range(n_sizes):\n",
        "                        print(\"SIZE\", si)\n",
        "                        fig, ax = plt.subplots(1, 2,  gridspec_kw = {'wspace':0, 'hspace':0},  figsize=(5 * 2 * box_width / 100, 5 * box_height / 100))\n",
        "                        ax[0].imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                        pp = tf.reduce_sum(pixel_penalties[iix, si, :, :, :, di], [0])\n",
        "                        aim = ax[0].imshow(pp, alpha=0.7)\n",
        "                        max_y, max_x = np.unravel_index(pp.numpy().argmax(), pp.shape)\n",
        "                        min_y, min_x = np.unravel_index(pp.numpy().argmin(), pp.shape)\n",
        "                        ax[0].add_patch(Circle((max_x,max_y),sigmas[si]*4,edgecolor='w',facecolor=None,fill=False))\n",
        "                        ax[0].add_patch(Circle((min_x,min_y),sigmas[si]*4,edgecolor='w',facecolor=None,fill=False))\n",
        "                        fig.colorbar(aim, ax=ax[0])\n",
        "                        ax[1].imshow(pair_images[iix, :, :, di], alpha=0.7, cmap='gray')\n",
        "                        dim = ax[1].imshow(tf.reduce_sum(diffs[iix, si, :, :, :, zero_indices[iix]], axis=[0]))\n",
        "                        fig.colorbar(dim, ax=ax[1])\n",
        "                        plt.show()\n",
        "\n",
        "                plt.imshow(tf.reduce_sum(pixel_penalties[iix, :, :, :, :, di], [2, 3]))\n",
        "                plt.colorbar()\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "        if False:\n",
        "            print(\"Sigmas\")\n",
        "            sigmas = self.get_weights(\"SpatialAverage\")[0]\n",
        "            plt.plot(sigmas[0, 0, :, 0, 0])\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            print(\"After s/o dense convolution: lines, edges\")\n",
        "            blurred = self.get_val(\"SpatialAverage\")\n",
        "            subbed = self.get_val(\"pow\")\n",
        "            hra_total = self.get_val(\"hr_atotal\")\n",
        "            size_factor = 2\n",
        "            fig, ax = plt.subplots(nrows=n_sizes, ncols=4,  gridspec_kw = {'wspace':0, 'hspace':0}, figsize=(size_factor * n_orientations * box_width / 100, size_factor * n_sizes * box_height / 100))\n",
        "            for si in range(n_sizes):\n",
        "                ax[si, 0].imshow(tf.reduce_sum(abstotal, axis=[2])[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 0].set_xticklabels([])\n",
        "                ax[si, 0].set_yticklabels([])\n",
        "                ax[si, 1].imshow(blurred[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 1].set_xticklabels([])\n",
        "                ax[si, 1].set_yticklabels([])\n",
        "                ax[si, 2].imshow(subbed[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 2].set_xticklabels([])\n",
        "                ax[si, 2].set_yticklabels([])\n",
        "                ax[si, 3].imshow(hra_total[iix, si, :, :, zero_indices[iix]])\n",
        "                ax[si, 3].set_xticklabels([])\n",
        "                ax[si, 3].set_yticklabels([])\n",
        "\n",
        "                print(\"size\", si, tf.reduce_sum(hra_total[iix, si, :, :, zero_indices[iix]]))\n",
        "            plt.show()\n",
        "\n",
        "        if False:\n",
        "            hravars = self.get_weights(\"hr_atotal\")\n",
        "            print(\"hravars\", hravars)\n",
        "            print(\"Hyperbolic ratio variables.\")\n",
        "            print(\"Exponents\")\n",
        "            plt.plot(tf.nn.softplus(hravars[0][0, :, 0, 0, 0]))\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"Alphas\")\n",
        "            plt.plot(tf.nn.softplus(hravars[1][0, :, 0, 0, 0]))\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "            print(\"m-scale\")\n",
        "            plt.plot(hravars[2][0, :, 0, 0, 0])\n",
        "            #plt.colorbar()\n",
        "            plt.show()\n",
        "    \n",
        "            print(\"Total for each pixel\")\n",
        "            out = self.get_val(\"Sum_1\")\n",
        "            plt.imshow(out[iix, :, :, 0])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(out[iix, :, :, zero_indices[iix]])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.imshow(out[iix, :, :, -1])\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            #for i in range(n_sample_distances):\n",
        "            #    plt.imshow(tf.reduce_sum(out, axis=[3,4], keepdims=True)[iix, :, :, 0, 0, i])\n",
        "            #    plt.colorbar()\n",
        "            #    plt.show()\n",
        "            #    print(\"Index:\", i, \"total is\", tf.reduce_sum(out, axis=[1,2,3,4])[iix, i])\n",
        "        if False:\n",
        "            # Display what the penalties look like\n",
        "            pass\n",
        "\n",
        "        if True:\n",
        "            self.print_weights(\"penalize_zero\")\n",
        "\n",
        "        if False:             \n",
        "            print(\"Diglyphiness values\")\n",
        "            out = self.get_val(\"Sum\")\n",
        "            pred = self.get_val(\"distance_estimator\")\n",
        "            print(\"OUT PRED\", out.shape, pred.shape)\n",
        "            plt.plot(sample_distances[iix, :], out[iix, :])\n",
        "            print(\"Pred distance is\", pred[iix], \"correct distance is\", sample_distances[iix, zero_indices[iix]])\n",
        "            plt.scatter([pred[iix]], [0])\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIbPWIeyH2Xh",
        "colab_type": "text"
      },
      "source": [
        "## Keras pipeline setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRnxgssIHQGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "ig = InputGenerator(batch_size)\n",
        "if True:\n",
        "    model = get_model()\n",
        "    model.compile(loss=compute_loss,\n",
        "                optimizer=tf.keras.optimizers.Adam(0.15))\n",
        "    #model.summary()\n",
        "    history = model.fit_generator(ig,\n",
        "                                callbacks=[MonitorProgressCallback(ig)],\n",
        "                                validation_data=ig,\n",
        "                                validation_steps=1,\n",
        "                                validation_freq=10,\n",
        "                                epochs=3000,\n",
        "                                steps_per_epoch=10, use_multiprocessing=False)\n",
        "ig.kill()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQ3_GtPPHqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fontParts.world import *\n",
        "import string\n",
        "font = OpenFont(\"DroidSerif.ttf\", showInterface=False)\n",
        "print(dir(font.kerning))\n",
        "for gl in string.ascii_lowercase:\n",
        "    #lsb, rsb = yourletterfitter.find_sidebearings(g)\n",
        "    font.layers[0][gl].leftMargin = 0#lsb\n",
        "    font.layers[0][gl].rightMargin = 0#rsb\n",
        "    for gr in string.ascii_lowercase:\n",
        "        font.kerning._setItem((gl, gr), 10)\n",
        "font.save(\"Autospaced.otf\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}