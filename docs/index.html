<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>YinYangFit</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="source/style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<h1 id="yinyangfit">YinYangFit ☯</h1>
<p><em>Modelling for automatic letterfitting, inspired by neuroscience</em></p>
<p><img src="img/abstract.png" alt="Abstract"/></p>
<p class="missing">
This article is only half done. Please free to follow the project on Github and check back later for more. Thanks :)
</p>
<h2 class="nonumber">
Acknowledgements
</h2>
<p>This research would not have been possible without funding from Google, for which I have Dave Crossland to thank in particular. I am grateful also to Simon Cozens and others for many valuable discussions.</p>
<!--## Contents
1. [Abstract](#abstract)
2. [A good fit: what does that mean?](#intro)
4. [What can vision research teach us about letterfitting?](#vision_research_letterfitting)
5. [Models of the visual cortex](#modelling_visual_cortex)
6. [Building a multi-scale letter pair analyzer](#multiscale)
7. [Extending our model: lateral inhibition and divisive normalization](#extending)
8. Results (check back soon!)
9. Parameter tuning (check back soon!)
10. YinYangFit, the tool (check back soon!)
3. Appendix: [Exisiting letterfitting tools](#existing_tools) -->
<h2 class="nonumber">
Abstract
</h2>
<p>Adjusting letter distances to be visually pleasing is a challenging and time-consuming task. As existing tools are too primitive to reliably handle the infinite variety of typefaces, designers still need to rely on their intuitive judgment. I review how letterfitting fits into the current scientific understanding of how letters and words are perceived in the brain, and present approximate models that can be fitted to to existing, hand-fitted fonts using backpropagation.</p>
<h2 class="nonumber">
Target audience
</h2>
<p>Designers and developers with an interest in neuroaesthetics.</p>
<h2 class="nonumber">
Epistemic status: provisional
</h2>
<p>This article is based on a survey of hundreds of peer-reviewed articles, and in line with mainstream ideas in vision and neuroscience research. It is the product of many months of work and countless revisions. That said, even the in-vivo evidence for the suggested models is often indirect or circumstantial. Nothing in this article should be construed as final. I welcome corrections!</p>
<h2 id="introduction">Introduction:</h2>
<p>Letterfitting refers to the process of adjusting the distances between pairs of <nobr>letters<input type="checkbox" id="sn-sidenote-1" class="margin-toggle"></nobr><label for="sn-sidenote-1" class="margin-toggle sidenote-number"></label><span class="sidenote">I use the word “letter” very liberally; the more general term is <a href="https://en.wikipedia.org/wiki/Glyph">glyph</a><sup>W</sup>.</span> during typeface design. <nobr><input type="checkbox" id="mn-marginnote-2" class="margin-toggle"></nobr><label for="mn-marginnote-2" class="margin-toggle"></label><span class="marginnote"><img src="img/spacingkerning.png" alt="Spacing and kerning"><br> Red vertical bars show side bearings, blue vertical bar shows a negative kern.</span> It’s often referred to as “spacing and kerning”, because pair distances are the sum of fixed amounts of space around every letter (so-called <em>side bearings</em>) and additional adjustment values for individual pairs (so-called <em>kerns</em><nobr>).<input type="checkbox" id="sn-sidenote-3" class="margin-toggle"></nobr><label for="sn-sidenote-3" class="margin-toggle sidenote-number"></label><span class="sidenote">Many existing heuristics try to either auto-space or auto-kern, which is doomed to fail. See the <a href="#space_kern_lp">appendix</a> for the correct mathematical approach to split pair distances into side bearings and kerns.</span> Quality fonts often contain thousands of hand-kerned pairs that undergo weeks of testing and refinement, all by hand—because surprisingly, there still are no automated solutions that reliably do the <nobr>job.<input type="checkbox" id="sn-sidenote-4" class="margin-toggle"></nobr><label for="sn-sidenote-4" class="margin-toggle sidenote-number"></label><span class="sidenote">And not for lack of trying: many approaches exist, the most popular of which are listed in the <a href="#existing_tools">appendix</a> below.</span></p>
<p>The heart of the problem: typographers can’t even agree what letterfitting <em>does</em>. Some say that it’s about achieving a certain <em>balance</em> between letter pairs, the judgment of which is to spring from the designer’s personal aesthetic <nobr>intuition.<input type="checkbox" id="sn-sidenote-5" class="margin-toggle"></nobr><label for="sn-sidenote-5" class="margin-toggle sidenote-number"></label><span class="sidenote">It goes without saying that as for the design decisions of professional typographers, <em>non disputandum est</em>. This is the premise behind the venerable <a href="https://type.method.ac/">kern game</a>.</span> Others say that the goal is to produce an “even colour”, i.e. a printed page with a uniform texture and without noticeable blobs of black or white. Yet others have <nobr>insinuated<input type="checkbox" id="sn-sidenote-6" class="margin-toggle"></nobr><label for="sn-sidenote-6" class="margin-toggle sidenote-number"></label><span class="sidenote">First and foremost Frank Blokland, who in his <a href="https://www.lettermodel.org/">PhD thesis</a> investigated how practical considerations in the Renaissance printing trade may have led to a standardization of font metrics.</span> that the distances between letter stems are really quite arbitrary, and that we are simply conditioned by existing fonts to (prefer to) read letters at particular pair distances.</p>
<p>All three of the above descriptions seem to point to the same story: that skilled designers achieve a pleasing visual balance between letter pairs because they have honed their perception through the careful study of existing fonts, and that perfectly balanced letter pairs also happen to result in perfect legibility and a perfectly even typographic colour. Does that story hold water?</p>
<p>As it turns out, research suggests that colour, balance, and legibility have <em>different</em> neural correlates. They are often in rough agreement, but optimizing for one does not guarantee a good outcome for the others.</p>
<p><img src="img/introduction_overview.png" alt="Neural correlates of different
typographic concepts"></p>
<p>Evenness of colour is a question of texture perception; quality of balance is a question of competitive inhibition between perceptual gestalt groups; and legibility is a question of the reliable detection of letters and n-grams from pre-processed visual features. On top of that, all of the above are affected differently by font size and colour contrast.</p>
<p>The premise behind today’s letterfitting tools is that the gaps between letters can be measured and equalized. But human brains don’t perceive gaps; they perceive shapes whose neural representations interact across space in particular ways. If we want to develop robust, universal automatic letterfitting algorithms—algorithms that work on both hairline slab serifs and broad-nib italics, on both captions and headline sizes, on both Latin and Hangul—then we need to build better intuitions for the neural dynamics of our vision system. That’s what this article is about.</p>
<p>In a way, it is surprising that type design and cognitive psychology are so divorced from one <nobr>another.<input type="checkbox" id="sn-sidenote-7" class="margin-toggle"></nobr><label for="sn-sidenote-7" class="margin-toggle sidenote-number"></label><span class="sidenote">The studies that do exist are almost exclusively empirical (see e.g. the <nobr><a href="https://doi.org/10.1016/j.visres.2019.05.003">review of legibility studies</a><span class="oa" title="Open
Access"></span></nobr> compiled recently by type legend Charles Bigelow) but have no explanatory power. In fact, the typesetting of most preprints suggests that cognitive scientists are altogether unaware of typography as a discipline.</span> Computational models of vision and reading need to be tested against ground-truth data, of which existing fonts are a rich, reliable, and free source. Conversely, type designers could massively benefit from tools that emulate aspects of human vision. I hope to see much more cross-fertilization between the two fields in the future.</p>
<h2 id="a-letterfitters-objectives">A letterfitter’s objectives</h2>
<p>Before we dive into the details, let’s review how the three objectives fit into a broader cognitive science context.</p>
<h3 id="typographic-colour">Typographic colour</h3>
<p>Typographic colour refers to the visual texture created by the ink on the page. Most obviously, a darker colour is the result of bolder, narrower, more tightly-fit type. But the line spacing contributes to a document’s characteristic texture as well, and so does the angle of the letters (i.e. upright vs. italic) and, ultimately, the design of the individual letters. Some design teachers like to give colour-based letterfitting prescriptions, like “match the black and the white” or “equalize the negative space in the counters with the negative space in the gaps.”<nobr><input type="checkbox" id="sn-sidenote-8" class="margin-toggle"></nobr><label for="sn-sidenote-8" class="margin-toggle sidenote-number"></label><span class="sidenote">Like horoscopes, these rules only work when they are formulated vaguely enough to be useless.</span> As we will see later, these heuristics are actually a primitive version of the kind of spatial frequency correlations that form the basis of texture perception.</p>
<h3 id="balance">Balance</h3>
<p>The brain has a general tendency to group visual features into perceptually coherent objects. Meanwhile, the typographer’s job is to group letters into perceptually coherent words. When the letters are fitted poorly, the perceptual grouping into words will fail: this we call poor balance.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-9" class="margin-toggle"></nobr><label for="mn-marginnote-9" class="margin-toggle"></label><span class="marginnote">Here, the saturation of the coloured blobs indicates the intensity of grouping at different scales. Small perceptual groups tend to outcompete larger ones, so unless the grouping is balanced, the word will be fragmented. The poorly fitted word in the last column triggers the perception of two separate objects, namely the single letter <em>c</em> and a pair <em>at</em>.</span> <img src="img/grouping_relativity.png" alt="Illustration of the importance of consistency of fit vs absolute distances."></p>
<p>Perceptual grouping networks are a very fundamental piece of our vision circuitry, and not exclusive to reading. Researchers have known about them for a long time, too: psychologists over a century ago described our tendency to perceive the sum, not the parts, of certain arrangements of <nobr>shapes:<input type="checkbox" id="mn-marginnote-10" class="margin-toggle"></nobr><label for="mn-marginnote-10" class="margin-toggle"></label><span class="marginnote">These are often listed as the <a href="https://en.wikipedia.org/wiki/Principles_of_grouping">Gestalt laws of grouping</a><sup>W</sup>, or the principle of <a href="https://en.wikipedia.org/wiki/Gestalt_psychology#Pr%C3%A4gnanz">Prägnanz</a><sup>W</sup>. Because these early Gestalt psychologists knew little about the brain’s vision system, they hypothesized about their findings using sometimes abstruse metaphors drawn from electromagnetics, fluid mechanics, and even personality studies—in fact, their vocabulary of lights, shadows, and force fields closely matched that often employed to justify today’s letterfitting heuristics. I recommend Johan Wagemans et al.’s fantastic two-part historical review of Gestalt psychology, published in 2012 (<nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3482144/">part I</a><span class="oa" title="Open
Access"></span></nobr>, <a href="https://dx.doi.org/10.1037%2Fa0029334">part II</a><sup><a href="http://gestaltrevision.be/pdfs/A%20century%20of%20Gestalt%20psychology%20in%20visual%20perception%20II.pdf">PDF</a></sup>).</span> <img
src="img/gestalt_laws.png" alt="Illustration of gestalt laws" /></p>
<p>In order to quantify the strength of perceptual grouping between pairs, we need to understand why and how our visual system binds image fragments together.</p>
<h3 id="legibility">Legibility</h3>
<p>Vision and reading are not the same thing, and neither an even texture nor perfectly balanced pair grouping guarantees good legibility. So what should we optimize for?</p>
<p>As we will see, reading is actually a collection of different modes of perception, each of which corresponds to a different stage of reading acquisition in childhood and to a different brain region. A type designer manipulating letter shapes is performing an entirely different mental task than someone reading text set in their font. In fact, it appears that most designers are not directly optimizing for legibility at all. This topic requires a discussion of the various letter- and word-classifying neural networks in our brain, of their strengths and weaknesses, and of the importance of word-dividing spaces in fusional languages like English.</p>
<h2 id="a-brief-tour-through-our-visual-system-area-v1">A brief tour through our visual system: area V1</h2>
<p>Our brain’s visual processing system is divided into multiple regions, each of which represents the incoming visual imagery at a different level of abstraction. Anything we see—landscapes, patterns, text—activates neurons in each one of these brain areas. While neurons in the lower-level areas respond to concrete details in the visual input, neurons in higher-level areas respond to the presence of particular configurations of such details. Both low- and higher-level areas are involved in perception, allowing us to simultaneously experience the raw visual qualia <em>and</em> comprehend what we see on a more abstract level.</p>
<p>Whether we are looking at an apple (and recognizing it as such), a tree (and recognizing it as such), or a word (and reading it)–most of of the neurons involved are the same.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-11" class="margin-toggle"></nobr><label for="mn-marginnote-11" class="margin-toggle"></label><span class="marginnote"> All visual input activates several pieces of visual cortex before reaching dedicated object-detection circuitry such as the Visual Word Form Area (VWFA). We will discuss mainly V1, V2, and V4 (the so-called <a href="https://en.wikipedia.org/wiki/Two-streams_hypothesis">ventral stream</a><sup>W</sup>); many other regions exist that are dedicated to visual tasks less relevant to reading, such as keeping track of moving objects. This big-picture view of reading was perhaps most clearly articulated in <a href="https://doi.org/10.1016/S1364-6613(03)00134-7">this 2003 article</a><sup><a href="http://ling.umd.edu/~ellenlau/courses/ling646/McCandliss_2003.pdf">PDF</a></sup> by the prolific reading researchers McCandliss, Cohen and Dehaene. As we will discuss later, the VWFA is actually multiple areas.</span> <img
src="img/vision_model.png" alt="Vision model"></p>
<p>Many readers may have had some exposure, however superficial, to the concept of deep convolutional networks. It is tempting to conceptualize the architecture of the visual cortex as such a network: yes, raw visual input enters at the bottom, undergoes convolution through multiple layers, then comes out the top as a neat classification of a word. But perception, and perceptual grouping in particular, is a dynamic process. It is not a computation with input and output, but a dance of electrical activity that evolves through <nobr>time.<input type="checkbox" id="sn-sidenote-12" class="margin-toggle"></nobr><label for="sn-sidenote-12" class="margin-toggle sidenote-number"></label><span class="sidenote"><a href="http://nxxcxx.github.io/Neural-Network/">This interactive visualization</a> is far from realistic but a much more useful visual metaphor than feed-forward deep learning diagrams.</span> At high resolution, it unfortunately takes unworkably large computational resources to simulate these dynamics accurately, so feed-forward convolutional nets may well end up playing a role in the design of letterfitting models, but for now, the goal is to gain an appreciation for our neural feedback loops.</p>
<p>With that in mind, let’s go on a brief tour through our visual system.</p>
<h3 id="edge-and-line-detection-by-simple-cells">Edge and line detection by simple cells</h3>
<p>Sensory input from the eye travels up the optic nerve, through the lateral geniculate nucleus (LGN) on the brain’s thalamus, to the visual cortex at the very back of the <nobr>head.<input type="checkbox" id="sn-sidenote-13" class="margin-toggle"></nobr><label for="sn-sidenote-13" class="margin-toggle sidenote-number"></label><span class="sidenote">For our computational purposes, we will ignore any image processing performed by the retina and thalamus, such as the luminance adaptation and pooling operations performed by <a href="https://en.wikipedia.org/wiki/Retinal_ganglion_cell">retinal ganglion cells</a><sup>W</sup>.</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-14" class="margin-toggle"></nobr><label for="mn-marginnote-14" class="margin-toggle"></label><span class="marginnote">Illustration adapted from Nicolas Henri Jacob (1781–1871), <em>Traité complet de l’anatomie de l’homme comprenant la médecine opératoire, par le docteur Bourgery</em>. Available in the <a href="https://anatomia.library.utoronto.ca/">Anatomia Collection</a> of the Thomas Fisher Rare Book Library, University of Toronto.</span> <img src="img/vc_anatomy.png" alt="Anatomy; location of the visual cortex"></p>
<p>The first phalanx of cells—the primary visual cortex, or V1—performs what amounts to a band-filtered wavelet decomposition. Each neuron here is <nobr>retinotopically<input type="checkbox" id="sn-sidenote-15" class="margin-toggle"></nobr><label for="sn-sidenote-15" class="margin-toggle sidenote-number"></label><span class="sidenote">That is, neurons are laid out to <a href="https://en.wikipedia.org/wiki/Retinotopy">roughly mirror the organization of the retina</a><sup>W</sup>, such that adjacent photoreceptors are connected to nearby neurons in the cortex.</span> connected directly to a small contiguous group of photoreceptors, its <em>receptive field</em> <nobr>(RF),<input type="checkbox" id="sn-sidenote-16" class="margin-toggle"></nobr><label for="sn-sidenote-16" class="margin-toggle sidenote-number"></label><span class="sidenote">To be clear, the majority of neurons physically located in V1 don’t actually receive direct input from the eye but rather just serve as local connections to facilitate basic image enhancement, such as contrast normalization, but we will skip here the organization of V1’s layers.</span> and activates whenever a particular subset of the receptors detects light but the others don’t. The on/off subsets are laid out such that each neuron effectively detects a small piece of a line or edge of a particular size and orientation somewhere in the field of <nobr>vision.<input type="checkbox" id="sn-sidenote-17" class="margin-toggle"></nobr><label for="sn-sidenote-17" class="margin-toggle sidenote-number"></label><span class="sidenote">This is a relatively well-known concept, because the same kinds of receptive fields tend to emerge in the first layer of image-classifying convolutional networks. For those readers completely unfamiliar with these ideas, I recommend watching <a href="https://www.youtube.com/watch?v=NnVLXr0qFT8">this introductory animation</a>, followed by <a href="https://www.youtube.com/watch?v=mtPgW1ebxmE">this Allen Institute talk</a> about the visual system, followed by <a href="https://www.youtube.com/watch?v=T9HYPlE8xzc">this in-depth MIT lecture</a> on the anatomical details.</span></p>
<p><img src="img/edge_line_rfs.png" /></p>
<p>These neurons are called <em>simple cells</em>, and we can easily predict their response to a given input, depending on the tuning and location of their receptive <nobr>fields.<input type="checkbox" id="sn-sidenote-18" class="margin-toggle"></nobr><label for="sn-sidenote-18" class="margin-toggle sidenote-number"></label><span class="sidenote">David Hubel and Torsten Wiesel first discovered this in the 1950s by showing patterns of light to a cat after sticking electrodes into its brain (Youtube has a <a href="https://www.youtube.com/watch?v=Yoo4GWiAx94">video of said cat</a>). The researchers went on to win a Nobel Prize for their experiments.</span> In software models, the filtering operation performed by simple cells is typically implemented as Fourier-domain multiplication with a bank of complex band-pass filters, each of which is tuned to a particular orientation and spatial frequency. Given a dark vertical bar as visual input, sets of similarly-tuned V1 simple cells might respond as such:</p>
<p><img src="img/single_i_example.png" /></p>
<h3 id="complex-cells">Complex cells</h3>
<p>As it turns out, some V1 neurons are less sensitive to phase than others, and some may even respond equally to both lines and edges, as long as scale and orientation match their tuning. Those cells are called <em>complex cells</em><nobr>.<input type="checkbox" id="sn-sidenote-19" class="margin-toggle"></nobr><label for="sn-sidenote-19" class="margin-toggle sidenote-number"></label><span class="sidenote">Simple and complex cells lie along a spectrum of phase specificity, which is brilliantly explained by <a href="https://doi.org/10.1101/782151">this recent paper</a><sup><a href="https://www.biorxiv.org/content/biorxiv/early/2019/09/25/782151.full.pdf">PDF</a></sup> by Korean researchers Gwangsu Kim, Jaeson Jang and Se-Bum Paik. But it seems that there’s even more to the story, as complex cells seem to <a href="https://doi.org/10.1038/nn.2861">change their simpleness index</a><sup><a href="https://hal.archives-ouvertes.fr/hal-00660536/document">PDF</a></sup> in response to their input as well.</span> Thanks to their phase invariance, complex cells can extract key structural information at the expense of colour and contrast data. They respond wherever the frequency scale and orientation matches their tuning. In the following picture, all complex cell responses of a given frequency scale are shown together, regardless of the orientation:</p>
<p><img src="img/single_i_complex_example.png" /></p>
<p>Coincidentally, contrast and colour are irrelevant to reading—we can read black-on-white just as well as white-on-black—suggesting that it is mainly complex cells that provide the relevant signals to higher-level brain <nobr>areas.<input type="checkbox" id="sn-sidenote-20" class="margin-toggle"></nobr><label for="sn-sidenote-20" class="margin-toggle sidenote-number"></label><span class="sidenote">In practice, it is measurably easier to read dark text on light backgrounds. Not only do light backgrounds make the pupil contract, <nobr><a href="http://dx.doi.org/10.1016/j.apergo.2016.11.001">creating a sharper image</a><sup><a href="http://jdobr.es/pdf/Dobres-etal-2017-Ambient.pdf">PDF</a></sup></nobr>, but V1 outputs are also <nobr><a href="https:///doi.org/10.1523/JNEUROSCI.1991-09.2009">stronger for darker colours</a><span class="oa" title="Open Access"></span></nobr>, which may contribute to shape perception in higher-level stages. Nevertheless, reading is primarily shape- and not colour-based.</span></p>
<p>To be clear, this does not mean that the signals from simple cells are lost or discarded. Just like the signals from colour-detecting cells in the so-called <em>blob</em> regions of V1, which are not further discussed here, the signals from simple cells do contribute both to our experience of vision and to the activity of higher-level brain regions. For reading (and thus letterfitting) purposes, however, we will focus on the responses of complex cells.</p>
<h3 id="lateral-inhibition">Lateral inhibition</h3>
<p>Neurons in V1 (and elsewhere in the cortex) use lateral connections to inhibit their neighbours. This is called <em>lateral inhibition</em>. Because the strength of the inhibition depends directly on the strength of the neuron’s own activation, this setup helps the most active neuron to mute its neighbours. This sharpens the response landscape, which is necessary in practice considering that neurons tuned <em>almost</em> to the right orientation and frequency (but not quite) will still fire quite a bit, effectively adding noise to the signal. Lateral inhibition means that V1 neuron’s firing rates take some time to stabilize, something that models may need to take into account.</p>
<h3 id="contrast-sensitivity-to-spatial-frequencies">Contrast sensitivity to spatial frequencies</h3>
<p><nobr><input type="checkbox" id="mn-marginnote-21" class="margin-toggle"></nobr><label for="mn-marginnote-21" class="margin-toggle"></label><span class="marginnote"><img src="img/csf.png" alt="Contrast sensitivity function">Contrast sensitivity function. The vertical gradient in contrast is uniform across the image, but we most easily perceive the mid-frequency gratings even at lower contrasts. Note that the red line, shown here only for illustrative purposes, may not match the contrast sensitivity function you experience at your current viewing distance and screen settings.</span> Another aspect of vision that appears to manifest quite early during visual processing—setting aside the optical limitations of our eye—is our specific sensitivity to spatial frequencies. Humans respond particularly well to angular frequencies of about 2–5 cycles per degree, and unsurprisingly this translates to reading speed as well, <em>especially</em> under low-contrast <nobr>conditions.<input type="checkbox" id="sn-sidenote-22" class="margin-toggle"></nobr><label for="sn-sidenote-22" class="margin-toggle sidenote-number"></label><span class="sidenote">See studies like <nobr><a href="https://doi.org/10.1167/9.9.16">Chung and Tjan (2009)</a><span class="oa" title="Open
Access"></span></nobr>, <nobr><a href="https://doi.org/10.1167/9.9.4">Oruç and Landy (2009)</a><span class="oa" title="Open
Access"></span></nobr>, and many others.</span> This, of course, is a key reason why e.g. hairline type is difficult to read at smaller-than-huge sizes and a comparatively wide fit. The reader’s contrast sensitivity function may in fact contribute to the exact relative weighting of the laterally-inhibitive connections; in other words, mid-scale signals may outcompete fine-scale signals by default. Even lacking perfect information about such correlations, we can point to the contrast sensitivity function as the most basic biological <em>raison d’être</em> for optical sizes in typography, and to models based on spatial frequency channels as a natural fit for this aspect of letterfitting.</p>
<p>We will return to the question of how V1 outputs vary in response to changing pair distances in a later section. For now, let’s move on to how these signals are processed in subsequent areas.</p>
<h2 id="area-v2-portilla-simoncelli-texture-correlations-and-crowding-effects">Area V2, Portilla-Simoncelli texture correlations, and crowding effects</h2>
<p>Area V1 deconstructs the incoming imagery into thousands of edge and line fragments. Area V2 helps find patterns in those signals, patterns that form the basis for the perceptual grouping effect we are interested in.</p>
<p>Each neuron in V2 takes its input from a combinations of neurons in <nobr>V1,<input type="checkbox" id="sn-sidenote-23" class="margin-toggle"></nobr><label for="sn-sidenote-23" class="margin-toggle sidenote-number"></label><span class="sidenote">Again, we will skip here a discussion of the various layers and interneurons of V2.</span> creating receptive fields that can be twice as large as those in V1. For each V2 neuron, the choices of V1 inputs are endless (within the constraints of approximate retinotopicity), and indeed, V2 contains a vast variety of cells representing all kinds of different correlations between V1 signals: correlations between V1 simple cells and complex cells, between V1 cells of different scales and orientations, and between V1 cells at different spatial locations. <nobr><input type="checkbox" id="mn-marginnote-24" class="margin-toggle"></nobr><label for="mn-marginnote-24" class="margin-toggle"></label><span class="marginnote"><img src="img/v1_v2.png" alt="Connections from V1 to V2">V2 cells take their input from a nearby V1 cells, correlating receptive fields across dimensions of space, simpleness/complexity, orientation, and spatial frequency scale.</span></p>
<p>Presumably, the ability to respond to correlations—not just sums—of inputs from V1 is conferred to V2 neurons by their nonlinear activation curve. Consider a toy example in which two V1 neurons each fire with rates between 0 and 1.0. Then a V2 neuron with the following activation curve would fire only if <em>both</em> inputs are sufficiently active, summing to at least 1.5, thereby implementing correlation:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-25" class="margin-toggle"></nobr><label for="mn-marginnote-25" class="margin-toggle"></label><span class="marginnote">Shown on the left is another hyperbolic ratio function. But even simple squaring nonlinearities would allow computing correlations; Anthony Movshon and Eero Simoncelli <a href="https://doi.org/10.1101/sqb.2014.79.024844">call this</a><sup><a href="http://symposium.cshlp.org/content/early/2015/04/29/sqb.2014.79.024844.full.pdf">PDF</a></sup> the “cross term”, referring to the <span class="math inline">ab</span> in <span class="math inline">(a+b)^2 = a^2 + 2ab + b^2</span>. Finally, the dashed line shows the deep-learning equivalent nonlinearity <span class="math inline">\mathrm{ReLU(x-1.0)}</span>.</span> <img
src="img/v2_nonlinearity.png" alt="Nonlinear activation of V2 neurons
enables computation of correlations"></p>
<h3 id="texture-detection-via-v2-statistics">Texture detection via V2 statistics</h3>
<p>Unfortunately, we have no direct measurements of what each of these neurons respond to most strongly. However, pre-trained image classification networks contain units in their early convolutional layers that are, presumably, somewhat analog to V2 cells. By iteratively adjusting white noise until these units are maximally activated, we can estimate what kinds of correlations in the input they are tuned to:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-26" class="margin-toggle"></nobr><label for="mn-marginnote-26" class="margin-toggle"></label><span class="marginnote">These images were adapted from an <nobr><a href="https://doi.org/10.23915/distill.00024">interactive online article</a><span class="oa" title="Open
Access"></span></nobr> by Chris Olah and his colleagues at OpenAI, who have published lots of neat approaches to explain and interpret the inner workings of convolutional networks. Note that in the human brain, colour information is not integrated quite like it is here.</span> <img src="img/v2_texture_neurons.png" alt="Some kernels from Inception V1"/></p>
<p>On their own, many of these correlations may appear to be meaningless. Together, however, they describe the local texture of an image. As it turns out, a mere few dozen of such correlations are enough to fool human texture perception: we can iteratively generate fake images, starting again from white noise, that result in the same combination of local averages of these presumed V2 responses as in the original <nobr>image.<input type="checkbox" id="sn-sidenote-27" class="margin-toggle"></nobr><label for="sn-sidenote-27" class="margin-toggle sidenote-number"></label><span class="sidenote">The first iteration of this <nobr><a href="https://doi.org/10.1023/A:1026553619983">idea</a><sup><a href="https://www.cns.nyu.edu/pub/lcv/portilla99-reprint.pdf">PDF</a></sup></nobr> came about in 1999, long before the heyday of convolutional deep nets, and is due to to Javier Portilla and Eero Simoncelli. Two decades later, these “Portilla-Simoncelli textures” have inspired countless psychological studies and attempts to refine the model.</span> If the local averaging takes place over a large area, as is the case in the visual periphery, this can result in very distorted imagery that nonetheless appears uncannily real:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-28" class="margin-toggle"></nobr><label for="mn-marginnote-28" class="margin-toggle"></label><span class="marginnote">The “image metamer” shown here was <a href="https://dx.doi.org/10.1038%2Fnn.2889">generated</a><sup><a href="https://www.cns.nyu.edu/pub/eero/freeman10-reprint.pdf">PDF</a></sup> by Jeremy Freeman and Eero Simoncelli in 2011 based on the same principle of matching image statistics. As in the human brain, the authors averaged the statistics over a wider area in the periphery than in the fovea. When focusing strictly on the image center (best viewed closely or after zooming in), the metamer is difficult to distinguish from the original.</span> <img src="img/metamers.png" alt="From 'Metamers of the ventral
stream'"></p>
<p>As evident here, a mere approximation of these averaged image statistics measured by V2 is enough to simulate, with eerie fidelity, how we perceive our visual periphery. This is no coincidence: after all, higher-level areas (here, V4) precisely respond to particular configurations of such V2 neurons, so synthesizing images which evoke similar V2 activations will also result in similar higher-level perceptions, even if the actual input signals are quite <nobr>different.<input type="checkbox" id="sn-sidenote-29" class="margin-toggle"></nobr><label for="sn-sidenote-29" class="margin-toggle sidenote-number"></label><span class="sidenote">One could think of this as the bizarro-version of an <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">adversarial input</a><sup>W</sup>.</span></p>
<h3 id="texture-statistics-and-letterfitting">Texture statistics and letterfitting</h3>
<p>That V2 neurons so effectively capture local image statistics presents us with a first opportunity to reify the heretofore vague concept of typographic “colour” into something concrete and computable: namely, local combinations of such (simulated) V2 responses. If these remain uniform across the whole page, the texture is perceived as even:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-30" class="margin-toggle"></nobr><label for="mn-marginnote-30" class="margin-toggle"></label><span class="marginnote">Here, Javier Portilla and Eero Simoncelli demonstrated how a set of V2 statistics computed and averaged over an image of text could be used to extrapolate the perceived texture outwards. The comparably poor quality of this example taken from their paper should not be taken as reason to dismiss the idea; it was generated at low resolution over two decades ago and averaged the statistics too aggressively. Many more sophisticated variants of the model have since been published, with promising results especially on natural scenes.</span> <img
src="img/text_v2_texture.png" alt="Texture extension on image of text.
From Portilla and Simoncelli, 2000."></p>
<p>In a truly colour-based letterfitting strategy, which should be relatively easy to implement, we would iteratively adjust pair distances within an image of text until a chosen set of V2 responses is nice and uniform across the entire image. And indeed, this would probably be the most effective and biologically faithful approach to achieve a perfectly even texture. Unfortunately, in shifting letters to optimize solely for overall colour, the algorithm would disfigure the <em>gestalten</em> of individual words, at times even rendering them <nobr>illegible.<input type="checkbox" id="sn-sidenote-31" class="margin-toggle"></nobr><label for="sn-sidenote-31" class="margin-toggle sidenote-number"></label><span class="sidenote">In the theoretical limit, a perfectly uniform texture determined by a fixed number of such correlations would need to be perfectly periodical, thereby constraining our test image, at best, to a set of repeating letters.</span> For that reason, it does not make for a good optimization target, even though the texture of well-fitted text is typically (but not necessarily) quite even across the page.</p>
<h3 id="surround-suppression">Surround suppression</h3>
<p>When V2 neurons detect texture-like correlations between neighbouring V1 neurons, they tend to return inhibitive feedback signals, especially to the V1 neurons in the center. This kind of “surround suppression”, which acts in addition to the lateral inhibition between V1 cells discussed above, helps mute V1 activity inside similarly-textured <nobr>areas.<input type="checkbox" id="sn-sidenote-32" class="margin-toggle"></nobr><label for="sn-sidenote-32" class="margin-toggle sidenote-number"></label><span class="sidenote">Although we are mainly interested in suppressive feedback here, multiple mechanisms seem to be implicated in the modulation of visual signals; the 2019 <nobr><a href="https://doi.org/10.1167/19.4.12">EEG study</a><span class="oa" title="Open
Access"></span></nobr> by Schallmo et al. contains a comprehensive review. For attempts to reproduce the effect using computational modelling, see e.g. the work of Ruben Coen-Cagli et al. <nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4624479/">here</a><span class="oa" title="Open Access"></span></nobr>.</span> Because this mechanism <em>leasts</em> affects the boundaries between differently-textured surfaces, it allows us to perceive the outlines of textured objects even when those are weaker (in terms of raw V1 responses) than the textures themselves: think of a Zebra on the savanna, or of a cluster of regular strokes on a white background—perhaps a word on a page!</p>
<p><img src="img/surround_suppression.png" alt="surround suppression example" /></p>
<p>This surround suppression therefore is a kind of early perceptual grouping mechanism, enabled by correlation-detecting V2 <nobr>neurons.<input type="checkbox" id="sn-sidenote-33" class="margin-toggle"></nobr><label for="sn-sidenote-33" class="margin-toggle sidenote-number"></label><span class="sidenote">Another way to think of this, from the perspective of <a href="https://en.wikipedia.org/wiki/Predictive_coding">predictive coding</a><sup>W</sup>, is as compression of redundant signals, as <nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5836998/">pointed out</a><span class="oa" title="Open
Access"></span></nobr> by Laurence Aitchison and Máté Lengyel.</span> The strength of the segmentation certainly depends greatly on the scale, pattern, and contrast of the objects involved, so it is difficult to say to what degree it affects the perception of words. But inhibitive (as well as facilitatory) feedback is likely present between higher-level brain areas as well, and the corresponding dynamics are implicated in other grouping-related phenomena as well, such as <em>crowding</em>, which we will address later.</p>
<p>Finally, it doesn’t take a cognitive scientist to know that fonts designed based on consistent, repeating elements are easier to <nobr>read.<input type="checkbox" id="sn-sidenote-34" class="margin-toggle"></nobr><label for="sn-sidenote-34" class="margin-toggle sidenote-number"></label><span class="sidenote">Still, they have studied it more thoroughly than one might expect; see <nobr><a href="https://doi.org/10.3758/s13414-011-0220-9">this 2012 review</a><span class="oa" title="Open
Access"></span></nobr> by Thomas Sanocki and Mary Dyson.</span> This so-called “font tuning” has in the past been attributed to an unexplained ability of letter detectors (which we discuss below) to rapidly adjust their filter kernels to font designs. But of course, we here have a perfectly parsimonious explanation in V2 texture correlations: stylistic similarities between letters are simply redundancies in spatial statistics. Texture-detecting neurons absorb and suppress them, such that primarily the non-repeating features (i.e., terminals and horizontals rather than stems and serifs) maintain enough salience to excite letter detectors in higher-level areas. In “frankenfonts”, such suppression is impossible, resulting in more irrelevant visual data impinging on letter detectors, resulting in poorer letter classification performance. Perhaps future design tools could visualize this mechanism to help designers find inconsistencies in their fonts.</p>
<h3 id="contour-integration-and-v1-feedback">Contour integration and V1 feedback</h3>
<p>Not all V2 neurons respond to such peculiar V1 correlations, expressing elements of texture. Some pick up on signals with more human-interpretable salience, such as continuous edges and lines. Experiments suggest that they do so by responding to V1 complex cells that co-align:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-35" class="margin-toggle"></nobr><label for="mn-marginnote-35" class="margin-toggle"></label><span class="marginnote">Each cell corresponds to a V1 complex cell tuned to a certain orientation (the distribution in frequency scales is ignored here). Note that the number of V1 cells is exaggerated for effect. This neuron responds to collinear V1 activations suggesting the presence of a horizontal contour, even if curved (see the gray stroke in the sample shown). It may be inhibited by parallel flanking contours and perpendicular contours, although this is less clear. This pattern has been called “association field”, “bipole”, and many other names in papers going back to the 1990s.</span> <img src="img/v2_contour_integration.png"
alt="Receptive fields of a V2 contour integration neuron"></p>
<p>This allows these V2 cells to detect continous contours, even if these contours are curved or <nobr>interrupted.<input type="checkbox" id="sn-sidenote-36" class="margin-toggle"></nobr><label for="sn-sidenote-36" class="margin-toggle sidenote-number"></label><span class="sidenote">Two studies showing this most clearly are by <a href="https://doi.org/10.1016/j.neuron.2014.03.023">Minggui Chen et al. from 2014</a><span class="oa" title="Open Access"></span> and by <a href="https://doi.org/10.1016/j.neuron.2017.11.004">Rujia Chen et al. from 2017</a><span class="oa" title="Open Access"></span>.</span> Interrupted contours are a constant challenge to the vision system: the edges of an object can be occluded not only by other objects—think tree branches in front of a mountain—but also by the spider web of light-scattering nerve bundles and capillaries that carpet our <nobr>retina.<input type="checkbox" id="sn-sidenote-37" class="margin-toggle"></nobr><label for="sn-sidenote-37" class="margin-toggle sidenote-number"></label><span class="sidenote">Not to mention our <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)">blind spot</a><sup>W</sup>.</span> Contour-integrating V2 cells thus help us perceive contours even where we cannot actually see them. Of course, the same principle applies to texture integration across space.</p>
<p>Having thus detected a piece of contour, the V2 neuron now sends an amplifying signal to all of its V1 inputs, which in turn increases the input to the V2 cell itself, creating a positive feedback loop between V1 and V2. Crucially, however, this feedback only amplifies neurons that are already firing; it does not induce activity in other inputs (and may even suppress <nobr>them).<input type="checkbox" id="sn-sidenote-38" class="margin-toggle"></nobr><label for="sn-sidenote-38" class="margin-toggle sidenote-number"></label><span class="sidenote">Physiologically, this kind of modulatory amplification may be related to increased spike synchrony between neurons, as explored in <nobr><a href="https://doi.org/10.1152/jn.01142.2015">this 2016 study</a><span class="oa" title="Open
Access"></span></nobr> by Wagatsuma et al.</span> <nobr><input type="checkbox" id="mn-marginnote-39" class="margin-toggle"></nobr><label for="mn-marginnote-39" class="margin-toggle"></label><span class="marginnote"><img src="img/contour_integration_example.png" alt="Contour
integration example">Typical contour integration test image demonstrating contour pop-out. Adapted from <nobr><a href="https://doi.org/10.3389/fpsyg.2013.00356">Roudaia et al.</a><span class="oa" title="Open
Access"></span></nobr>, 2013.</span> Thanks to this feedback loop, contiguous contours pop out to us perceptually in a matter of milliseconds, while non-contour features (like the dot in the illustration below) do not:</p>
<p><img src="img/v2_contour_integration_2.png"></p>
<p>This kind of feedback loop is a simple grouping mechanism of its own, and responsible for many (though not all) observations of <em>prägnanz</em> due to collinearity. As we will see below, however, it is also an important ingredient in letter and word perception.</p>
<h2 id="v4-and-higher-level-areas">V4 and higher-level areas</h2>
<p>The next area of the visual cortex, area V4, mirrors the architecture of V2 in that it performs a set of convolutions detecting correlations between its inputs. It is reasonable to conceptualize V4 as V2, only with larger receptive fields. Its neurons respond, once again, to a large variety of spatial correlations in the input image, although these correlations can be more complex, looking perhaps more like this:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-40" class="margin-toggle"></nobr><label for="mn-marginnote-40" class="margin-toggle"></label><span class="marginnote">Again, these images are taken from <nobr><a href="https://doi.org/10.23915/distill.00024">Olah et al., 2020</a><span class="oa" title="Open
Access"></span></nobr>. The images give a good intuition for the higher complexity of the patterns detected in V4.</span> <img src="img/v4_texture_neurons.png" alt="higher-level receptive fields from InceptionNet"></p>
<p>Once again, some neurons tend to be more tuned to textures while others detect straight or curved contour fragments, although there certainly is overlap between the two <nobr>categories.<input type="checkbox" id="sn-sidenote-41" class="margin-toggle"></nobr><label for="sn-sidenote-41" class="margin-toggle sidenote-number"></label><span class="sidenote">As in the case in V4, at least in macaques, as shown by studies like <nobr><a href="https://doi.org/10.1523/JNEUROSCI.3073-18.2019">this one</a><span class="oa" title="Open
Access"></span></nobr> by Anitha Pasupathy and her collaborators.</span> Just as in V2, the contour detectors integrate smaller contour fragments across a larger region. However, the larger receptive fields of V4 allow for the target contours to be substantially offset from the center of the neuron’s receptive field. As such, a neuron centered <em>on</em> the target object can detect parts of its contour:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-42" class="margin-toggle"></nobr><label for="mn-marginnote-42" class="margin-toggle"></label><span class="marginnote">Note how all shapes have in common the convexity on the lower left.</span> <img src="img/v4_rf.png" alt="Receptive field and some example stimuli for a V4
object-centered contour-detecting cell"></p>
<h2 id="perceptual-grouping-based-on-border-ownership">Perceptual grouping based on border ownership</h2>
<p>Consider that navigating our natural environment requires us to correctly identify three-dimensional objects in three-dimensional space. But the shape of these objects varies heavily depending on perspective—after all, we only see a two-dimensional projection of reality—and is available only as a collection of the abovementioned V4 contour fragments. What’s more, the contour detectors will activate on <em>both sides</em> of each object:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-43" class="margin-toggle"></nobr><label for="mn-marginnote-43" class="margin-toggle"></label><span class="marginnote">Two V4 contour detectors, tuned to the same eccentricity, angle, and curvature, activate in response to a dark blob shape. One of them (shown in red) is centered on the object as expected, the other is centered outside. Many (though not all) of these detectors are connected mainly to V1 complex cells, rendering them more responsive to the sheer presence of an edge than to its contrast polarity.</span> <img src="img/v4_rf_outside.png" alt="activation of V4 contour receptor on outside"></p>
<p>How can we recognize a half-overlapped object, discount its perspective foreshortening and assign it a relative depth, going only by a population of V4 contour detectors, half of which are gratuitously detecting the objects’ outsides? The solution lies in feedback loops that enable perceptual grouping.</p>
<p>The first feedback loop connects V4 with a special class of V2 neurons called <em>border ownership cells</em> or B-cells. These B-cells, like the V2 contour-integrating cells already discussed, detect the presence of edges based on the activity of V1 complex cells. While they are agnostic to the edge’s contrast polarity, B-cells fire only if they are on one particular side of an object. For instance, the B-cell whose receptive field is marked in red below only detects edges on the <em>left side</em> of objects, as indicated here by the small protrusion pointing toward the <nobr>right.<input type="checkbox" id="sn-sidenote-44" class="margin-toggle"></nobr><label for="sn-sidenote-44" class="margin-toggle sidenote-number"></label><span class="sidenote">Almost everything we know about border ownership networks is owed to Johns Hopkins researcher Rüdiger von der Heydt and his students. His <nobr><a href="https://doi.org/10.3389/fpsyg.2015.01695">2015 review</a><span class="oa" title="Open
Access"></span></nobr> summarizes the key findings well.</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-45" class="margin-toggle"></nobr><label for="mn-marginnote-45" class="margin-toggle"></label><span class="marginnote">Here, the B-cell responds to stimuli 1 and 2, but not 3 and 4.</span> <img src="img/b_cell_1.png" alt="B cell illustration"></p>
<p>This is remarkable. After all, the B-cell only sees a single edge. It cannot know which part of the object it is on; its receptive field is much too small. So its activity must be gated by a neuron which does: namely, one of our higher-level V4 <nobr>neurons.<input type="checkbox" id="sn-sidenote-46" class="margin-toggle"></nobr><label for="sn-sidenote-46" class="margin-toggle sidenote-number"></label><span class="sidenote">Lateral inhibition from other V2 neurons cannot explain this behaviour, because horizontal connections conduct <nobr><a href="https://dx.doi.org/10.1152%2Fjn.00928.2010">too slowly</a><span class="oa" title="Open
Access"></span></nobr> to explain the lab-measured response times of B-cells.</span> The object owning the edge fragment could have any shape and size, so <em>all</em> active V4 neurons whose contour templates coincide with the edge fragment send amplifying signals to our B-cell. In turn, our B-cell directly contributes to their activation, establishing a positive feedback loop:</p>
<p><img src="img/bg_feedback_0.png" alt="B-cell feedback loop"></p>
<p>There is an entire population of B-cells, distributed across V2’s retinotopy. For instance, consider a right-side B-cell (blue below) neighbouring our left-side B-cell. Both B-cells are engaged in feedback loops with V4 neurons while simultaneously inhibiting local competitors—i.e., each other—in proportion to their own activation strength (recall our discussion of lateral inhibition in V1):</p>
<p><img src="img/bg_feedback_1.png" alt="B-cell feedback loop"></p>
<p>If the interior (red) V4 cells now were to fire more strongly than the exterior (blue) ones, then the inward-pointing (red) B-cells would quickly inhibit the outward-pointing (blue) ones, firmly establishing that the border belongs to an object on the right. What would cause the interior (red) V4 cells to dominate?</p>
<p>Research suggests that higher-level cells, perhaps in the posterior inferotemporal cortex, respond to combinations of V4 contour-detecting neurons centered on the same retinal location. Such cells effectively group together the borders owned by an object, and are therefore called G-cells.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-47" class="margin-toggle"></nobr><label for="mn-marginnote-47" class="margin-toggle"></label><span class="marginnote">Here, the G-cell is shown as a blurred circle around a center. The blurred circle corresponds to the location of contours that this G-cell responds to.</span> <img src="img/bg_feedback_2.png" alt="B-cell feedback loop"></p>
<p>Because the external (blue) B-cells and V4 contour signals do not combine to excite a higher-level G-cell, they do not receive positive feedback, and lateral competition (between B-cells, and likely also between V4 and G-cells) quickly silences them.</p>
<p>The exact receptive field of each G-cell is likely quite unique, but a popular approach is to assume that they are <nobr>circular:<input type="checkbox" id="sn-sidenote-48" class="margin-toggle"></nobr><label for="sn-sidenote-48" class="margin-toggle sidenote-number"></label><span class="sidenote">The first to run a simulation of this idea in earnest were <nobr><a href="https://doi.org/10.1152/jn.00203.2007">Edward Craft et al.</a><span class="oa" title="Open
Access"></span></nobr> in 2011.</span></p>
<p><img src="img/bg_rfs.png" alt="Receptive fields of G cells"></p>
<p>This means that the square in the example above would strongly activate a circular G-cell in the center, which takes input from V4 contours on all four sides of the square, and somewhat less strongly activate the circular G-cells along the square’s diagonals, which take input from two sides of the square:</p>
<p><img src="img/g_responses.png" alt="Sample responses of some G cells"></p>
<h3 id="g-cells-skeletonize-shapes">G-cells skeletonize shapes</h3>
<p>Once B-cells and G-cells have settled into an equilibrium, the locus of peak responses of G-cells across different scales neatly represents the skeleton of the shape, shown on the <nobr>right:<input type="checkbox" id="sn-sidenote-49" class="margin-toggle"></nobr><label for="sn-sidenote-49" class="margin-toggle sidenote-number"></label><span class="sidenote">The technical term for this feat is <a href="https://en.wikipedia.org/wiki/Medial_axis">medial axis transform</a><sup>W</sup>.</span></p>
<p><img src="img/g_responses_skeleton.png" alt="Sample responses of some G cells,
forming a skeleton"></p>
<p>This skeletonization step is critical to object recognition. It translates a shape’s contour fragments into its underlying geometric structure in a way that is very robust to perspective <nobr>distortions.<input type="checkbox" id="sn-sidenote-50" class="margin-toggle"></nobr><label for="sn-sidenote-50" class="margin-toggle sidenote-number"></label><span class="sidenote">And indeed, the inferotemporal neurons in macaque monkeys appear to respond to skeleton fragments, such that a small population of such neurons suffices to represent a variety of complicated 3D shapes, as Chia-Chun Hung and colleagues have <nobr><a href="https://doi.org/10.1016/j.neuron.2012.04.029">demonstrated</a><span class="oa" title="Open
Access"></span></nobr>.</span> Conveniently, this ability translates directly to letter recognition. Consider, for instance, our ability to recognize the following four styles of uppercase <em>E</em> with the same ease:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-51" class="margin-toggle"></nobr><label for="mn-marginnote-51" class="margin-toggle"></label><span class="marginnote">Many different uppercase-E designs exist, but all of them share a relationship between the relative locations of large-scale G-cell peaks (within the counters) and smaller-scale peaks (at the terminals). Note that this illustration is tremendously simplified, as it does not take into account competition at the level of B-cells.</span> <img src="img/e_skeletons.png" alt="Some skeletons at different scales"></p>
<p>Although the shared features of the skeletons (counters, stems, etc.) appear at different scales for different letter shapes, they are present in the same configuration for all of them. This is true even for letters that are outlined (last row), as V4 contour detector neurons respond primarily to the contour, not to the fill (or the absence of fill).</p>
<p>When is a stroke perceived as a contour, and when does it turn into a shape of its own right, a shape that owns contours on either side? With letter weights ranging from hairline to ultra-heavy, this is a particularly salient question:</p>
<p><img src="img/letter_weights.png" alt="A range of letter weights"></p>
<p>The hairline letter is, arguably, too thin to allow readers to clearly perceive border ownership of the left and right side of each <nobr>stem.<input type="checkbox" id="sn-sidenote-52" class="margin-toggle"></nobr><label for="sn-sidenote-52" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course this depends on the font size and the contrast sensitivity function, as discussed earlier.</span> At this point, the concept of B-cells and G-cells breaks down; real neurons don’t follow these neat abstractions. It is quite conceivable that specialized cells detect fine lines even in area V4 and beyond, blurring the line between contour- and skeleton-based <nobr>representations.<input type="checkbox" id="sn-sidenote-53" class="margin-toggle"></nobr><label for="sn-sidenote-53" class="margin-toggle sidenote-number"></label><span class="sidenote">See e.g. <nobr><a href="https://doi.org/10.1016/j.neuron.2018.03.009">this 2018 discovery</a><span class="oa" title="Open Access"></span></nobr> of acuity-preserving neural clustering by Yiliang Lu et al., and <nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5693639/">this 2017 simulation</a><span class="oa" title="Open Access"></span></nobr> of contour-dedicated G-cells by Brian Hu et al.</span> For the purposes of our model, however, such considerations are unlikely to be required; we will assume here that the idealized labour division between B- and G-cells is universal.</p>
<p>By the way: that G-cells are presumed to interact with circularly-arranged populations of contour detectors, thus skeletonizating shapes, aligns neatly with the Gestalt principle of convexity: after all, it is circular shapes that are most easily perceived as coherent objects, while more concave contours add visual complexity at the expense of <em>prägnanz</em>. Of course, the preference for convex shapes would not be possible if V4 contour detectors were not also overwhelmingly tuned for convex contour <nobr>fragments.<input type="checkbox" id="sn-sidenote-54" class="margin-toggle"></nobr><label for="sn-sidenote-54" class="margin-toggle sidenote-number"></label><span class="sidenote">As we know they are, thanks to studies like <nobr><a href="https://doi.org/10.1152/jn.2001.86.5.2505">this one</a><span class="oa" title="Open
Access"></span></nobr> and <nobr><a href="https://doi.org/10.1152/jn.01265.2006">this one</a><span class="oa" title="Open
Access"></span></nobr>, again by Anitha Pasupathy and colleagues.</span> Meanwhile, the Gestalt principle of proximity is explained by the fact that G-cells with smaller receptive fields tend to outcompete larger ones.</p>
<h3 id="competitive-contour-integration-along-t-junctions">Competitive contour integration along T-junctions</h3>
<p>Before we discuss how this perceptual grouping plays out across letter pairs and entire words, one more phenomenon should be mentioned for completeness’ sake. Consider the following situation:</p>
<p><img src="img/gestalt_1.png" alt="Gestalt B-cell 1"></p>
<p>Here, the circle is perceived to be in front of, and overlapping, the dark shape. We intuitively assume that the dark shape continues with a straight edge behind the circle, and also that it continues beyond the edges of the image, as if the scene were seen through a window.</p>
<p><img src="img/gestalt_2.png" alt="Gestalt B-cell 2"></p>
<p>The T-junctions created by overlapping shapes activate three sets of convex contour detectors, illustrated here in red, blue, and green. Experiments suggest that between such configurations of contour detectors, it is the straight, continuous one (here in red) that inhibits the other <nobr>two.<input type="checkbox" id="sn-sidenote-55" class="margin-toggle"></nobr><label for="sn-sidenote-55" class="margin-toggle sidenote-number"></label><span class="sidenote">See <nobr><a href="https://doi.org/10.1523/JNEUROSCI.4766-10.2011">here</a><span class="oa" title="Open
Access"></span></nobr> for a painstaking study of these effects by Brittany Bushnell et al. from the Pasupathy lab.</span> As a result, the border ownership at the T-section is assigned to the circle, while the contours of the dark shape disappear near the corners. A similar effect takes place at the edges of the scene. <nobr><input type="checkbox" id="mn-marginnote-56" class="margin-toggle"></nobr><label for="mn-marginnote-56" class="margin-toggle"></label><span class="marginnote"><img src="img/kanisza.png"
alt="kanisza"><br/> Both effects combine in the classic <a href="https://en.wikipedia.org/wiki/Illusory_contours">illusions</a><sup>W</sup> by Gaetano Kanizsa, in the square, which is evoked mainly through its skeleton at the corners, creates T-junctions with the flankers that are invisible yet strong enough to determine relative depth.</span> Now, contour-integrating cells in V2 and V4 are at liberty to connect the loose ends via a straight edge, in collaboration with G-cells that encode the likely skeleton of the dark shape. We thus perceive the dark shape as the corner of a rectangle of indeterminate size. In addition, the T-junctions contribute to the depth perception that layers the two objects—but this is less relevant to perceptual grouping.</p>
<h2 id="attention-crowding-and-the-spread-of-activity">Attention, crowding, and the spread of activity</h2>
<p>To complete the perceptual grouping process, the activity in the B-cells, V4 and G-cells must spread out across objects until it envelops the entire perceptual group. This happens naturally and quickly. For instance, neural activity will quickly spread along contours, thanks to the feedback connections from contour-integrating V2 and V4 neurons to their inputs, which then activate other contour integrators in turn, which feed back to other inputs, etc.</p>
<p>For neural activity to spread out is healthy and useful. But we are interested only in one object, or in one perceptual group of objects, at a time. The challenge lies in containing the spread to the perceptual group of interest only. Naturally, the brain relies on inhibitive connections between neigbouring neurons to slow the spread of activity.</p>
<p>In letterfitting, the objective is for neural activity to spread from one letter to its neighbours both left and right. At this point, we can revisit the illustration from above and consider how “balance between letter pairs” is a question of perceptual grouping mediated by the spread of neural activity (and the inhibition of same):</p>
<p><img src="img/grouping_relativity.png" alt="Illustration of the importance of consistency of fit vs absolute distances."></p>
<h3 id="attention">Attention</h3>
<p>A key concept in this context is <em>attention</em>. Given the ease with which neural activity spreads outwards via feedback and feedforward connections, any little bit of extra activity injected into the network will quickly result in extra activity in the whole nearby network. Of course, neural activity is constantly provided by the retina, and most of that activity comes from the fovea. But we can also pay attention, quite literally: by feeding top-down signals into a single G-cell, for example, we can amplify all associated contour detectors, B-cells and even V1 <nobr>cells.<input type="checkbox" id="sn-sidenote-57" class="margin-toggle"></nobr><label for="sn-sidenote-57" class="margin-toggle sidenote-number"></label><span class="sidenote">For a simulation of how this might play out between G-cells and V1, take a look at Stefan Mihalaş et al.’s <nobr><a href="https://doi.org/10.1073/pnas.1014655108">2011 paper</a><span class="oa" title="Open
Access"></span></nobr>.</span> This can happen both <nobr>consciously<input type="checkbox" id="sn-sidenote-58" class="margin-toggle"></nobr><label for="sn-sidenote-58" class="margin-toggle sidenote-number"></label><span class="sidenote">By which I merely mean “triggered by frontal-lobe areas”, without endorsing any Cartesian notions of dualism.</span> and unconsciously, and top-down attention can be directed to populations of neurons encoding many different things. Attending to a high-level neuron representing a particular object, for instance, is a rapid way to light up said object in the input image. <nobr><input type="checkbox" id="sn-sidenote-59" class="margin-toggle"></nobr><label for="sn-sidenote-59" class="margin-toggle sidenote-number"></label><span class="sidenote">The attention-selected V1 and V2 neurons, of course, have connections to many brain regions besides V4. This had led cognitive scientists to call the early visual cortex a <nobr><a href="https://doi.org/10.1146/annurev-vision-111815-114443">“cognitive blackboard”</a><span class="oa" title="Open
Access"></span></nobr>.</span></p>
<h3 id="crowding-and-grouping">Crowding and grouping</h3>
<p>One situation in which this containment can fail is when texture detectors are recruited into the frenzy of activity, and these texture detectors then compete with smaller-scale feature detectors that we rely on for object identification. In the following illustration, it is very difficult to make out the uppercase V while focusing on the center cross, even though recognizing the left-hand A, which is exactly the same distance away, is no problem. Any attempts to attend to the V just reroutes more activity into the texture representation.</p>
<p><img src="img/crowding_example.png" alt="Example of crowding between letters"></p>
<p>This phenomenon, called <em>crowding</em>, has captured the fascination of reading researchers since the 1970s. The severity of crowding increases with the distance from the fovea, as the periphery contains more, and much larger, texture-detecting <nobr>neurons.<input type="checkbox" id="sn-sidenote-60" class="margin-toggle"></nobr><label for="sn-sidenote-60" class="margin-toggle sidenote-number"></label><span class="sidenote">As a rule of thumb, the spacing needs to be at least half the eccentricity, i.e. to the distance from the fovea (see Herman Bouma’s <a href="https://doi.org/10.1038/226177a0">1970 report</a>).</span> Crowding is also made worse by regularity in the spacing of the flanking <nobr>objects;<input type="checkbox" id="sn-sidenote-61" class="margin-toggle"></nobr><label for="sn-sidenote-61" class="margin-toggle sidenote-number"></label><span class="sidenote">As <nobr><a href="https://doi.org/10.1167/10.10.17">demonstrated in 2010</a><span class="oa" title="Open
Access"></span></nobr> by Toni Sareela et al.</span> again, presumably, because periodicity strengthens texture perception.</p>
<p>That crowding and perceptual grouping are two sides of the same coin—namely, the spreading of activity across neural populations—is a surprisingly recent idea, but a very convincing <nobr>one.<input type="checkbox" id="sn-sidenote-62" class="margin-toggle"></nobr><label for="sn-sidenote-62" class="margin-toggle sidenote-number"></label><span class="sidenote">Michael Herzog’s group at EFPL were the first to strongly advocate for it; see <nobr><a href="https://dx.doi.org/10.1167%2F15.6.5">this review</a><span class="oa" title="Open
Access"></span></nobr> for a great summary of the evidence.</span> Among the many computational models of crowding, only one reacts to most input images in the same way human subjects do: it is the model that simulates crowding as perceptual <nobr>grouping.<input type="checkbox" id="sn-sidenote-63" class="margin-toggle"></nobr><label for="sn-sidenote-63" class="margin-toggle sidenote-number"></label><span class="sidenote">See <nobr><a href="https://doi.org/10.1371/journal.pcbi.1006580">Doerig et al., 2019</a><span class="oa" title="Open
Access"></span></nobr>, for a comparison of approaches, and <a href="http://dx.doi.org/10.1037/rev0000070">Francis et al., 2017</a><sup><a href="https://whitneylab.berkeley.edu/PDFs/Francis_Manassi_2017.pdf">PDF</a></sup> and <nobr><a href="https://doi.org/10.3389/fnbot.2019.00033">Bornet et al., 2019</a><span class="oa" title="Open
Access"></span></nobr>, for the grouping-based model. Also check out <nobr><a href="https://doi.org/10.1101/747394">their attempt</a><span class="oa" title="Open
Access"></span></nobr> to reproduce the effect in capsule networks.</span></p>
<h3 id="summary">Summary</h3>
<p>At this point, let’s recapitulate what happens to the image of letters on a page:</p>
<ol type="1">
<li><p>In a forward sweep from V1 to V4, edges and lines in V1 activate contour-integrating V2 neurons (mostly in the fovea and parafovea) and texture-detecting V2 neurons (mostly in the periphery). These, in turn, activate V4 neurons that detect more complex visual patterns, among them convex contour fragments.</p></li>
<li><p>As these V4 signals begin to excite higher-level brain areas, feedback signals from V2 to V1 and from V4 to V2 begin to rapidly reinforce spatially integrated patterns (mainly contours).</p></li>
<li><p>Surround-suppressive feedback mutes spatially redundant signals, allowing boundaries to pop out between textured surfaces even in the absence of strong contour signals.</p></li>
<li><p>Lateral inhibition between neurons further prevents activity from spreading, as more active neurons can dampen their neighbours. Because signals travel more slowly through intracortical horizontal connections, lateral inhibition takes a bit longer to kick in fully.</p></li>
<li><p>Top-down attention exerted on individual (or small populations of) high-level neurons shifts the dynamics of the entire network. A little bit of attention can go a long <nobr>way.<input type="checkbox" id="sn-sidenote-64" class="margin-toggle"></nobr><label for="sn-sidenote-64" class="margin-toggle sidenote-number"></label><span class="sidenote">Thomas Miconi and Rufin VanRullen <a href="https://doi.org/10.1109/CIMSIVP.2011.5949241">describe how</a><sup><a href="https://hal.archives-ouvertes.fr/hal-00706798/file/miconi_t_11_106.pdf">PDF</a></sup> a little bit of extra activity can effectively shift the entire receptive field of a neuron. In Stefan Mihalaş et al.’s <nobr><a href="https://doi.org/10.1073/pnas.1014655108">2011 simulations</a><span class="oa" title="Open
Access"></span></nobr>, referenced above, increasing G-cell activity by a mere 7% was enough to reproduce the effects seen in human subjects.</span></p></li>
<li><p>As neural activity travels outwards along contours and textures, some regions (retinotopic, not cortical) are suddenly flooded with activity. This new activity, in turn, can command attention, via direct connection to higher-level <nobr>areas.<input type="checkbox" id="sn-sidenote-65" class="margin-toggle"></nobr><label for="sn-sidenote-65" class="margin-toggle sidenote-number"></label><span class="sidenote">The <a href="https://en.wikipedia.org/wiki/Frontal_eye_fields">frontal eye fields</a><sup>W</sup> seem to be one brain region involved in keeping track of visual attention, and in making saccades when necessary.</span></p></li>
</ol>
<p>If this understanding of perceptual grouping is correct, then the objective of letterfitting—or, at least, of the Gestalt-based variant—boils down to ensuring that neural activity can spread uniformly from a letter to both of its neighbours until it envelops the entire word, without coagulating into separate, smaller, more stable perceptual groups.</p>
<h2 id="reading-in-the-brain-from-letter-skeletons-to-words">Reading in the brain: from letter skeletons to words</h2>
<p>Before we explore in more detail how neural activity might bind letters into pairs and words, let’s briefly review how all of this fits in with recent models of <em>reading</em>. In other words: what happens after V4, and what does it tell us about how letterfitting influences legibility?</p>
<p>Researchers broadly agree that reading is based on the same mechanisms as early vision: convolution and feedback. In a first step, <nobr>neurons<input type="checkbox" id="sn-sidenote-66" class="margin-toggle"></nobr><label for="sn-sidenote-66" class="margin-toggle sidenote-number"></label><span class="sidenote">Or constellations of neurons, sometimes referred to as <em>nodes</em>, but here simply called <em>detectors</em>.</span> detect the presence of letters from the skeletons made up of V4 contour fragments. Then, higher-level neurons detect ordered combinations of these letters; next, combinations of combinations; and those eventually activate a population of candidate word detectors associated with said letter combinations.</p>
<p>In the feedback stage, each word detector competes (via lateral inhibition) with the others and sends positive feedback back to the hierarchy of letter-combination detectors that activated it, which also compete. This results in a vigorous electrical back-and-forth for about a quarter of a second, until activity settles on the winning word detector. Because the word detectors are largely at the mercy of the brain’s language circuitry that parses sentences based on grammar and semantic associations, the raw signal from the letter-combination detectors is easily overruled in our awareness.</p>
<h3 id="robust-interactivity-via-n-gram-detectors">Robust interactivity via <em>n</em>-gram detectors</h3>
<p>The archetypal letter-combination detector responds to ordered pairs of letters, often called “open bigrams” in the <nobr>literature.<input type="checkbox" id="sn-sidenote-67" class="margin-toggle"></nobr><label for="sn-sidenote-67" class="margin-toggle sidenote-number"></label><span class="sidenote">Early open-bigram models were primitive and regularly maligned. Today, the idea is no longer under much dispute, in a win for its early champions like Jonathan Grainger and Carol Whitney.</span> Because letters can appear anywhere in the retina, and at any size, we must assume that <em>all</em> pairs present in a word will be detected: for instance, the word <em>cat</em> will trigger the detectors for <em>CA</em>, <em>AT</em>, and <em>CT</em>. Due to the inherent softness of the detectors’ filter <nobr>kernels,<input type="checkbox" id="sn-sidenote-68" class="margin-toggle"></nobr><label for="sn-sidenote-68" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course, the “filter kernels” here refer to the distribution of synapses from input neurons, assumed to be decreasing with retinotopic distance.</span> the exact spatial position of the letters and bigrams is somewhat <nobr>uncertain:<input type="checkbox" id="sn-sidenote-69" class="margin-toggle"></nobr><label for="sn-sidenote-69" class="margin-toggle sidenote-number"></label><span class="sidenote">One of the influential reading models featuring such uncertainty was the 2008 <a href="https://doi.org/10.1037/a0012667">overlap model</a><sup><a href="https://www.uv.es/~mperea/overlapPsychReview.pdf">PDF</a></sup> by Gomez, Ratcliff, and Perea.</span></p>
<p><img src="img/ld_lcd.png" alt="open bigram detection" /></p>
<p>This uncertainty results in the (light, but nonzero) activation of reverse bigrams, which allows us to read wodrs wiht jmbuled <nobr>ltetres,<input type="checkbox" id="sn-sidenote-70" class="margin-toggle"></nobr><label for="sn-sidenote-70" class="margin-toggle sidenote-number"></label><span class="sidenote"><a href="https://en.wikipedia.org/wiki/Transposed_letter_effect">Jumbled letters</a><sup>W</sup> are a crowd favourite ever since the infamous <a href="http://www.mrc-cbu.cam.ac.uk/people/dennis.norris/personal/cambridgeemail/">Cambridge email</a> meme. The strength of the effect appears to depend on many factors, including the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2662926/">relative position of the letter</a><span class="oa" title="Open
Access"></span> and on <nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6542500/">the reader’s age</a><span class="oa" title="Open
Access"></span></nobr> (curiously, it does not depend on whether the reader is a human <a href="https://journals.sagepub.com/doi/abs/10.1177/0956797612474322">or a baboon</a><sup><a href="https://www.researchgate.net/profile/Johannes_Ziegler3/publication/237147842_Transposed-Letter_Effects_Reveal_Orthographic_Processing_in_Baboons/links/00b7d51c965f40f647000000/Transposed-Letter-Effects-Reveal-Orthographic-Processing-in-Baboons.pdf?origin=publication_detail">PDF</a></sup>). English words are particularly forgiving to letter transpositions, while e.g. Semitic languages are much more sensitive to them, as <nobr><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3677812/">pointed out</a><span class="oa" title="Open
Access"></span></nobr> by Israeli researcher Ram Frost.</span> even though we have no trouble distinguishing the letter order between <em>cat</em> and <em>act</em>. Such feats would be impossible without the dynamic interactions with a population of word detectors which, in turn, are gated by our language-comprehension networks.</p>
<h3 id="diversity-of-brain-areas-involved-in-reading-acquisition">Diversity of brain areas involved in reading acquisition</h3>
<p>This neat hierarchy of bigram detectors—and more generally, <em>n</em>-gram detectors—takes lots of reading practice to develop, but it is becoming increasingly clear that this is only one of many steps in the long and awkward process of reading <nobr>acquisition.<input type="checkbox" id="sn-sidenote-71" class="margin-toggle"></nobr><label for="sn-sidenote-71" class="margin-toggle sidenote-number"></label><span class="sidenote">The summary given here is based primarily on a well-sourced review preprint <sup><a href="https://psyarxiv.com/g3n2m/download?format=pdf">PDF</a></sup> by Carol Whitney and colleagues. Sadly, this was Carol’s last paper; she died in late 2019.</span> It appears that children first learn to recognize letters as individual objects, just as they learn to recognize chairs, spoons, and fire trucks. In particular, children develop letter representations in a brain area otherwise associated with small, graspable objects such as hand tools. Then, children learn that these letters, just as other objects and tools, are associated with sounds. Initially, the saccade distance is a single letter. After a few years, the grapheme-phoneme associations are strong enough that five-letter saccades are sufficient; within these five letters, the child quickly and covertly shifts attention from letter to letter. Remarkably, this requires the developing <em>n</em>-gram detectors to recognize letters that activate not simultaneously but in sequence.</p>
<p>In experienced adult readers, the <em>n</em>-gram detectors appear to be directly connected to letter-shape detectors the visual cortex. The development of this shortcut is the final step of learning to read, and these letter-shape detectors are no longer associated with the conscious experience of e.g. handling a letter-shaped toy. We don’t lose those original letter-representing neurons—but we don’t make use of them when reading quickly.</p>
<h3 id="temporal-vs.-spatial-encoding-of-n-gram-sequences">Temporal vs. spatial encoding of <em>n</em>-gram sequences</h3>
<p>The <em>n</em>-gram detectors are used to letters arriving in temporal sequence, and experiments suggest that even the “fast” adult letter detectors still activate the <em>n</em>-gram detectors in series, perhaps via lateral and feedback inhibition coupled with imperceptibly fast (≈16ms) gamma <nobr>cycles.<input type="checkbox" id="sn-sidenote-72" class="margin-toggle"></nobr><label for="sn-sidenote-72" class="margin-toggle sidenote-number"></label><span class="sidenote">See e.g. SERIOL2<sup><a href="https://files.eric.ed.gov/fulltext/ED543279.pdf">PDF</a></sup> by Whitney and Marton, which cleverly tests this hypothesis on both left-to-right and right-to-left readers to confirm the model’s assumptions about the effect of the lateralization of our reading circuitry to the left hemisphere.</span> Such a time-based encoding would also eliminate the need for an enormous number of retinotopic <em>n</em>-gram detectors.</p>
<p>But whether or not the distance between letters is encoded temporally or spatially (i.e. via convolutional filtering), it seems clear that the activation of <em>n</em>-gram detectors depends directly on the physical distance between printed letters. In other words: we read best what we are used to; legibility is a question of conditioning.</p>
<p>Of course, this seems disappointing. If conditioning is all that matters, why not simply copy the metrics from other fonts? How can we justify our tedious efforts to model neural Gestalt dynamics? The answer, of course, is that the tuning of <em>n</em>-gram detectors is only one of several factors in legibility, and by far the most forgiving one. Much more important are letter classification and word segmentation, both of which are questions of Gestalt.</p>
<h3 id="letter-classification">Letter classification</h3>
<p><nobr><input type="checkbox" id="mn-marginnote-73" class="margin-toggle"></nobr><label for="mn-marginnote-73" class="margin-toggle"></label><span class="marginnote"><img src="img/letter_features_bubbles.png" alt="Most salient letter features, as
identified by Fiset et al."></span> When letters are too tightly clustered, perhaps even overlapping, the performance of letter detectors will drop. This is not suprising; classic examples are <em>rn</em> or <em>nn</em> being misread as <em>m</em>. Recall that letter detectors detect correlations of V4 contour features, and each letter detector is particularly tuned to features that most reliably distinguish its target from other <nobr>candidates.<input type="checkbox" id="sn-sidenote-74" class="margin-toggle"></nobr><label for="sn-sidenote-74" class="margin-toggle sidenote-number"></label><span class="sidenote">We can visualize the results of experimental studies like <a href="https://doi.org/10.1111%2Fj.1467-9280.2008.02218.x">Fiset et al. (2008)</a><sup><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.4652&amp;rep=rep1&amp;type=pdf">PDF</a></sup> (the source of the image above), <a href="https://doi.org/10.1080/02643290802421160">Fiset et al. (2009)</a><sup><a href="http://lpvs-uqo.ca/wp-content/uploads/2016/04/spatio_temporal_dynamics.pdf">PDF</a></sup> and <a href="https://doi.org/10.3758/PBR.16.1.67">Lanthier et al.</a><sup><a href="https://link.springer.com/content/pdf/10.3758/PBR.16.1.67.pdf">PDF</a></sup> to gain an intuition for the features individual letter detectors are most tuned to. </span> From a gestalt-optimization perspective, it is the objective of the letterfitting designer to maintain sufficient distance between letters such that their medial-axis skeletons do not interfere with each other, <em>particularly</em> the parts of the skeleton most relevant to classification. As we will discuss later, such interference takes place in spatial frequency channels in V1 even when the letters do not overlap.</p>
<p>Ironically, interference between letters is actually rather negligible in grid-based <nobr>approaches<input type="checkbox" id="sn-sidenote-75" class="margin-toggle"></nobr><label for="sn-sidenote-75" class="margin-toggle sidenote-number"></label><span class="sidenote">Such as LetterModel, kernagic, the Hz-Program, etc.</span> because their pre-tabulated pair distances are applied between outside extrema. Meanwhile, the reading-conditioned <em>n</em>-gram detectors that would give biological plausibility to these methods are convolutional in nature, anchoring their reference frames on the letter centroids instead.</p>
<h3 id="word-segmentation">Word segmentation</h3>
<p>Besides letter classification, legibility depends on successful word <nobr>segmentation.<input type="checkbox" id="sn-sidenote-76" class="margin-toggle"></nobr><label for="sn-sidenote-76" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course, some scripts don’t use <a href="https://en.wikipedia.org/wiki/Word_divider">word dividers</a><sup>W</sup> at all. <a href="https://en.wikipedia.org/wiki/Thai_script">Thai</a><sup>W</sup> and <a href="https://en.wikipedia.org/wiki/Burmese_alphabet">Burmese</a><sup>W</sup> are in this category, and it is probably true more generally for <a href="https://en.wikipedia.org/wiki/Isolating_language">isolating languages</a><sup>W</sup>, i.e. those in which virtually every syllable maps directly onto a <a href="https://en.wikipedia.org/wiki/Bound_and_free_morphemes">free morpheme</a><sup>W</sup>. After all, in such grammars, word spaces aren’t of much use anyway. Koreans sometimes omit word spaces in <a href="https://en.wikipedia.org/wiki/Hangul">Hangul</a><sup>W</sup> in informal writing, as well. In <a href="https://en.wikipedia.org/wiki/Fusional_language">fusional languages</a><sup>W</sup> like English, however, word segmentation is crucial.</span> Word segmentation, of course, is all about perceptual grouping.</p>
<p>Our previous discussions might suggest that during reading, word segmentation happens as a result of attention spreading outwards to the word boundaries, thereby allowing us to select a single word at a time. However, experiments suggest that reality is not that simple. It appears that during fast reading, multiple words are perceived and processed at <nobr>once.<input type="checkbox" id="sn-sidenote-77" class="margin-toggle"></nobr><label for="sn-sidenote-77" class="margin-toggle sidenote-number"></label><span class="sidenote">Credit for championing this idea goes mainly to Joshua Snell and his collaborators in Jonathan Grainger’s research group. A key argument is the word transposition effect, in which word detectors are activated (nearly) in parallel, and our language comprehension networks pick out words in grammatical sequence: <nobr><em>you that read wrong; you that read wrong, too.</em></nobr> See <nobr><a href="http://dx.doi.org/10.1037/rev0000119">here</a><span class="oa" title="Open
Access"></span></nobr> for their model, and <nobr><a href="https://doi.org/10.1016/j.tics.2019.04.006">here</a><span class="oa" title="Open Access"></span></nobr> for a recent review of experimental evidence. MRI studies by <a href="https://doi.org/10.1016/j.tics.2019.07.001">Alex White et al. (2019)</a> appear to support this view.</span> This means that parallel <em>n</em>-gram detectors and multiple word candidates are activated in parallel, and could influence one another in the process. How, then, does the brain keep different words apart at all? For instance, what keeps us from reading <em>hello live</em> as <em>hell olive</em>?</p>
<p>One plausible explanation is that the activation of the <em>ol</em> bigram detector is a bit weaker in the first pair, whereas the <em>lo</em> bigram detector is weaker in the second. Given our ability to read jumbled letters, this may not seem like a reliable mechanism. But such ambiguous word pairings are extremely rare, and when they do occur, our grammar-based language circuitry would quickly resolve any ambiguity. If this is correct, then word segmentation is purely a result of neighbouring words not being able to co-activate <em>n</em>-gram detectors sufficiently to cause confusion. To prevent accidental word segmentation, a letterfitting model would need to contain the entire response curves of bigram detectors.</p>
<p>Another possible explanation may be that word-initial and word-final letters may be perceived as distinct from word-central letters. If that were true, transposing letters such that initial or final letters are jumbled out of place would effectively amount to letter substitution, rather than mere transposition. And indeed, <em>ujmlbde etxt</em> is much more difficult to decipher than <em>jmulebd txet</em>, although the number of transpositions is equal. If this theory is <nobr>correct,<input type="checkbox" id="sn-sidenote-78" class="margin-toggle"></nobr><label for="sn-sidenote-78" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course, the two explanations are not mutually exclusive.</span> then we must model how an initial- or final-letter detector recognizes that the letter in question coincides with the beginning or end of a word. Because these letter detectors are fed directly by the skeletons derived from V4 contour fragments, we may assume that word endings are detected as V4 contour fragments as well, which once again brings us back to our Gestalt-analysis approach.</p>
<p class="missing">
Still needs clearer explanation. How to justify we can deal with evenly tracked-out text, where spaces are a relative phenomenon?
</p>
<h3 id="human-designers-fit-letters-based-on-gestalt-grouping">Human designers fit letters based on gestalt grouping</h3>
<p>At this point, it is worth noting that type designers try hard <em>not</em> to engage their reading circuitry when fitting letters. Instead, they adjust letter pairs by staring straight at them, sometimes flipping them upside down to really see them “as they are”.</p>
<p>That human designers are so successful with such a purely gestalt-based approach is encouraging: it suggests that gestalt-based algorithms can be used widely, leading primarily to a perception of visual beauty (or perhaps the absence of visual distractions), and indirectly to good <nobr>legibility.<input type="checkbox" id="sn-sidenote-79" class="margin-toggle"></nobr><label for="sn-sidenote-79" class="margin-toggle sidenote-number"></label><span class="sidenote">To achieve <em>optimal</em> legibility, designers would need to abandon their current approach and pursue legibility directly. Perhaps someday we’ll see letterfitting based on double-blinded, randomly-controlled crossover trials of reading speed and comprehension.</span> It may well be that the approach works not only on alphabetic scripts, but also on the relative placement of strokes and/or radicals in Hangul and Hanzi.</p>
<h2 id="from-perceptual-grouping-to-letterfitting">From perceptual grouping to letterfitting</h2>
<p>Given a pair of letters, our objective is to minimize the risk that a word boundary is perceived between them (by fitting them tightly), while preserving the identity of both letters (by not fitting them <em>too</em> tightly).</p>
<p>[image]</p>
<p>With B-cells and G-cells, which correspond to configurations of V2 and V4 contour detectors, we now have the vocabulary to describe where and how this trade-off takes place as the two letters approach one another.</p>
<h3 id="losing-a-letters-skeleton">Losing a letter’s skeleton</h3>
<p>At the scale of the stem thickness, each letter activates a population of G-cells corresponding to its medial axis skeleton. Primarily, it is the letter’s ink that gets skeletonized; but in some situations, counter-space features might be recruited as well:</p>
<p><img src="img/skeleton_example.png" alt="Skeletons"></p>
<p>As noted, these skeletons stay relatively invariant across font styles, enabling letter-detecting neurons to function simply via spatial integration of particular skeleton <nobr>features.<input type="checkbox" id="sn-sidenote-80" class="margin-toggle"></nobr><label for="sn-sidenote-80" class="margin-toggle sidenote-number"></label><span class="sidenote">Even serifs are simply small extensions of the skeletal structure that occurs naturally at corners, even in sans-serif designs.</span> Consider now that the skeletonization depends on the activity of B-cells, but B-cells depend on the activity of V1 complex cells, and those in turn are affected by the presence of neighbouring letters.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-81" class="margin-toggle"></nobr><label for="mn-marginnote-81" class="margin-toggle"></label><span class="marginnote"><img src="img/v1_interference_example.png" alt="V1 interference
example"><br><em>Left:</em> A simple cell activates fully, thanks to the presence of the left letter’s right stem. <em>Right:</em> Tigthening the pair places the neighbouring letter into the cell’s receptive field, reducing its activation.</span> To illustrate this point, let’s consider a simple cell tuned to a light-dark-light pattern. The left letter of a pair is positioned such that its right-hand stem coincides with the “dark” region, activating the cell. We now move the right letter closer to the left. Eventually, its left stem will enter the cell’s receptive field in the “light” region. Even though the letters are still a considerable distance apart, this will reduce the cell’s activation. One way to think about this is that to our visual system, whether two letters are overlapping isn’t a binary question; it rather depends on the spatial frequency in question.</p>
<p>Therefore, in locations where two letters approach very closely, only the finest-scale complex cell activations will stay intact. This grossly reduces the activation of B-cells and, in turn, of the G-cells that constitute the skeleton from them:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-82" class="margin-toggle"></nobr><label for="mn-marginnote-82" class="margin-toggle"></label><span class="marginnote">Shown here is an extremely tightly fitted sans-serif, for effect. Serifs naturally enforce wider gaps.</span> <img src="img/skeleton_example_reduction.png" alt="skeletons"></p>
<p>On top of that, G-cells located in the gap now absorb some activity as well. This creates ambiguity about the polarity of border ownership in the gap, and the associated inhibition further dampens the G-cells that make up the stems’ skeletons.</p>
<p><img src="img/skeleton_example_reduction2.png" alt="skeletons"></p>
<p>Note that there is also a set of larger G-cells centered on the gap, encompassing both letters. This is a classic example of perceptual grouping: activity corresponding to the left letter’s outer edge will filter up to these larger G-cells, which will feed back to the right letter’s outer edge. Attention can be deployed at different scales, allowing us to shift the polarity of B-cells to focus on the gap, on either letter, or on the pair as a whole.</p>
<p>The complexity is quite impressive, and we have not yet taken into account amplifying effects from contour integration, both along the stems and across gaps, which in turn create illusory T-junctions, which lead to additional suppression, etc.</p>
<p>Generally, and attentional effects notwithstanding, we need to worry about any effects that draw neural activity away from the letters’ original skeletons. A computational model must therefore compare the skeletons of the standalone letters with the skeletons that remain once they are placed together. Different frequency scales should be weighted differently when the losses are tallied up, such that degraded stem skeletons are penalized more heavily than degraded serifs. In more advanced models, pre-trained letter classification networks could be used to determine the parts of the skeleton most relevant to distinguishing the letter in <nobr>question,<input type="checkbox" id="sn-sidenote-83" class="margin-toggle"></nobr><label for="sn-sidenote-83" class="margin-toggle sidenote-number"></label><span class="sidenote">This would probably rely on some salience-mapping technique like <a href="http://gradcam.cloudcv.org/">GradCAM</a>.</span> and penalize losses to these parts most heavily.</p>
<h3 id="losing-a-words-edge">Losing a word’s edge</h3>
<p>While placing letters too close together puts their skeletons at risk, placing them too far apart can compromise the integrity of the word as a perceptual group. This effect is more difficult to model, because letterfitting algorithms deal with pairs of letters at a time, while words can consist of many, many letters at once.</p>
<p>A wider-than-average gap in the middle of a word will stand out for multiple reasons. In long words, it will appear salient because it receives less surround suppression; it offers less opportunity for horizontal integration of letter contours along the baseline and x-height; and it evokes the activation of larger-scale V1 complex cells, translating to a stronger activation of (more, and larger-scale) B-cells. This, in turn, has a particularly noticeable effect on G-cells of x-height scale, or larger:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-84" class="margin-toggle"></nobr><label for="mn-marginnote-84" class="margin-toggle"></label><span class="marginnote">G-cells of all scales will participate in the activity associated with the boundary created by the larger gap. Small G-cells involving the inner edge of the gap will activate quite strongly; this draws neural activity towards the gap as soon as attention lands near it. Larger G-cells are less active, simply because they do not receive enough input.</span> <img src="img/wordboundary_gcells.png" alt="g cells at a word boundary"></p>
<p>In principle, this is a natural encoding of word boundaries: gaps that are wider than average will activate intra-word G-cells more strongly than smaller gaps. Attention-induced neural activity is therefore more likely to spread up to such gaps but no further.</p>
<p>The shape of a gap’s inner edge can contribute to this effect or weaken it. Round letters activate G-cells more strongly; <em>o</em>’s are so perfectly convex that even a small gap is sufficient for them to halt the spread of neural activity beyond it. In alignment with gestalt rules, an <em>o</em> is the perfect way to cap off a string of letters into a optimally rounded-off perceptual group. Type designers know that to prevent this from fragmenting a word, round letters need to be fitted more tightly than straight ones.</p>
<p>When two letters get closer to one another within a word, the risk of fragmentation at their gap drops thanks to the weakening of the V1 complex cells that indirectly enable the activation of large, fragmentation-inducing G-cells. In other words, the same mechanism is responsible for both the degradation of letter skeletons and for word <nobr>fragmentation.<input type="checkbox" id="sn-sidenote-85" class="margin-toggle"></nobr><label for="sn-sidenote-85" class="margin-toggle sidenote-number"></label><span class="sidenote">We could of course think of word fragmentation as a degradation of the word skeleton.</span></p>
<p>An effective letterfitting algorithm would quantify this drop in fragmentation risk, subtract it from the penalty associated with skeleton loss, and iteratively search for the pair distance that minimizes the resulting total penalty.</p>
<p>But letterfitting algorithms only see two letters at a time. This means that for some letters, the degree of inherent fragmentation risk appears much lower than it actually would be in the context of a word:</p>
<p><img src="img/wordboundary_letters.png" alt="effect of extra letters on g-cells
at word boundary"></p>
<p>Working with more than two letters doesn’t solve the problem either: in an exotic font, letters could be arbitrarily thin or wide. Of course, we <em>know</em> from experience that the width of a letter should have little to no influence on its tendency to fragment a word, and so it is in the context of other letters; and yet modelling the pair is difficult.</p>
<p><img src="img/benchmark_gaps.png" alt="Some benchmark letter pairs: nn, oo, nl,
and IUL"></p>
<p class="missing">
Explain the asymptotic length-invariance of parallel stems
</p>
<p class="missing">
Explain round-round interactions via weaker contour integration and less disruption of V4
</p>
<p class="missing">
Explain IUL via balance between weaker horizontal contour integration and stronger inhibition from false inter-stem medial axis (b/c smaller radius).
</p>
<p class="missing">
Illustrate effect of serifs, italics, x-height and weight
</p>
<h2 id="building-practical-letterfitting-algorithms">Building practical letterfitting algorithms</h2>
<p>Unfortunately, the dynamism of the scientific model(s) introduced thus far makes them unsuitable for use in practical letterfitting tools for type designers. Although it is relatively straightforward to set up systems of coupled differential equations representing individual neurons, integrating them at a sufficiently fine spatial resolution is immensely costly, and doing so over many iterations for each letter combination is outright infeasible, at least with consumer-grade hardware. We therefore need to consider potential <nobr>approximations.<input type="checkbox" id="sn-sidenote-86" class="margin-toggle"></nobr><label for="sn-sidenote-86" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course, existing approaches <em>are</em> approximations (see the <a href="#existing_tools">appendix</a> for an incomplete list).</span></p>
<h3 id="modelling-activations-of-v1-cells">Modelling activations of V1 cells</h3>
<p>As explained above, V1 simple cells are typically modelled as responding linearly via a simple Fourier-domain multiplication with a bank of bandpass filters <span class="math inline">G(s, o)</span>, where <span class="math inline">s</span> is the frequency scale and <span class="math inline">o</span> the <nobr>orientation.<input type="checkbox" id="sn-sidenote-87" class="margin-toggle"></nobr><label for="sn-sidenote-87" class="margin-toggle sidenote-number"></label><span class="sidenote"><a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor patches</a><sup>W</sup> are most commonly used, but many alternative models with better mathematical properties are available.</span> This set of convolutions turns the two-dimensional input image (width × height) into a four-dimensional tensor of complex numbers (width × height × spatial frequency scales × orientations), the magnitude and phase angle of which capture the activation of simple cells <span class="math inline">S_\mathrm{V1}</span> at every location:</p>
<p><span class="math display">
S_\mathrm{V1}(x, y, s, o) = \mathcal{F}^{-1}(\mathcal{F}(I(x, y)) \mathcal{F}(G(s, o))),
</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-88" class="margin-toggle"></nobr><label for="mn-marginnote-88" class="margin-toggle"></label><span class="marginnote"><img src="img/complex_value.png"></span></p>
<p>where <span class="math inline">\mathcal{F}</span> is the Fourier transform. For instance, to retrieve wthe activation of representative simple cells at phases 0°, 90°, w180° and 270°, one ould half-wave-rectify as follows:</p>
<p><span class="math display">
\begin{aligned}
S_{\mathrm{V1, 0\degree}}(x, y, s, o) &amp;= |\mathrm{Re}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 90\degree}}(x, y, s, o) &amp;= |\mathrm{Im}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 180\degree}}(x, y, s, o) &amp;= |-\mathrm{Re}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 270\degree}}(x, y, s, o) &amp;= |-\mathrm{Im}(S_1(x, y, s, o)| \\
\end{aligned}
</span></p>
<p>Traditionally, complex cells were thought to sum the outputs of nearby simple cells of equal scale and orientation. This is now known to be a gross oversimplification. In software, a summation-like approach is nevertheless taken to approximate the output of complex cells <span class="math inline">C_{\mathrm{V1}}</span>, namely a simple computation of the absolute magnitude of the complex tensor:</p>
<p><span class="math display">
C_\mathrm{V1}(x, y, s, o) = |S_\mathrm{V1}(x, y, s, o)|^2
</span></p>
<p>This is often called the <em>local energy</em>. The squaring operation shown here is often used in the literature to approximate the nonlinear behaviour of complex cells in particular.</p>
<h3 id="a-simple-pair-differential-model-of-interference-in-v1">A simple pair-differential model of interference in V1</h3>
<p>Most existing letterfitting algorithms rely purely on geometric features in the spatial domain, ignoring interference effects in the frequency-domain that result from the band-pass filtering in V1.</p>
<p>When two letters are far apart, i.e. farther apart than the diameter of the largest relevant V1 receptive fields, they do not interfere in <nobr>V1.<input type="checkbox" id="sn-sidenote-89" class="margin-toggle"></nobr><label for="sn-sidenote-89" class="margin-toggle sidenote-number"></label><span class="sidenote">This ignores feedback effects from higher-level areas with larger effective receptive fields.</span> In other words: let the 4D tensor of complex-valued simple-cell excitations <span class="math inline">S_\mathrm{V1}</span> due to the left letter be called <span class="math inline">S_i</span>, and the one due to the right letter be <span class="math inline">S_j</span>. When the letters are sufficiently separated, <span class="math inline">S_i</span> is not affected by the presence of the right letter, and vice versa.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-90" class="margin-toggle"></nobr><label for="mn-marginnote-90" class="margin-toggle"></label><span class="marginnote"><img src="img/v1_interference_example.png" alt="V1 interference
example"></span> Consider now a particular simple cell tuned to a light-dark-light pattern. The left letter is positioned such that its right-hand stem coincides with the “dark” region, activating the cell to some level <span class="math inline">s_i</span>. We now move the right letter closer to the left. Eventually, its left stem will enter the cell’s receptive field in the “light” region. Even though the letters are still a considerable distance apart, this will reduce the cell’s activation to a level lower than <span class="math inline">s_{ij}</span>. One way to think about this is that to our visual system, whether two letters are overlapping isn’t a binary question; it rather depends on the spatial frequency in question.</p>
<p>Conveniently, we get the magnitude of this reduction for free: because simple cells are assumed to be just as linear as the Fourier transform, <span class="math inline">S_{ij} = S_i + S_j</span>, and any reductions correspond to phase cancellations between the complex-valued <span class="math inline">S_i</span> and <span class="math inline">S_j</span>.</p>
<p>This offers us a straightforward way to visualize how approaching letter shapes affect one another’s perception in the visual cortex: we simply look at the changes to the local energy as a result of placing the two letters next to one another:</p>
<p><img src="img/simple_energy_model.png" alt="Simple energy model"></p>
<p>Note that while the interference <span class="math inline">|S_{ij}| - |S_i| - |S_j|</span> is always destructive for simple <nobr>cells,<input type="checkbox" id="sn-sidenote-91" class="margin-toggle"></nobr><label for="sn-sidenote-91" class="margin-toggle sidenote-number"></label><span class="sidenote">By a trivial triangle inequality.</span> squaring the magnitudes allows us to visualize both constructive and destructive interference at the level of complex cells. Generally, this results in stronger activations in the gap between letters and a weakening of the letter’s edges:</p>
<p><img src="img/abstract.png"></p>
<p>Granted, this alone does not a letterfitting algorithm make. But this very simple frequency-based representation already captures many of the geometric relationships that most existing letterfitting algorithms need to approximate via heuristics and epicycles.</p>
<p class="missing">
Quickly compare to existing gap quadrature models.
</p>
<p class="missing">
Comment on tuning parameters; pair gains approximate word-scale grouping strength, pair losses approximate stem-scale losses. Does not consider contour pop-out or actual grouping dynamics; does quite poorly on uppercase letters.
</p>
<p class="missing">
Explain training this + simple spline or neural net on existing fonts via backprop for a first approximation. Show results.
</p>
<h3 id="modelling-lateral-inhibition-via-divisive-normalization">Modelling lateral inhibition via divisive normalization</h3>
<p><nobr><input type="checkbox" id="mn-marginnote-92" class="margin-toggle"></nobr><label for="mn-marginnote-92" class="margin-toggle"></label><span class="marginnote"><img src="img/hra.png" alt="HRA"> Solid line: hyperbolic ratio curve, a.k.a. <a href="https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)%3Csup%3EW%3C/sup%3E">Hill function</a> or Naka-Rushton function. Dotted line: monotonic polynomial (e.g. <span class="math inline">x^2</span>).</span> Of course, the squaring operation in the expression for complex cell activations is a rather unrealistic (if practical) choice. In a real cells, the firing rate will level off after the input has been increased beyond some limit. A popular model for this is the hyperbolic ratio sigmoid</p>
<p><span class="math display">y = \frac{fx^k}{\beta^k + x^k}</span></p>
<p>The <span class="math inline">f</span> scales the curve vertically, <span class="math inline">k</span> makes the kink steeper, and <span class="math inline">\beta</span> shifts the threshold to the right. Consider how the numerator increases the firing rate, and the denominator decreases it. For relatively small values of <span class="math inline">x</span>, <span class="math inline">\beta^k</span> dominates the denominator, yielding a scaled-down version of <span class="math inline">fx^k</span> (values of about 2 or 3 are common for <span class="math inline">k</span>, in agreement with the square often used). But once <span class="math inline">x^k</span> gets large enough, <span class="math inline">\beta^k</span> pales in comparison, and we are left approaching <span class="math inline">f</span><nobr>.<input type="checkbox" id="sn-sidenote-93" class="margin-toggle"></nobr><label for="sn-sidenote-93" class="margin-toggle sidenote-number"></label><span class="sidenote">This specific activation function is effectively never used in deep learning, both for historical reasons and because <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">its asymptotic behaviour would slow down training</a><sup>W</sup>.</span></p>
<p>This formula is particularly relevant thanks to <em>lateral inhibition</em>, a common architectural pattern in the brain in which neurons within a cortical area suppress their neighbours in proportion to their own firing rate. Locally, this allows the most active neuron to suppress its neighbours more than those neighbours are able to suppress it in return. Lateral inhibition thus sharpens peaks and flattens valleys in the activity landscape; it is a simple and effective way to boost salient signals relative to the weaker ones that inevitably arise from the correlations between similarly tuned convolution filters. In V1, lateral inhibition thus sharpens the orientation and frequency-scale signals, while also normalizing local contrast.</p>
<p>Because lateral inhibition is a recurrent process that takes time to reach a steady state, it is most accurately modelled using a system of coupled differential equations which describe the time dependence of each neuron’s firing rate on its neighbours. Conveniently, however, the steady-state activations can also be approximated directly using our hyperbolic ratio model, by simply sneaking the neighbouring neurons’ activities into the <nobr>denominator:<input type="checkbox" id="sn-sidenote-94" class="margin-toggle"></nobr><label for="sn-sidenote-94" class="margin-toggle sidenote-number"></label><span class="sidenote">See <nobr><a href="https://arxiv.org/pdf/1906.08246.pdf">this analysis</a><span class="oa" title="Open
Access"></span></nobr> by Jesús Malo et al. to understand how this approximation works.</span></p>
<p><span class="math display">y_i = \frac{fx_i^k}{\beta^k + \sum_j w_j x_j^k}</span></p>
<p>This approximation is called <em>divisive normalization</em>. One can find many variations on the above formula in the literature: extra constants, extra square roots in the denominator, extra rectifiers, etc.; but the core idea is always the same.</p>
<p>This raises the challenge of determining the right values for <span class="math inline">w_j</span>, i.e. modelling the inhibitive strengths of neighbourly connections. Researchers have collected <nobr>formulas,<input type="checkbox" id="sn-sidenote-95" class="margin-toggle"></nobr><label for="sn-sidenote-95" class="margin-toggle sidenote-number"></label><span class="sidenote">In 2011, Tadamasa Sawada and Alexander Petrov compiled a <nobr><a href="https://doi.org/10.1152/jn.00821.2016">very long review</a><span class="oa" title="Open
Access"></span></nobr> of divisive normalization models of V1. To my knowledge, it is still the most comprehensive reference today.</span> but it is not clear that they capture all of the interactions there are. What’s more, the last decade of research has revealed that some measured behaviours previously ascribed to lateral inhibition may instead be the result of feedback from higher-level areas. If nothing else, <span class="math inline">w_j</span> is probably a convenient place for modellers to incorporate the effects of spatial frequency dependency (i.e. contrast sensitivity curves).</p>
<h3 id="extending-our-model-to-incorporate-more-dynamics">Extending our model to incorporate more dynamics</h3>
<p class="missing">
Lots of work missing here …
</p>
<p class="missing">
Brief nod to residual nets, which effectively unroll the dynamics over a few fixed time steps. Also mention Ricky Chen’s Neural ODE option.
</p>
<p class="missing">
Potentially feasible: one forward sweep; V1 → V1 DivN → V2 → contour integration DivN → grouping via fuzzy circular G cells → feedback to V2 B → update G cells. Take difference between pair and letters; weight and integrate; backprop-fit against existing fonts. Show some results.
</p>
<a name="existing_tools"></a>
<h2 class="appendix">
Appendix: Existing letterfitting tools
</h2>
<p>Most existing approaches operate either on the distance between stems, or on the area of the gap between them. Some are hybrids, more complex, or unpublished; finally, there has been some experimental work using neural nets:</p>
<p><img src="img/heuristics_classification.png"
alt="Heuristics" /></p>
<p><strong>Fixed-distance methods:</strong> A family of approaches that insert pre-defined distances between letter pairs. In their simplest incarnation, these heuristics are equivalent to simply adding sidebearings to every letter, without any kerns. <a href="https://github.com/hodefoting/kernagic">Kernagic</a>, inspired by <a href="https://www.lettermodel.org/">Frank Blokland’s research</a>, uses heuristics to identify stems or stem-equivalents (such as the round sides of an o) in every letter shape, and then aligns them. This works reasonably well with very regular type (think blackletter), but manual adjustments are usually required. Less well known is Barry Schwartz’ <a href="https://github.com/chemoelectric/sortsmill/blob/master/tools/spacing_by_anchors.py">anchor point implementation</a> of what amounts to basically the same idea. Adrian Frutiger, Walter Tracy and Miguel Sousa have devised similar systems, described in Fernando Mello’s <a href="http://www.fermello.org/FernandoMello_essay.pdf">MATD thesis</a>. The legendary <a href="https://en.wikipedia.org/wiki/Hz-program">Hz-Program</a><sup>W</sup> is also included in this category, as its <a href="https://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&amp;date=19941019&amp;DB=&amp;locale=en_EP&amp;CC=EP&amp;NR=0466953B1&amp;KC=B1&amp;ND=1#">patent application</a> reveals that letter pair distances were simply stored in a hardcoded table.</p>
<p><strong>Gap area quadrature:</strong> A family of algorithms that attempt to quantify and equalize the perceived area of the inter-letter gap. The crux, of course, lies in deciding where the gap ends. <a href="https://huertatipografica.github.io/HTLetterspacer/">HT Letterspacer</a>, the crudest one of these tools, considers everything between baseline and x-height (modulo some minor refinements). Simon Cozens’ <a href="https://github.com/simoncozens/CounterSpace">CounterSpace</a> uses blurs and convex hulls to more effectively exclude regions that arguably don’t belong to the gap (such as the counter of c). My own <a href="https://www.aldusleaf.org/2019-03-17-letterfitting-attention-model.html">Electric Bubble</a> model measures Euclidean instead of horizontal distances, but imposes geometric constraints that produce similar results to CounterSpace. CounterSpace currently wins in terms of performance-complexity ratio but it, too, struggles to fit certain letter pairs.</p>
<p><strong>Other shape-based methods:</strong> These include more exotic approaches, such as stonecarver <a href="https://en.wikipedia.org/wiki/David_Kindersley">David Kindersley</a>’s<sup>W</sup> “wedge method” from the 1960s, which operated on letter area moments of inertia (and didn’t really work), and <a href="https://ikern.com/k1/">iKern</a>, which produces great results but, just like Adobe’s <a href="https://typedrawers.com/discussion/3006/how-does-adobes-automatic-optical-kerning-work">Optical Kerning</a> feature, remains unpublished. Last but not least, the <a href="http://charlesmchen.github.io/typefacet/topics/autokern/typefacet-autokern-manual.html">TypeFacet Autokern</a> tool identifies parts of letter outlines that jut out horizontally, and adds kerning to compensate, based on a few parameters.</p>
<p><strong>Neural nets:</strong> Yes, we can train convolutional nets to recognize images of well-fitted and poorly-fitted type. Simon Cozens has built several versions of his <a href="https://github.com/simoncozens/atokern">kerncritic</a> model (formerly AtoKern), and the recent ones perform surprisingly well on many (if not all) pairs. While neural nets are fascinating, they tend to be black boxes: we can only make guesses at how they work, and we cannot tune their behaviour to suit our taste. This problem holds not just for convolutional nets, but for statistical function approximators in general; I do not discuss them further here.</p>
<p><strong>Honorable mention:</strong> <a href="https://groups.google.com/forum/#!searchin/comp.fonts/laurence$20penney$20kern/comp.fonts/GEjTE9_H52M/BSLdSE2lgmsJ">Bubble Kerning</a> is a proposal that type designers draw a bubble around every letter, such that software can automatically find pair distances by simply abutting the bubbles. While this isn’t technically a letterfitting heuristic at all, it’s still worth mentioning as a neat idea that could perhaps save designers some time. Toshi Omagari has built a <a href="https://github.com/Tosche/BubbleKern">Glyphs plugin</a>.</p>
<a name="space_kern_lp"></a>
<h2 class="appendix">
Appendix: Finding side bearings and kerns from pair distances
</h2>
<p>Given a set of pair distances (measured as the horizontal component of the line between outline extrema) <span class="math inline">d_{ij}</span>, where <span class="math inline">i</span> and <span class="math inline">j</span> index the left and right glyph from a set of glyphs <span class="math inline">G</span>, we can find the optimal assignment of side bearings and kerning values via a simple linear programming model <span class="math display">
\begin{aligned}
\mathrm{Min.}&amp;\sum_{i\in G,j\in G} |k_{ij}|\\
\mathrm{such\;that\;}&amp;r_i + k_{ij} + l_j = d_{ij}\quad\forall i \in G, j \in G,\\
\end{aligned}
</span> where <span class="math inline">l_i</span> and <span class="math inline">r_i</span> are the left and right side bearings of glyph <span class="math inline">i</span>, and <span class="math inline">k_{ij}</span> the kerning value required.</p>
<p>Alternatively, a quadratic objective can be used instead of the sum of magnitudes,</p>
<p><span class="math display">
\begin{aligned}
\mathrm{Min.}&amp;\sum_{i\in G,j\in G} k_{ij}^2\\
\mathrm{such\;that\;}&amp;r_i + k_{ij} + l_j = d_{ij}\quad\forall i \in G, j \in G;\\
\end{aligned}
</span></p>
<p>different formulations will either minimize overall kerning (for best appearance with applications that do not support kerning, e.g. sprite-based rendering) or maximize the number of near-zero kerns that can then be eliminated entirely (minimizing file size).</p>
<p class="missing">
More complex optimization models to implement efficient class kerning, likely via constraint programming.
</p>
</body>
</html>
