<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>YinYangFit</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="source/style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<h1 id="yinyangfit">YinYangFit ☯</h1>
<p><em>A letterfitting model inspired by biology</em></p>
<h2 id="overview">Overview</h2>
<p>There are three parts to this project:</p>
<ol type="1">
<li><strong>An explanation of the model,</strong> updated regularly as I refine it. (You’re reading this at the moment.)</li>
<li><strong>A Jupyter notebook</strong> hosted on Google Colab, which I use to experiment with different variations on the model. I will make the notebook public soon. Note that I’m using it as a live-coding environment, so don’t expect clean code.</li>
<li><strong>A letterfitting tool</strong> designed for end users. This will probably take the form of a Python script, perhaps with a simple browser interface, which takes in any font file and adds spacing and kerning values according to the model. This doesn’t exist yet.</li>
</ol>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>A big thank-you to Google for their generous funding of this research project.</p>
<h2 id="project-plan">Project plan</h2>
<ul>
<li>Early October 2019: publish basic concept</li>
<li>October-November 2019: publish Colab notebook</li>
<li>Sometime, hopefully in 2019: converge on a satisfactory model</li>
<li>Late 2019 / early 2020: build and publish letterfitting tool</li>
</ul>
<h2 id="contents">Contents</h2>
<ol type="1">
<li><a href="#intro">A good fit: what does that mean?</a></li>
<li><a href="#existing_tools">Exisiting letterfitting tools</a></li>
<li><a href="#vision_research_letterfitting">What can vision research teach us about letterfitting?</a></li>
<li><a href="#modelling_visual_cortex">Models of the visual cortex</a></li>
<li><a href="#multiscale">Building a multi-scale letter pair analyzer</a> (in progress)</li>
<li>Results (check back soon!)</li>
<li>Parameter tuning (check back soon!)</li>
<li>YinYangFit, the tool (check back soon!)</li>
</ol>
<p><a name="intro"></a></p>
<h2 id="a-good-fit-what-does-that-mean">A good fit: what does that mean?</h2>
<p>Spacing and kerning type (hence: “letterfitting”) is painstaking work. Heuristics exist, and some of them have been implemented in software, but letter shapes are so varied across typefaces that extensive manual adjustments are often still required. I provide a review of some of the current heuristics below.</p>
<p>Looking for new approaches can feel like groping in the dark. Often, heuristics designed to accommodate one font style perform poorly with other styles.</p>
<p>Let’s take a step back and ask: what does it <em>mean</em> for letters to be well-fitted? And while we’re at it: what does it <em>mean</em> for objects to be well-<a href="https://medium.com/@Lyst/your-favorite-insta-aesthetic-has-an-unexpected-history-a929e078d4ea">knolled</a>? And what does it <em>mean</em> for a kanji to be <a href="https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/">well-balanced</a>?</p>
<p><img style="max-width: 35rem;" src="img/overview.png" alt="Overview"/></p>
<p>Cognitive scientists have built up a large repertoire of ideas about how vision works in the brain, and I believe that those ideas can guide us towards a clearer understanding of what it means for shapes to be “too close” or “too far” – and towards better letterfitting tools, of course.</p>
<p><a name="existing_tools"></a></p>
<h2 id="existing-letterfitting-tools">Existing letterfitting tools</h2>
<p>Most existing approaches operate either on the distance between stems, or on the area of the gap between them. Some are hybrids, more complex, or unpublished; finally, there has been some experimental work using neural nets:</p>
<p><img src="img/heuristics_classification.png"
alt="Heuristics" /></p>
<p><strong>Fixed-distance methods:</strong> A family of approaches that insert pre-defined distances between letter pairs. In their simplest incarnation, these heuristics are equivalent to simply adding sidebearings to every letter, without any kerns. <a href="https://github.com/hodefoting/kernagic">Kernagic</a>, inspired by <a href="https://www.lettermodel.org/">Frank Blokland’s research</a>, uses heuristics to identify stems or stem-equivalents (such as the round sides of an o) in every letter shape, and then aligns them. This works reasonably well with very regular type (think blackletter), but manual adjustments are usually required. Less well known is Barry Schwartz’ <a href="https://github.com/chemoelectric/sortsmill/blob/master/tools/spacing_by_anchors.py">anchor point implementation</a> of what amounts to basically the same idea. Adrian Frutiger, Walter Tracy and Miguel Sousa have devised similar systems, described in Fernando Mello’s <a href="http://www.fermello.org/FernandoMello_essay.pdf">MATD thesis</a>. The legendary <a href="https://en.wikipedia.org/wiki/Hz-program">Hz-Program</a> is also included in this category, as its <a href="https://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&amp;date=19941019&amp;DB=&amp;locale=en_EP&amp;CC=EP&amp;NR=0466953B1&amp;KC=B1&amp;ND=1#">patent application</a> reveals that letter pair distances were simply stored in a hardcoded table.</p>
<p><strong>Gap area quadrature:</strong> A family of algorithms that attempt to quantify and equalize the perceived area of the inter-letter gap. The crux, of course, lies in deciding where the gap ends. <a href="https://huertatipografica.github.io/HTLetterspacer/">HT Letterspacer</a>, the crudest one of these tools, considers everything between baseline and x-height (modulo some minor refinements). Simon Cozens’ <a href="https://github.com/simoncozens/CounterSpace">CounterSpace</a> uses blurs and convex hulls to more effectively exclude regions that arguably don’t belong to the gap (such as the counter of c). My own <a href="https://www.aldusleaf.org/2019-03-17-letterfitting-attention-model.html">Electric Bubble</a> model measures Euclidean instead of horizontal distances, but imposes geometric constraints that produce similar results to CounterSpace. CounterSpace currently wins in terms of performance-complexity ratio but it, too, struggles to fit certain letter pairs.</p>
<p><strong>Other shape-based methods:</strong> These include more exotic approaches, such as stonecarver <a href="https://en.wikipedia.org/wiki/David_Kindersley">David Kindersley</a>’s “wedge method” from the 1960s, which operated on letter area moments of inertia (and didn’t really work), and <a href="https://ikern.com/k1/">iKern</a>, which produces great results but, just like Adobe’s <a href="https://typedrawers.com/discussion/3006/how-does-adobes-automatic-optical-kerning-work">Optical Kerning</a> feature, remains unpublished. Last but not least, the <a href="http://charlesmchen.github.io/typefacet/topics/autokern/typefacet-autokern-manual.html">TypeFacet Autokern</a> tool identifies parts of letter outlines that jut out horizontally, and adds kerning to compensate, based on a few parameters.</p>
<p><strong>Neural nets:</strong> Yes, we can train convolutional nets to recognize images of well-fitted and poorly-fitted type. Simon Cozens has built several incarnations of his <a href="https://github.com/simoncozens/atokern">kerncritic</a> model (formerly AtoKern), and the latest versions perform surprisingly well on many (if not all) pairs. While neural nets are fascinating, they tend to be black boxes: we can only make guesses at how they work, and we cannot tune their behaviour to suit our taste. The same is true not just for convolutional nets, but for statistical function approximators in general; I will not discuss them further in this post.</p>
<p><strong>Honorable mention:</strong> <a href="https://groups.google.com/forum/#!searchin/comp.fonts/laurence$20penney$20kern/comp.fonts/GEjTE9_H52M/BSLdSE2lgmsJ">Bubble Kerning</a> is a proposal that type designers draw a bubble around every letter, such that software can automatically find pair distances by simply abutting the bubbles. While this isn’t technically a letterfitting heuristic at all, it’s still worth mentioning as a neat idea that could perhaps save designers some time. Toshi Omagari has built a <a href="https://github.com/Tosche/BubbleKern">Glyphs plugin</a>.</p>
<p><a name="vision_research_letterfitting"></a></p>
<h2 id="what-can-vision-research-teach-us-about-letterfitting">What can vision research teach us about letterfitting?</h2>
<p>Flippantly speaking, much of vision research is about designing optical illusions and testing whether or not we fall for them. And whenever we do, we can propose neural architectures that might explain the effect (and then run expensive fMRI and in-vivo animal studies to discredit competing theories).</p>
<p>Decades of such experiments have produced a wealth of literature, which can be summarized as follows: in order to maximize object detection performance, our brain first enhances incoming imagery using what amounts to a set of Photoshop filters: contrast normalization, sharpening, edge detection, and so forth. These filters are implemented as amplifying and inhibiting connections between neurons (and particular artificial patterns can trick these networks into funny, oscillating behaviour – optical illusions!).</p>
<p>Crucially, the first set of neurons processes the image by detecting lines and edges at multiple scales: some filters pick out fine details, others larger structures. Any objects, including letters, are recognized based on the presence of their features across multiple scales.</p>
<p>Letters are very simple structures, so in order to explain letterfitting (or kanji design), the following theory may be enough to produce good results:</p>
<ul>
<li><p>Fitting letters too tightly attenuates some of their features at medium and larger scales (compared to standalone letters), slowing down letter recognition</p></li>
<li><p>Fitting letters too loosely creates saliency in the gap between the letters at larger scales, lowering the signal-to-noise ratio and slowing down word recognition</p></li>
</ul>
<p>To test this hypothesis, we need a reasonably accurate software model of the early stages of our brain’s visual processing pipeline.</p>
<p><a name="modelling_visual_cortex"></a></p>
<h2 id="models-of-the-visual-cortex">Models of the visual cortex</h2>
<p><label for="mn-simple-cell-sample-rfs"
class="margin-toggle">⊕</label> <input type="checkbox"
id="mn-simple-cell-sample-rfs" class="margin-toggle"> <span class="marginnote"> <img src="img/simple_cell_sample_rfs.png" alt="Some
receptive fields">Illustration of some receptive fields of these so-called “simple cells” in the primary visual cortex (V1).</span>As you are reading these words, image data from your retina flows up your optical nerve into your brain, where it gets routed to the visual cortex at the back of your head. Each neuron there combines the input from a few neighbouring photoreceptors in your retina. The specific combinations are chosen to allow each neuron to respond to a small, meaningful feature – for instance, a small dark-light edge angled at 45 degrees. Generally, neighbouring neurons correspond to neighbouring locations on the retina, but their receptive fields come in different sizes, orientations, and phases (dark-light sequences).</p>
<p>To model the output from simple cells sharing a particular size, orientation, and phase – let’s call that a <em>channel</em> – we can simply convolve the input image with a kernel corresponding to the receptive field and rectify the result.<label for="sn-gab"
class="margin-toggle sidenote-number"></label> <input type="checkbox"
id="sn-gab" class="margin-toggle"> <span class="sidenote"> Popular mathematical functions used to generate such simulated kernels include <a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor patches</a>, differences/derivatives/Laplacians of Gaussians, and others – they all look like the sample receptive fields above. </span></p>
<p>Cognitive scientists are confident that the above is largely correct. What happens next, however, is less certain. These simple cells don’t just pass their outputs up to higher-level brain areas; they also interact with one another, electrochemically stimulating and/or muting nearby cells in proportion to their own activity. What’s more, neurons from higher-level areas can also “reach down” and modulate perception as it is still taking shape.<label for="sn-tdm"
class="margin-toggle sidenote-number"></label> <input type="checkbox"
id="sn-tdm" class="margin-toggle"> <span class="sidenote">This really means that our learned expectations directly shape our experience, so it’s not unlikely that your familiarity with the Latin script and English orthography subtly affects your perception of a font’s metrics. Modelling <em>that</em> is hopelessly out of reach, of course.</span>The waves of electricity that flash through these inscrutably connected networks are, in aggregate, responsible for the experimental findings: local contrast normalization, contour pop-out, and so forth. These don’t necessarily happen in sequence, but rather in parallel, until the electrical oscillations settle into a stable pattern for some tens of milliseconds before new information comes in.</p>
<p>Of course, that biological complexity hasn’t kept generations of clever PhD students from inventing simple(r) formulas that can reproduce the experimental results <em>without</em> simulating the temporal dynamics of the whole network. Such a model is what we will use to predict how the distance between a pair of letters affects their appearance relative to how they appear individually.</p>
<p>Although I do not provide a literature review here (perhaps in a future version?), I strongly encourage interested readers to dig into the huge variety of models people have come up with. A great place to start is <a href="https://doi.org/10.1016/j.visres.2011.02.007">this 2011 review of the last 25 years of research</a> by Norma Graham, followed by Michael Morgan’s <a href="https://doi.org/10.1016/j.visres.2010.08.002">review of image features</a>.</p>
<p><a name="multiscale"></a></p>
<h2 id="building-a-multi-scale-letter-pair-analyzer">Building a multi-scale letter pair analyzer</h2>
<p>One of the models that has found some recent popularity is the <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003079">second-order contrast</a> (SOC) model by Kendrick Kay et al. It’s relatively simple and ticks many of the boxes that other filter-rectify-filter models don’t, and it even appears to predict fMRI responses quite well (bonus points!).</p>
<p>However, it was tested only at a single filter scale, whereas our hypothesis incorporates the agreement between feature detectors across multiple scales. We will therefore run the SOC model at multiple scales.</p>
<p>(To be continued …)</p>
<h2 id="results">Results</h2>
<p>(Come back soon!)</p>
<h2 id="parameter-tuning">Parameter tuning</h2>
<p>(Come back soon!)</p>
<h2 id="yinyangfit-the-tool">YinYangFit, the tool</h2>
<p>(Come back soon!)</p>
</body>
</html>
