<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>YinYangFit</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="source/style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<h1 id="yinyangfit">YinYangFit ☯</h1>
<p><em>Modelling for automatic letterfitting, inspired by neuroscience</em></p>
<p><img src="img/abstract.png" alt="Abstract"/></p>
<h2 class="nonumber">
Acknowledgements
</h2>
<p>This research would not have been possible without funding from Google, for which I have Dave Crossland to thank in particular. I am grateful also to Simon Cozens and others for many valuable discussions.</p>
<!--## Contents
1. [Abstract](#abstract)
2. [A good fit: what does that mean?](#intro)
4. [What can vision research teach us about letterfitting?](#vision_research_letterfitting)
5. [Models of the visual cortex](#modelling_visual_cortex)
6. [Building a multi-scale letter pair analyzer](#multiscale)
7. [Extending our model: lateral inhibition and divisive normalization](#extending)
8. Results (check back soon!)
9. Parameter tuning (check back soon!)
10. YinYangFit, the tool (check back soon!)
3. Appendix: [Exisiting letterfitting tools](#existing_tools) -->
<h2 class="nonumber">
Abstract
</h2>
<p>Adjusting letter distances to be visually pleasing is a challenging and time-consuming task. As existing tools are too primitive to reliably handle the infinite variety of typefaces, designers have to mostly rely on their intuitive judgment. I review how letterfitting fits into the current scientific understanding of how letters and words are perceived in the brain, and present approximate models that can be fitted to to existing, hand-fitted fonts using backpropagation.</p>
<h2 class="nonumber">
Target audience
</h2>
<p>Designers and developers with an interest in neuroaesthetics.</p>
<h2 class="nonumber">
Epistemic status: tentative
</h2>
<p>This article is based on a survey of hundreds of peer-reviewed articles, and in line with mainstream ideas in vision and neuroscience research. It is the product of nearly a year of work and countless revisions. That said, even the in-vivo evidence for the suggested models is often indirect or circumstantial. Nothing in this article should be construed as final. I welcome corrections!</p>
<h2 id="introduction-form-follows-function-beauty-follows-legibility">Introduction: form follows function, beauty follows legibility</h2>
<p>Letterfitting refers to the process of adjusting the distances between pairs of <nobr>letters<input type="checkbox" id="sn-sidenote-1" class="margin-toggle"></nobr><label for="sn-sidenote-1" class="margin-toggle sidenote-number"></label><span class="sidenote">I use the word “letter” very liberally; the more general term is <a href="https://en.wikipedia.org/wiki/Glyph">glyph</a>.</span> during typeface design. <nobr><input type="checkbox" id="mn-marginnote-2" class="margin-toggle"></nobr><label for="mn-marginnote-2" class="margin-toggle"></label><span class="marginnote"><img src="img/spacingkerning.png" alt="Spacing and
kerning"><br> Red vertical bars show side bearings, blue vertical bar shows a negative kern.</span> It’s often referred to as “spacing and kerning”, because pair distances are the sum of fixed amounts of space around every letter (so-called <em>side bearings</em>) and additional adjustment values for individual pairs (so-called <em>kerns</em>). Quality fonts often contain thousands of hand-kerned pairs that undergo weeks of testing and refinement.</p>
<p>Some would say that a good fit is simply the result of the designer’s personal intuition for <nobr>beauty.<input type="checkbox" id="sn-sidenote-3" class="margin-toggle"></nobr><label for="sn-sidenote-3" class="margin-toggle sidenote-number"></label><span class="sidenote">Type designers who adjust pair distances by hand often feel that there seems to be no right answer. In those moments, letterfitting can feel quite arbitrary. If you have not experienced this yourself, the venerable <a href="https://type.method.ac/">kern game</a> lets you try your hand on existing fonts.</span> Others have invoked the aesthetics of an “even colour”, i.e. a printed page with a uniform texture and no noticeable blobs of black or white. Meanwhile, Frank Blokland has <nobr>argued<input type="checkbox" id="sn-sidenote-4" class="margin-toggle"></nobr><label for="sn-sidenote-4" class="margin-toggle sidenote-number"></label><span class="sidenote">See his <a href="https://www.lettermodel.org/">PhD thesis</a>.</span> that the distances between letter stems are mainly a holdover from the early days of printing, when the measurements of cast type were the result of practical considerations.</p>
<p>These ideas aren’t wrong, but they’re underpowered. None have led to automated letterfitting <nobr>algorithms<input type="checkbox" id="sn-sidenote-5" class="margin-toggle"></nobr><label for="sn-sidenote-5" class="margin-toggle sidenote-number"></label><span class="sidenote">I’ve listed the most popular existing attempts in the <a href="#existing_tools">appendix</a>.</span> that reliably reproduce the hand-tweaked pair distances in existing fonts—from hairline slab serifs to broad-nib italics, from captions to headline sizes, and from Latin to Khmer.</p>
<p>So let’s start from square one: with a solid model of how reading works in the brain. By reframing letterfitting as the pursuit of <em>optimal legibility</em> (and not of beauty), we can climb on the giant shoulders of generations of vision researchers and begin to understand what vague ideas like “black-white balance” actually mean in our visual cortex. Our working hypothesis: well-fitted text is text that most effectively activates the neural circuitry that allows us to read letters and words, and therefore maximizes reading speed.</p>
<p>The connection between type design and legibility is self-evident and <nobr>well-studied.<input type="checkbox" id="sn-sidenote-6" class="margin-toggle"></nobr><label for="sn-sidenote-6" class="margin-toggle sidenote-number"></label><span class="sidenote">Type legend Charles Bigelow recently compiled a <a href="https://www.sciencedirect.com/science/article/pii/S004269891930%201087">comprehensive review of empirical legibility studies</a>, covering many important concepts including weight and optical sizing.</span> But even though psychologists have published empirical findings galore, we are only just beginning to understand the neural architectures which explain them, and which will one day power the type design tools of the future. For vision researchers looking for ways to test neural models of shape and Gestalt perception, the thousands of high-quality, hand-fitted fonts in existence today present a rich testing ground—and a completely underappreciated one. I hope that this article will inspire both typographers and vision researchers to more deeply explore the connection between letterfitting and perception.</p>
<h2 id="letterfitting-as-perceptual-grouping">Letterfitting as perceptual grouping</h2>
<p>Letterfitting means making a compromise between the legibility of letters and the integrity of words. Intuitively, moving letters further apart makes it easier to recognize them:</p>
<p><img src="img/distance_letter_identification.png" alt="Letter identifiability is low for extremely tight pairs and high for very loose pairs."></p>
<p>Letter identifiability is important. But to form a <em>word</em>, letters need to stay close enough to allow our visual system to perceive them as a group.</p>
<p>Perceptual grouping is a complicated process. The neurons that group visual signals into coherent objects operate at different scales, and they compete with one another. When one pair of letters is tighter than the next, perceptual grouping will bind the first pair tightly together, at the expense of the second. Consider the following example:</p>
<p><img src="img/grouping_relativity.png" alt="Illustration of the importance of consistency of fit vs absolute distances."></p>
<p>In the first column, the well-fitted word is perceptually grouped into a single object. In the middle column, the loosely fitted word is still perceived as an object, but it has to compete with the perception of its constituent letters. Finally, the poorly fitted word in the last column triggers the perception of <em>two</em> separate objects, namely the single letter <em>c</em> and a pair <em>at</em>.</p>
<p>The primary objective of a good fit is to avoid the latter situation. When some pairs group more strongly than others, words are fragmented into separate perceptual objects, which makes reading difficult. The secondary objective of a good fit is to make the fit as tight as possible without hampering the identifiability of each letter.</p>
<p>Perceptual grouping networks are a fundamental piece of our vision circuitry, and not exclusive to reading. Still, knowledge about letter shapes can affect fitting decisions. We will therefore review the latest scientific models of both vision <em>and</em> reading.</p>
<p>Our brain’s visual processing system is divided into multiple regions, each of which represents the incoming visual imagery at a different level of abstraction. Anything we see—landscapes, patterns, text—activates neurons in each one of these brain areas. While neurons in the lower-level areas respond to concrete details in the visual input, neurons in higher-level areas respond to the presence of particular configurations of such details. This allows us to simultaneously experience the raw visual qualia <em>and</em> comprehend what we see on a more abstract level.</p>
<p>Whether we are looking at an apple (and recognizing it as such), a tree (and recognizing it as such), or a word (and reading it), the same visual circuitry is at work—with the exception that the highest-level neurons responsible for recognizing apples and trees are located in a different brain area than those dedicated to recognizing letters and words.</p>
<p><img src="img/vision_model.png" alt="Vision model"></p>
<p>Many readers may have had some exposure, however superficial, to the concept of deep convolutional networks. It is dangerously tempting to conceptualize the architecture of the visual cortex as such a network: raw visual input enters at the bottom, undergoes processing through multiple layers, then comes out the top as a neat classification of a word. But perception, and perceptual grouping in particular, is a dynamic process. It is not a computation with input and output, but a dance of electrical activity that evolves through <nobr>time.<input type="checkbox" id="sn-sidenote-7" class="margin-toggle"></nobr><label for="sn-sidenote-7" class="margin-toggle sidenote-number"></label><span class="sidenote"><a href="http://nxxcxx.github.io/Neural-Network/">This interactive visualization</a> is far from realistic but a much more useful visual metaphor than feed-forward deep learning architectures.</span></p>
<p>With that in mind, let’s go on a brief tour through our visual system.</p>
<h2 id="from-the-retina-to-the-primary-visual-cortex">From the retina to the primary visual cortex</h2>
<p>Sensory input from the eye travels up the optic nerve, through the lateral geniculate nucleus (LGN) on the brain’s thalamus, to the visual cortex at the very back of the <nobr>head.<input type="checkbox" id="sn-sidenote-8" class="margin-toggle"></nobr><label for="sn-sidenote-8" class="margin-toggle sidenote-number"></label><span class="sidenote">For our computational purposes, we will ignore any image processing performed by the retina and thalamus, such as the luminance adaptation and pooling operations performed by <a href="https://en.wikipedia.org/wiki/Retinal_ganglion_cell">retinal ganglion cells</a>.</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-9" class="margin-toggle"></nobr><label for="mn-marginnote-9" class="margin-toggle"></label><span class="marginnote">Illustration adapted from Nicolas Henri Jacob (1781–1871), <em>Traité complet de l’anatomie de l’homme comprenant la medecine operatoire, par le docteur Bourgery</em>. Available in the <a href="https://anatomia.library.utoronto.ca/">Anatomia Collection</a> of the Thomas Fisher Rare Book Library, University of Toronto.</span> <img src="img/vc_anatomy.png" alt="Anatomy; location of the visual cortex"></p>
<p>The first phalanx of cells—the primary visual cortex, or V1—performs what amounts to a band-filtered wavelet decomposition. Each neuron here is <nobr>retinotopically<input type="checkbox" id="sn-sidenote-10" class="margin-toggle"></nobr><label for="sn-sidenote-10" class="margin-toggle sidenote-number"></label><span class="sidenote">That is, neurons are laid out to <a href="https://en.wikipedia.org/wiki/Retinotopy">roughly mirror the organization of the retina</a>, such that adjacent photoreceptors are connected to nearby neurons in the cortex.</span> connected directly to a small contiguous group of photoreceptors, its <em>receptive field</em> <nobr>(RF),<input type="checkbox" id="sn-sidenote-11" class="margin-toggle"></nobr><label for="sn-sidenote-11" class="margin-toggle sidenote-number"></label><span class="sidenote">To be clear, the majority of neurons physically located in V1 don’t actually receive direct input from the eye but rather just serve as local connections to facilitate basic image enhancement, such as contrast normalization, but we will skip here the organization of V1’s layers.</span> and activates whenever a particular subset of the receptors detects light but the others don’t. The on/off subsets are laid out such that each neuron effectively detects a small piece of a line or edge of a particular size and orientation somewhere in the field of vision. <nobr><input type="checkbox" id="sn-sidenote-12" class="margin-toggle"></nobr><label for="sn-sidenote-12" class="margin-toggle sidenote-number"></label><span class="sidenote">For those readers completely unfamiliar with these concepts, I recommend watching <a href="https://www.youtube.com/watch?v=NnVLXr0qFT8">this introductory animation</a>, followed by <a href="https://www.youtube.com/watch?v=mtPgW1ebxmE">this Allen Institute talk</a> about the visual system, followed by <a href="https://www.youtube.com/watch?v=T9HYPlE8xzc">this in-depth MIT lecture</a> on the anatomical details.</span></p>
<p><img src="img/edge_line_rfs.png" /></p>
<p>These neurons are called <em>simple cells</em>, and we can easily predict their response to a given input. For instance, when we see an single uppercase <em>I</em> on a page, some simple cells will respond strongly and others not at all, depending on the tuning and location of their receptive <nobr>fields.<input type="checkbox" id="sn-sidenote-13" class="margin-toggle"></nobr><label for="sn-sidenote-13" class="margin-toggle sidenote-number"></label><span class="sidenote">David Hubel and Torsten Wiesel first discovered this in the 1950s by showing patterns of light to a cat after sticking electrodes into its brain (Youtube has a <a href="https://www.youtube.com/watch?v=Yoo4GWiAx94">video of said cat</a>). The researchers went on to win a Nobel Prize for their experiments.</span></p>
<p>In software models, the filtering operation performed by simple cells is typically implemented as Fourier-domain multiplication with a bank of complex band-pass filters <span class="math inline">G(s, o)</span> (where <span class="math inline">s</span> is the frequency scale and <span class="math inline">o</span> the <nobr>orientation).<input type="checkbox" id="sn-sidenote-14" class="margin-toggle"></nobr><label for="sn-sidenote-14" class="margin-toggle sidenote-number"></label><span class="sidenote"><a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor patches</a> are most commonly used, but many alternative models with better mathematical properties are available.</span> This set of convolutions turns the two-dimensional input image into a four-dimensional tensor of complex numbers (widtht × height × spatial frequency scales × orientations), the magnitude and phase angle of which capture the activation of simple cells <span class="math inline">S_\mathrm{V1}</span> at every location.</p>
<p><span class="math display">
S_\mathrm{V1}(x, y, s, o) = \mathcal{F}^{-1}(\mathcal{F}(I(x, y)) \mathcal{F}(G(s, o)))
</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-15" class="margin-toggle"></nobr><label for="mn-marginnote-15" class="margin-toggle"></label><span class="marginnote"><img src="img/complex_value.png"></span></p>
<p>For instance, to retrieve the activation of representative simple cells at phases 0°, 90°, 180° and 270°, we could half-wave-rectify as follows:</p>
<p><span class="math display">
\begin{aligned}
S_{\mathrm{V1, 0\degree}}(x, y, s, o) &amp;= |\mathrm{Re}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 90\degree}}(x, y, s, o) &amp;= |\mathrm{Im}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 180\degree}}(x, y, s, o) &amp;= |-\mathrm{Re}(S_1(x, y, s, o)| \\
S_{\mathrm{V1, 270\degree}}(x, y, s, o) &amp;= |-\mathrm{Im}(S_1(x, y, s, o)| \\
\end{aligned}
</span></p>
<p>This operation yields responses like these:</p>
<p><img src="img/single_i_example.png" /></p>
<p>As it turns out, some V1 neurons are less sensitive to phase than others, and some may even respond equally to both lines and edges, as long as scale and orientation match their tuning. Those cells are called <em>complex cells</em><nobr>.<input type="checkbox" id="sn-sidenote-16" class="margin-toggle"></nobr><label for="sn-sidenote-16" class="margin-toggle sidenote-number"></label><span class="sidenote">Simple and complex cells lie along a spectrum of phase specificity, which is brilliantly explained by <a href="https://www.biorxiv.org/content/biorxiv/early/2019/09/25/782151.f%20ull.pdf">this recent paper</a> by Korean researchers Gwangsu Kim, Jaeson Jang and Se-Bum Paik. But it seems that there’s even more to the story, as complex cells seem to <a href="https://hal.archives-ouvertes.fr/hal-00660536/document">change their simpleness index</a> in response to their input as well.</span> Thanks to their phase invariance, complex cells can extract key structural information at the expense of colour and contrast data. But contrast and colour are irrelevant to reading—we can read black-on-white just as well as white-on-black—suggesting that it is mainly complex cells that provide the relevant signals to higher-level brain <nobr>areas.<input type="checkbox" id="sn-sidenote-17" class="margin-toggle"></nobr><label for="sn-sidenote-17" class="margin-toggle sidenote-number"></label><span class="sidenote">In practice, it is measurably easier to read dark text on light backgrounds. Not only do light backgrounds make the pupil contract, <a href="http://dx.doi.org/10.1016/j.apergo.2016.11.001">creating a sharper image</a>, but V1 outputs are also <a href="https:///doi.org/10.1523/JNEUROSCI.1991-09.2009">stronger for darker colours</a>, which may contribute in higher-level shape-recognition stages.</span></p>
<p>To be clear, this does not mean that the signals from simple cells are lost or discarded. Just like the signals from colour-detecting cells in the so-called <em>blob</em> regions of V1, which are not further discussed here, their outputs do contribute both to our experience of vision and to the activity of higher-level brain regions. For reading (and thus letterfitting) purposes, however, we will focus on the responses of complex cells.</p>
<p>Traditionally, complex cells were thought to sum the outputs of nearby simple cells of equal scale and orientation. This is now known to be a gross oversimplification. In software, a similar approach is nevertheless taken to approximate the output of complex cells <span class="math inline">C_{\mathrm{V1}}</span>, namely a simple computation of the absolute magnitude of the complex tensor:</p>
<p><span class="math display">
C_\mathrm{V1}(x, y, s, o) = |S_\mathrm{V1}(x, y, s, o)|^2
</span></p>
<p>This is often called the <em>local energy</em>. The squaring operation shown here is often used to approximate the nonlinear behaviour of complex cells in particular.</p>
<p>Applying this phase-squashing to the above images yields:</p>
<p><img src="img/single_i_complex_example.png" /></p>
<p><nobr><input type="checkbox" id="mn-marginnote-18" class="margin-toggle"></nobr><label for="mn-marginnote-18" class="margin-toggle"></label><span class="marginnote"><img src="img/hra.png" alt="HRA"> Solid line: hyperbolic ratio curve, a.k.a. <a href="https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)">Hill function</a> or Naka-Rushton function. Dotted line: monotonic polynomial (e.g. <span class="math inline">x^2</span>).</span> Of course, the squaring nonlinearity is rather unrealistic. Increase inputs enough, and the firing rate of real cells will level off. A popular model for this is the hyperbolic ratio sigmoid</p>
<p><span class="math display">y = \frac{fx^k}{\beta^k + x^k}</span></p>
<p>The <span class="math inline">f</span> scales the curve vertically, <span class="math inline">k</span> makes the kink steeper, and <span class="math inline">\beta</span> shifts the threshold to the right. Consider how the numerator increases the firing rate, and the denominator decreases it. For relatively small values of <span class="math inline">x</span>, <span class="math inline">\beta^k</span> dominates the denominator, yielding a scaled-down version of <span class="math inline">fx^k</span> (values of about 2 or 3 are common for <span class="math inline">k</span>, in agreement with the square often used). But once <span class="math inline">x^k</span> gets large enough, <span class="math inline">\beta^k</span> pales in comparison, and we are left approaching <span class="math inline">f</span><nobr>.<input type="checkbox" id="sn-sidenote-19" class="margin-toggle"></nobr><label for="sn-sidenote-19" class="margin-toggle sidenote-number"></label><span class="sidenote">This specific activation function is effectively never used in deep learning, both for historical reasons and because <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">its asymptotic behaviour would slow down training</a>.</span></p>
<p>A common architectural pattern in the brain is <em>lateral inhibition</em>, in which neurons within a cortical area suppress their neighbours in proportion to their own firing rate. Locally, this allows the most active neuron to suppress its neighbours more than those neighbours are able to suppress it in return. Lateral inhibition thus sharpens peaks and flattens valleys in the activity landscape; it is a simple and effective way to boost salient signals relative to weaker ones that inevitably arise from the correlations between similarly tuned convolution filters. In particular, lateral inhibition sharpens the orientation and frequency-scale signals we receive from <nobr>V1.<input type="checkbox" id="sn-sidenote-20" class="margin-toggle"></nobr><label for="sn-sidenote-20" class="margin-toggle sidenote-number"></label><span class="sidenote">Of course, its main raison d’être is local contrast normalization, but that seems less relevant in the context of digital typography.</span></p>
<p>Because lateral inhibition is a recurrent process that takes time to reach a steady state, it is most accurately modelled using a system of coupled differential equations which describe the time dependence of each neuron’s firing rate on its neighbours. Conveniently, however, the steady state can also be approximated directly using our hyperbolic ratio model, by simply sneaking the neighbouring neurons’ activities into the <nobr>denominator:<input type="checkbox" id="sn-sidenote-21" class="margin-toggle"></nobr><label for="sn-sidenote-21" class="margin-toggle sidenote-number"></label><span class="sidenote">See <a href="https://arxiv.org/pdf/1906.08246.pdf">this analysis</a> by Jesús Malo et al.’s to understand why this works.</span></p>
<p><span class="math display">y_i = \frac{fx_i^k}{\beta^k + \sum_j w_j x_j^k}</span></p>
<p>This approximation is called <em>divisive normalization</em>. One can find many variations on the above formula in the literature: extra constants, extra square roots in the denominator, extra rectifiers, etc., but the core idea is always the same.</p>
<p>This raises the challenge of determining the right values for <span class="math inline">w_j</span>, i.e. modelling the inhibitive strengths of neighbourly connections. Researchers have collected <nobr>formulas,<input type="checkbox" id="sn-sidenote-22" class="margin-toggle"></nobr><label for="sn-sidenote-22" class="margin-toggle sidenote-number"></label><span class="sidenote">In 2011, Tadamasa Sawada and Alexander Petrov compiled a <a href="https://doi.org/10.1152/jn.00821.2016">very review</a> of divisive normalization models of V1. To my knowledge, it is still the most comprehensive reference today.</span> but it is not clear that they capture all of the interactions there are. What’s more, the last decade of research has revealed the importance of feedback from higher-level areas (discussed below), some of which may have been mistaken for lateral inhibition within V1. Nevertheless, the popularity of divisive normalization models makes it important to understand them.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-23" class="margin-toggle"></nobr><label for="mn-marginnote-23" class="margin-toggle"></label><span class="marginnote"><img src="img/csf.png" alt="Contrast sensitivity function">Contrast sensitivity function. The vertical gradient in contrast is uniform across the image, but we most easily perceive the mid-frequency gratings even at lower contrasts. Note that the red line, shown here only for illustrative purposes, may not match the contrast sensitivity function you experience at your current viewing distance and screen settings.</span> Another aspect of vision that appears to manifest quite early—the optical limitations of our eye notwithstanding—is our specific sensitivity to spatial frequencies. Humans respond particularly well to angular frequencies of about 2–5 cycles per degree, and unsurprisingly this translates to reading speed as well, <em>especially</em> under low-contrast <nobr>conditions.<input type="checkbox" id="sn-sidenote-24" class="margin-toggle"></nobr><label for="sn-sidenote-24" class="margin-toggle sidenote-number"></label><span class="sidenote">See studies like <a href="https://doi.org/10.1167/9.9.16">Chung and Tjan (2009)</a>, <a href="https://doi.org/10.1167/6.6.118">Oruç and Landy (2006)</a>, and many others.</span> This, of course, is a key reason why e.g. hairline type is difficult to read unless at large sizes and a comparatively wide fit. The reader’s contrast sensitivity function may in fact contribute to <span class="math inline">w_j</span>; in other words, mid-scale signals may outcompete fine-scale signals by default.</p>
<h2 id="area-v2-portilla-simoncelli-texture-correlations-and-crowding-effects">Area V2, Portilla-Simoncelli texture correlations, and crowding effects</h2>
<p>Area V1 deconstructed the incoming imagery into thousands of edge and line fragments. Area V2 helps find patterns in those signals, patterns that form the basis for the perceptual grouping effect we are interested in.</p>
<p>Each neuron in V2 takes its input from a combinations of neurons in <nobr>V1,<input type="checkbox" id="sn-sidenote-25" class="margin-toggle"></nobr><label for="sn-sidenote-25" class="margin-toggle sidenote-number"></label><span class="sidenote">Again, we will skip here a discussion of the various layers and interneurons of V2.</span> creating receptive fields that can be twice as large as those in V1. The choices of V1 inputs are (nearly) endless, and indeed, V2 contains a vast variety of neurons representing all kinds of different correlations between V1 neurons: correlations between simple cells and complex cells, between cells of different scales and orientations, and between cells at different spatial locations. <nobr><input type="checkbox" id="mn-marginnote-26" class="margin-toggle"></nobr><label for="mn-marginnote-26" class="margin-toggle"></label><span class="marginnote"><img src="img/v1_v2.png" alt="Connections from V1 to V2">V2 cells take their input from a nearby V1 cells, correlating receptive fields across dimensions of space, simpleness/complexity, orientation, and spatial frequency scale.</span></p>
<p>Presumably, the ability to respond to correlations of inputs from V1 is conferred to V2 neurons by their nonlinear activation curve. Consider a toy example in which two V1 neurons each fire with rates between 0 and 1.0. Then a V2 neuron with the following activation curve would fire only if <em>both</em> inputs are sufficiently active, summing to at least 1.5, thereby implementing correlation:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-27" class="margin-toggle"></nobr><label for="mn-marginnote-27" class="margin-toggle"></label><span class="marginnote">Anthony Movshon and Eero Simoncelli <a href="10.1101/sqb.2014.79.024844">call this</a> the “cross term”, referring to the <span class="math inline">ab</span> in the <span class="math inline">(a+b)^2 = a^2 + 2ab + b^2</span> expression that arises in simplistic square-based activation models. The dashed line shows the deep-learning equivalent expression <span class="math inline">\mathrm{ReLU(x-1.0)}</span>.</span> <img
src="img/v2_nonlinearity.png" alt="Nonlinear activation of V2 neurons
enables computation of correlations"></p>
<p>Intuitively, many of these correlations may appear to be meaningless. As it turns out, however, the local ensemble of many of such correlations effectively describes the texture of the scene. In a now famous experiment, researchers systematically computed a few dozen of such correlations for a given scene. They then synthesized new images, by tweaking random pixels until local averages of their V2 correlations matched the ones in the original <nobr>scene:<input type="checkbox" id="sn-sidenote-28" class="margin-toggle"></nobr><label for="sn-sidenote-28" class="margin-toggle sidenote-number"></label><span class="sidenote">The first iteration of this <a href="https://doi.org/10.1023/A:1026553619983">idea</a> came about in 1999 and is due to to Javier Portilla and Eero Simoncelli. Two decades later, these “Portilla-Simoncelli textures” have inspired countless psychological studies and attempts to refine the model.</span></p>
<p><nobr><input type="checkbox" id="mn-marginnote-29" class="margin-toggle"></nobr><label for="mn-marginnote-29" class="margin-toggle"></label><span class="marginnote">The “image metamer” on the left was <a href="https://dx.doi.org/10.1038%2Fnn.2889">generated</a> by Jeremy Freeman and Eero Simoncelli in 2011 based on the same principle of matching image statistics. As in the human brain, the authors averaged the statistics over a wider area in the periphery than in the fovea. When focusing strictly on the image center, the metamer is difficult to distinguish from the original.</span> <img
src="img/metamers.png" alt="From 'Metamers of the ventral stream'"></p>
<p>As evident here, a mere approximation of these averaged image statistics measured by V2 is enough to simulate, with eerie fidelity, how we perceive our visual periphery. This is no coincidence: after all, higher-level areas (here, V4) precisely respond to particular configurations of such V2 neurons, so synthesizing images which evoke similar V2 activations will also result in similar higher-level perceptions, even if the actual input signals are quite <nobr>different.<input type="checkbox" id="sn-sidenote-30" class="margin-toggle"></nobr><label for="sn-sidenote-30" class="margin-toggle sidenote-number"></label><span class="sidenote">One could think of this as the inverse of an <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">adversarial input</a>.</span></p>
<p>That V2 neurons so effectively describe local image statistics presents us with a first opportunity to reify a heretofore vague concept into something concrete and computable: namely, that “rhythm” or “balance” between black and white translates to correlations between V1 responses. And indeed, this appears to be possible:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-31" class="margin-toggle"></nobr><label for="mn-marginnote-31" class="margin-toggle"></label><span class="marginnote">Here, Javier Portilla and Eero Simoncelli demonstrated how a set of V2 statistics computed and averaged over an image of text could be used to extrapolate the perceived texture outwards. The comparably poor quality of this example taken from their paper should not be taken as reason to dismiss the idea; it was generated at low resolution over two decades ago and averaged the statistics too aggressively. Many more sophisticated variants of the model have since been published, with promising results especially on natural scenes.</span> <img
src="img/text_v2_texture.png" alt="Texture extension on image of text.
From Portilla and Simoncelli, 2000."></p>
<p>We may be tempted to exploit this effect to build a simple letterfitting strategy in which we iteratively adjust pair distances in an image of text until a chosen set of V2 responses is nice and uniform across the entire image. And indeed, I believe this would be the most effective and biologically faithful approach to achieve a perfectly even typographic “colour”. However, it would likely create <em>too</em> even of a colour, at the expense of the readability of letters and words. There simply is no getting around understanding the grouping mechanisms that rule our perception of shapes.</p>
<p>Still, V2 statistics matter. Not only might they serve as a very useful tool to optimize visual consistency during the type design process itself—a topic for another research project!—but they are intimately related to perceptual grouping through a phenomenon called <em>crowding</em>, which we will address later.</p>
<!--
<p class="missing">
Surround suppression from uniform textures (Coen-Cagli et al., etc.)
</p>
-->
<h2 id="contour-integration-and-v1-feedback">Contour integration and V1 feedback</h2>
<p>Not all V2 neurons respond to peculiar V1 correlations expressing elements of texture. Some pick up on more obviously salient signals, such as continuous edges and lines. Experiments suggest that they do so by responding to V1 complex cells that co-align:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-32" class="margin-toggle"></nobr><label for="mn-marginnote-32" class="margin-toggle"></label><span class="marginnote">Each cell corresponds to a V1 complex cell tuned to a certain orientation (the distribution in frequency scales is ignored here). Note that the number of V1 cells is exaggerated for effect. This neuron responds to collinear V1 activations suggesting the presence of a horizontal contour, even if curved (as in the sample shown). It may be inhibited by parallel flanking contours and perpendicular contours, although this is less clear. This pattern has been called “association field”, “bipole”, and many other names in papers going back to the 1990s.</span> <img src="img/v2_contour_integration.png"
alt="Receptive fields of a V2 contour integration neuron"></p>
<p>This allows these V2 cells to detect continous contours, even if these contours are curved or <nobr>interrupted.<input type="checkbox" id="sn-sidenote-33" class="margin-toggle"></nobr><label for="sn-sidenote-33" class="margin-toggle sidenote-number"></label><span class="sidenote">Two studies showing this most clearly are by <a href="https://doi.org/10.1016/j.neuron.2014.03.023">Minggui Chen et al. in 2014</a> and by <a href="https://doi.org/10.1016/j.neuron.2017.11.004">Rujia Chen et al. in 2017</a>.</span> Interrupted contours are a constant challenge to the vision system: not only can the edges of an object be occluded (e.g. tree branches in front of a mountain), but our retina is carpeted with a spider web of light-scattering nerve bundles and <nobr>capillaries.<input type="checkbox" id="sn-sidenote-34" class="margin-toggle"></nobr><label for="sn-sidenote-34" class="margin-toggle sidenote-number"></label><span class="sidenote">Not to mention our <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)">blind spot</a>.</span> Contour-integrating V2 cells therefore allow us to perceive contours even where we cannot actually see them.</p>
<p><nobr><input type="checkbox" id="mn-marginnote-35" class="margin-toggle"></nobr><label for="mn-marginnote-35" class="margin-toggle"></label><span class="marginnote"><img src="img/contour_integration_example.png" alt="Contour
integration example">Typical contour integration test image. Adapted from <a href="https://doi.org/10.3389/fpsyg.2013.00356">Roudaia et al.</a>, 2013.</span> Having thus detected a piece of contour, the V2 neuron now sends an amplifying signal to all of its V1 inputs, which in turn increases the input to the V2 cell itself, creating a positive feedback loop between V1 and V2. Crucially, however, this feedback only amplifies neurons that are already firing; it does not induce activity in other inputs (and may even suppress <nobr>them).<input type="checkbox" id="sn-sidenote-36" class="margin-toggle"></nobr><label for="sn-sidenote-36" class="margin-toggle sidenote-number"></label><span class="sidenote">Physiologically, this kind of modulatory amplification may be related to increased spike synchrony between neurons, as explored in <a href="https://doi.org/10.1152/jn.01142.2015">this 2016 study</a> by Wagatsuma et al.</span> Thanks to this feedback loop, contiguous contours pop out to us perceptually in a matter of milliseconds, while non-contour features (like the dot in the illustration below) do not:</p>
<p><img src="img/v2_contour_integration_2.png"></p>
<p>This kind of feedback loop is not only a key ingredient in the perceptual grouping mechanism we will discuss below but, as we will see, a primitive grouping mechanism of its own.</p>
<h2 id="v4-and-higher-level-areas">V4 and higher-level areas</h2>
<p>The next area of the visual cortex, area <nobr>V4,<input type="checkbox" id="sn-sidenote-37" class="margin-toggle"></nobr><label for="sn-sidenote-37" class="margin-toggle sidenote-number"></label><span class="sidenote">This discussion is limited to the <a href="https://en.wikipedia.org/wiki/Two-streams_hypothesis">ventral stream</a>, i.e. parts of the visual system dedicated to object recognition. The dorsal stream, on the other hand, comprises areas concerned with keeping track of our environment and our position in it; it appears to be irrelevant to letterfitting.</span> mirrors the architecture of V2 in that it performs a set of convolutions detecting correlations between its inputs. Arguably, V4 is just like V2, only with larger receptive fields.</p>
<p>Just as in V2, two categories of neurons are particularly noteworthy: texture detectors and contour <nobr>detectors.<input type="checkbox" id="sn-sidenote-38" class="margin-toggle"></nobr><label for="sn-sidenote-38" class="margin-toggle sidenote-number"></label><span class="sidenote">Our understanding of V4 is primarily owed to Anitha Pasupathy and her collaborators, who have been publishing results like <a href="https://doi.org/10.1152/jn.2001.86.5.2505">this one</a>, <a href="https://doi.org/10.1152/jn.01265.2006">this one</a>, <a href="https://doi.org/10.1523/JNEUROSCI.3073-18.2019">this one</a> and <a href="https://www.nature.com/articles/s41467-017-02438-8">this one</a> for nearly two decades.</span> Just as in V2, the contour detectors integrate smaller contour fragments across some region. However, the larger receptive fields of V4 allow for the target contours to be substantially offset from the neuron’s receptive field center:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-39" class="margin-toggle"></nobr><label for="mn-marginnote-39" class="margin-toggle"></label><span class="marginnote">Note how all shapes have in common the convexity on the lower left.</span> <img src="img/v4_rf.png" alt="Receptive field and some example stimuli for a V4
object-centered contour-detecting cell"></p>
<p>This is significant in practice, because it allows for easy shape detection and, as we will see, perceptual grouping. For instance, consider how the following population of neurons might respond to the corners and sides of a large square. A higher-level square-detecting neuron, presumably located in the inferotemporal cortex, would be easily able to integrate all of these colocated responses to report a square shape:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-40" class="margin-toggle"></nobr><label for="mn-marginnote-40" class="margin-toggle"></label><span class="marginnote">The receptive fields, shown as a dashed gray circle in the diagram above, are not shown here. Instead, the dotted red lines illustrate the radius of each receptive field.</span> <img src="img/v4_combo.png" alt="Detecting a square"></p>
<p>Thanks to the robustness afforded by the integration mechanisms in V2, V4, and higher-level areas, we can now explain how we can simultaneously detect the presence of a square and yet not <em>see</em> it, in this classic optical <nobr>illusion:<input type="checkbox" id="sn-sidenote-41" class="margin-toggle"></nobr><label for="sn-sidenote-41" class="margin-toggle sidenote-number"></label><span class="sidenote">This phenomenon of <a href="https://en.wikipedia.org/wiki/Illusory_contours">illusory contours</a>, popularized by <a href="https://en.wikipedia.org/wiki/Gaetano_Kanizsa">Gaetano Kanizsa</a>, has inspired dozens of studies.</span></p>
<p><img src="img/kanisza.png" alt="Kanizsa square"></p>
<p>Even though the sides of the square are only weakly detected, the sum total of activations is enough to allow us to perceive the presence of a square although V1 detects no edges at the square’s sides at all.</p>
<h2 id="grouping-via-border-ownership">Grouping via border ownership</h2>
<p>Navigating our natural environment requires us to make sense of groups of contours as coherent objects in three-dimensional space, even though our visual system only has access to a two-dimensional projection. As the illusory square above demonstrates, our vision system manages to do this even if in the absence of binocular disparity cues, and despite interrupted contours and occluded objects. To accomplish this, V4 signals that likely constitute an object need to prevail over others that do not. For instance, a square not only excites contour detectors centered inside of it (red), but also on the outside (blue):</p>
<p><img src="img/v4_outside.png" alt="V4 respones outside of the square"></p>
<p>Once again, a feedback loop saves the day: the internal (red) neurons will almost immediately fire more rapidly, thanks to amplifying recurrent signals from the higher-level square detector. The external (blue) neurons do not constitute a coherent shape, therefore receive such feedback and fire much more slowly as a result:</p>
<p><img src="img/v4_it_feedback.png" alt="Feedback between V4 and IT"></p>
<p>This basic architectural pattern already explains quite well how contiguous, or nearly contiguous, objects rise to salience while other signals wane. But we have yet to discuss what happens when multiple objects get involved.</p>
<p>Researchers have discovered that the above feedback loop extends downwards to a class of V2 neurons called <em>border ownership cells</em> or B-cells. B-cells, like the V2 contour integration cells already discussed, detect the presence of edges based on V1 complex cells. While they are agnostic to the edge’s contrast polarity, B-cells fire only if they are on one particular side of an object. For instance, the B-cell whose receptive field is marked in red below only detects edges on the <em>left side</em> of objects, as indicated by the small protrusion pointing toward the <nobr>object.<input type="checkbox" id="sn-sidenote-42" class="margin-toggle"></nobr><label for="sn-sidenote-42" class="margin-toggle sidenote-number"></label><span class="sidenote">Almost everything we know about border ownership networks is owed to Johns Hopkins researcher Rüdiger von der Heydt and his students.</span> It responds to stimuli 1 and 2, but not 3 and 4:</p>
<p><img src="img/b_cell_1.png" alt="B cell illustration"></p>
<p>The B-cell only sees an edge. It cannot know which part of the object it is on; its receptive field is much too small. So its activity must be gated by a neuron which does: namely, one of our higher-level V4 <nobr>neurons.<input type="checkbox" id="sn-sidenote-43" class="margin-toggle"></nobr><label for="sn-sidenote-43" class="margin-toggle sidenote-number"></label><span class="sidenote">Signals from other V2 neurons are unlikely, because horizontal connections conduct <a href="https://dx.doi.org/10.1152%2Fjn.00928.2010">too slowly</a> to explain the lab-measured response times of B-cells.</span> The object owning the edge fragment could have any shape and size, so all active V4 neurons whose contour templates coincide with the edge fragment send amplifying signals to our B-cell. In turn, our B-cell directly contributes to their activation, establishing a positive feedback loop:</p>
<p><img src="img/bg_feedback_0.png" alt="B-cell feedback loop"></p>
<p>There is an entire population of B-cells, distributed across V2’s retinotopy. For instance, consider a right-side B-cell (blue below) neighbouring our left-side B-cell. Both B-cells are engaged in feedback loops with V4 neurons while simultaneously inhibiting local competitors—i.e., each other—in proportion to their own activation strength (recall our discussion of divisive normalization as an approximation of lateral inhibition in V1):</p>
<p><img src="img/bg_feedback_1.png" alt="B-cell feedback loop"></p>
<p>But as we know, the internal V4 contour detector (red) is already firing more strongly than the external one (blue), thanks to its participation in a coherent object. This enhances the activity of the left-side (red) B-cell, which suppresses its competitor, which in turn further reduces the input to the external (blue) V4 cell. After some tens of milliseconds, the percept of the square as its own object is solidly established.</p>
<p><img src="img/bg_feedback_2.png" alt="B-cell feedback loop"></p>
<p>Having introduced the concept of B-cells, we can now finally discuss what happens when multiple objects get involved—we are still talking about letterfitting, after all!</p>
<p>Consider the following situation, and make a guess whether at the circled location, left-side or right-side B-cells would win out:</p>
<p><img src="img/gestalt_1.png" alt="Gestalt B-cell 1"></p>
<p>Although <em>technically</em> the dark shape is a closed, contiguous shape and therefore on par with the light circle, it is intuitively obvious that the circle dominates, and even appears to lie above the black shape. As a result, we can confidently predict that humans will perceive the circled region as the left side of the circle, and not as the right side of the dark area.</p>
<p>As it turns out, the vast majority (although not all) of the object-centered contour detectors in V4 are either straight or convex in shape, with various degrees of curvature. This has the profound consequence that convex shapes tend to outcompete concave shapes in our perception.</p>
<p>Meanwhile, although the hypothetical square detector served us well in the examples above, we actually do not know the population of shape detectors in our inferotemporal cortex.</p>
<p>In simulations of perceptual grouping, it is therefore practical not to think directly about V4 contours and shape detectors. Instead, a popular approach is to work with a population of representative (if fictitious) “grouping cells” or G-cells, each of which receives input from an annulus of inward-directed <nobr>B-cells:<input type="checkbox" id="sn-sidenote-44" class="margin-toggle"></nobr><label for="sn-sidenote-44" class="margin-toggle sidenote-number"></label><span class="sidenote">The first to run an earnest simulation of this idea were <a href="https://doi.org/10.1152/jn.00203.2007">Edward Craft et al.</a> in 2011.</span></p>
<p><img src="img/bg_rfs.png" alt="Receptive fields of G cells"></p>
<p>Given that the V4 contour detectors chiefly pick up convex curvatures at some eccentricity, this is really quite a reasonable model for whatever may truly be going on in our posterior inferotemporal areas.</p>
<p>In the square below, a single large-scale G cell centered on the shape is excited by inward-directed B-cells on all four sides (red). Inside the corners, a series of smaller-scale G cells receive input from two sides (purple, blue).</p>
<p><img src="img/g_responses.png" alt="Sample responses of some G cells"></p>
<p>In many other locations, G-cells are activated by only a few B-cells on just one side. This is not enough to evoke a strong activation. In addition, we may assume that G-cells compete via local inhibition, such that those cells receiving inputs from more (and from nearer) B-cells dominate.</p>
<p>Once B-cells and G-cells have settled into an equilibrium, the locus of peak responses of G-cells across different scales neatly represents the skeleton of the shape, shown on the <nobr>right:<input type="checkbox" id="sn-sidenote-45" class="margin-toggle"></nobr><label for="sn-sidenote-45" class="margin-toggle sidenote-number"></label><span class="sidenote">The technical term for this feat is <a href="https://en.wikipedia.org/wiki/Medial_axis">medial axis transform</a>.</span></p>
<p><img src="img/g_responses.png" alt="Sample responses of some G cells"></p>
<p>Skeletonization is critical to object recognition, because it allows us to match on a shape’s underlying geometric structure. Consider, for instance, our ability to recognize all four letters with the same ease:</p>
<p><nobr><input type="checkbox" id="mn-marginnote-46" class="margin-toggle"></nobr><label for="mn-marginnote-46" class="margin-toggle"></label><span class="marginnote">Many different uppercase-E designs exist, but all of them share a relationship between the relative location of large-scale G-cell peaks (within the counters) and smaller-scale peaks (at the terminals).</span> <img src="img/e_skeletons.png" alt="Some skeletons at different scales"></p>
<p>Although the shared features of the skeletons (counters, stems, etc.) appear at different scales for different letter shapes, they are present in the same configuration for all of them.</p>
<p>This is true even for letters that are outlined, as V4 contour detector neurons respond primarily to the contour, not to the fill. Nevertheless: when is a stroke perceived as a contour, and when does it turn into a shape of its own right, with contours on either side? With letter weights ranging from hairline to ultra-heavy, this is a particularly salient question:</p>
<p><img src="img/letter_weights.png" alt="A range of letter weights"></p>
<p>The hairline letter is, arguably, too thin to allow readers to clearly perceive border ownership of the left and right side of each stem (of course this depends on the font size and the contrast sensitivity function, as discussed above). Nevertheless, we can evidently recognize the letter, so it follows that thin lines must be able to excite fine-scale G-cells even if the ownership of sides is fuzzy.</p>
<p class="missing">
Inihibition of accidental contours: <a href="https://dx.doi.org/10.1523%2FJNEUROSCI.4766-10.2011">Brittany Bushnell et al.</a> have reported that continuous straight contours inhibit the activation of abutting, acute V4 contour detectors, which are typically a side-effect of occlusion.
</p>
<p class="missing">
Serifs and skeleton analogs in sans serifs.
</p>
<p class="missing">
Classification vs. representation; V1/V2 as a blackboard (Roelfsema 2016)
</p>
<h2 id="why-grouping-cells-exist-attention-crowding-and-the-spread-of-activity">Why grouping cells exist: attention, crowding, and the spread of activity</h2>
<p>We have now roughly sketched out how cognitive scientists currently think about shape perception. Fortunately for us, reading does not involve much processing of colour, motion, and depth. Before we can discuss letterfitting, however, we need to outline how attention propagates through this architecture.</p>
<p class="missing">
A feedback model of attentional effects in the visualcortex https://doi.org/10.1109/CIMSIVP.2011.5949241 https://hal.archives-ouvertes.fr/hal-00706798/file/miconi_t_11_106.pdf
</p>
<p class="missing">
Review crowding and classic hypotheses of feature pooling and cortical magnification
</p>
<p class="missing">
Review Herzog lab papers; effect of regularity on crowding (Sareela et al. 2010) and uncrowding via LAMINART grouping (Francis et al.)
</p>
<p class="missing">
Offer intuition for perceptual grouping of words and crowding as related effects of lateral spread of neural activity.
</p>
<p class="missing">
Review the concept of attention, and its ability to affect the spread (both facilitatory and laterally inhibitive, see e.g. Mihalas et al.)
</p>
<h2 id="from-perceptual-grouping-to-letterfitting">From perceptual grouping to letterfitting</h2>
<p>The above model of our vision system’s perceptual grouping mechanisms finally allows us to make some predictions about typographic truths, and should ultimately allow us to build a robust, biologically plausible letterfitting tool.</p>
<p class="missing">
Competition between word-scale and stem-scale grouping cells
</p>
<p>Let’s see how our current understanding of perceptual grouping plays out in some axiomatic letter pairs:</p>
<p><img src="img/benchmark_gaps.png" alt="Some benchmark letter pairs: nn, oo, nl,
and IUL"></p>
<p class="missing">
Explain the asymptotic length-invariance of parallel stems
</p>
<p class="missing">
Explain round-round interactions via weaker contour integration and less disruption of V4
</p>
<p class="missing">
Explain IUL via balance between weaker horizontal contour integration and stronger inhibition from false inter-stem medial axis (b/c smaller radius).
</p>
<p class="missing">
Illustrate effect of serifs, italics, x-height and weight
</p>
<h2 id="why-word-grouping-matters-models-of-reading">Why word grouping matters: models of reading</h2>
<p>So far, we have described some important neural dynamics of the visual cortex, between V1 and the early inferotemporal cortex. Experimental results have yielded rich hypotheses about the neural architecture of perceptual grouping, hypotheses that appear to explain many aspects of letterfitting practice.</p>
<p>But the patterns of neural activity involved in reading don’t stop at the edge of the visual cortex: about a quarter second after we first see a word, neurons in the so-called <em>visual word form area</em> (VWFA) in our left fusiform gyrus (see the anatomical illustration above) have settled the identity of the word.</p>
<p>The above sketch of the influence of attention</p>
<p>Poor letterfitting can affect the performance of the VWFA beyond</p>
<p>The visual word form area contains neurons that identify letters and words. Letterfitting can affect the performance of these</p>
<p>Understanding how this detection works may help us augment our model to aware of semantic issues that can affect design decisions. One common issue is confusability (e.g. between <em>rn</em> and <em>m</em>), but we can look beyond alphabetic letterfitting to the relative placement of accents, and even to the strokes and components of Hangul and <nobr>Hanzi<input type="checkbox" id="sn-sidenote-47" class="margin-toggle"></nobr><label for="sn-sidenote-47" class="margin-toggle sidenote-number"></label><span class="sidenote">The recognition of Chinese characters takes place in the <em>right</em> fusiform gyrus (the VWFA is in the left), a region traditionally associated with face recognition (although <a href="https://doi.org/10.1371/journal.pone.0041103">EEG readings suggest that face-recognition circuitry isn’t directly involved</a>). The compositional mechanisms described here likely transfer in principle, however.</span>—all of these are instances of the problem of competitive perceptual grouping.</p>
<p class="missing">
Describe the latest reading model (overlap-based n-gram hierarchy; Graigner, Gomez et al.); reference letter transposition studies.
</p>
<p class="missing">
Comment on the role of word dividers/breaks, or the lack of them, in different languages; linguistic reasons for their necessity/absence.
</p>
<h2 id="building-practical-letterfitting-algorithms">Building practical letterfitting algorithms</h2>
<p>Unfortunately, the dynamism of the scientific model(s) introduced thus far makes them unsuitable for use in practical letterfitting tools for type designers. Although it is relatively straightforward to set up systems of coupled differential equations representing individual neurons, integrating them at a sufficiently fine spatial resolution is immensely costly, and doing so over many iterations for each letter combination is outright infeasible, at least with consumer-grade hardware. We must therefore consider potential approximations to the model.</p>
<p class="missing">
Brief nod to existing solutions in the appendix, and how they happen to approximate (or not) some of the model characteristics discussed.
</p>
<p class="missing">
First option: use only difference in V1 complex cell activations, weigh based on orientation and size. Pair gains approximate word-scale grouping strength, pair losses approximate stem-scale losses. Does not consider contour pop-out or actual grouping dynamics; does quite poorly on uppercase letters. However, straightforward to train on existing fonts via backprop. Show some results.
</p>
<p><img src="img/abstract.png"></p>
<p class="missing">
Brief nod to residual nets, which effectively unroll the dynamics over a few fixed time steps. Also mention Ricky Chen’s Neural ODE option.
</p>
<p class="missing">
Potentially feasible: one forward sweep; V1 → V1 DivN → V2 → contour integration DivN → grouping via fuzzy circular G cells → feedback to V2 B → update G cells. Take difference between pair and letters; weight and integrate; backprop-fit against existing fonts. Show some results.
</p>
<p>vicarius</p>
<p><a name="existing_tools"></a></p>
<h2 class="appendix">
Appendix: Existing letterfitting tools
</h2>
<p>Most existing approaches operate either on the distance between stems, or on the area of the gap between them. Some are hybrids, more complex, or unpublished; finally, there has been some experimental work using neural nets:</p>
<p><img src="img/heuristics_classification.png"
alt="Heuristics" /></p>
<p><strong>Fixed-distance methods:</strong> A family of approaches that insert pre-defined distances between letter pairs. In their simplest incarnation, these heuristics are equivalent to simply adding sidebearings to every letter, without any kerns. <a href="https://github.com/hodefoting/kernagic">Kernagic</a>, inspired by <a href="https://www.lettermodel.org/">Frank Blokland’s research</a>, uses heuristics to identify stems or stem-equivalents (such as the round sides of an o) in every letter shape, and then aligns them. This works reasonably well with very regular type (think blackletter), but manual adjustments are usually required. Less well known is Barry Schwartz’ <a href="https://github.com/chemoelectric/sortsmill/blob/master/tools/spacing_by_anchors.py">anchor point implementation</a> of what amounts to basically the same idea. Adrian Frutiger, Walter Tracy and Miguel Sousa have devised similar systems, described in Fernando Mello’s <a href="http://www.fermello.org/FernandoMello_essay.pdf">MATD thesis</a>. The legendary <a href="https://en.wikipedia.org/wiki/Hz-program">Hz-Program</a> is also included in this category, as its <a href="https://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&amp;date=19941019&amp;DB=&amp;locale=en_EP&amp;CC=EP&amp;NR=0466953B1&amp;KC=B1&amp;ND=1#">patent application</a> reveals that letter pair distances were simply stored in a hardcoded table.</p>
<p><strong>Gap area quadrature:</strong> A family of algorithms that attempt to quantify and equalize the perceived area of the inter-letter gap. The crux, of course, lies in deciding where the gap ends. <a href="https://huertatipografica.github.io/HTLetterspacer/">HT Letterspacer</a>, the crudest one of these tools, considers everything between baseline and x-height (modulo some minor refinements). Simon Cozens’ <a href="https://github.com/simoncozens/CounterSpace">CounterSpace</a> uses blurs and convex hulls to more effectively exclude regions that arguably don’t belong to the gap (such as the counter of c). My own <a href="https://www.aldusleaf.org/2019-03-17-letterfitting-attention-model.html">Electric Bubble</a> model measures Euclidean instead of horizontal distances, but imposes geometric constraints that produce similar results to CounterSpace. CounterSpace currently wins in terms of performance-complexity ratio but it, too, struggles to fit certain letter pairs.</p>
<p><strong>Other shape-based methods:</strong> These include more exotic approaches, such as stonecarver <a href="https://en.wikipedia.org/wiki/David_Kindersley">David Kindersley</a>’s “wedge method” from the 1960s, which operated on letter area moments of inertia (and didn’t really work), and <a href="https://ikern.com/k1/">iKern</a>, which produces great results but, just like Adobe’s <a href="https://typedrawers.com/discussion/3006/how-does-adobes-automatic-optical-kerning-work">Optical Kerning</a> feature, remains unpublished. Last but not least, the <a href="http://charlesmchen.github.io/typefacet/topics/autokern/typefacet-autokern-manual.html">TypeFacet Autokern</a> tool identifies parts of letter outlines that jut out horizontally, and adds kerning to compensate, based on a few parameters.</p>
<p><strong>Neural nets:</strong> Yes, we can train convolutional nets to recognize images of well-fitted and poorly-fitted type. Simon Cozens has built several versions of his <a href="https://github.com/simoncozens/atokern">kerncritic</a> model (formerly AtoKern), and the recent ones perform surprisingly well on many (if not all) pairs. While neural nets are fascinating, they tend to be black boxes: we can only make guesses at how they work, and we cannot tune their behaviour to suit our taste. This problem holds not just for convolutional nets, but for statistical function approximators in general; I do not discuss them further here.</p>
<p><strong>Honorable mention:</strong> <a href="https://groups.google.com/forum/#!searchin/comp.fonts/laurence$20penney$20kern/comp.fonts/GEjTE9_H52M/BSLdSE2lgmsJ">Bubble Kerning</a> is a proposal that type designers draw a bubble around every letter, such that software can automatically find pair distances by simply abutting the bubbles. While this isn’t technically a letterfitting heuristic at all, it’s still worth mentioning as a neat idea that could perhaps save designers some time. Toshi Omagari has built a <a href="https://github.com/Tosche/BubbleKern">Glyphs plugin</a>.</p>
</body>
</html>
